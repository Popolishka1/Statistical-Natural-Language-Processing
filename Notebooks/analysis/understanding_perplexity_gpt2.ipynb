{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities for each token in the input text:\n",
      "tensor([ -7.7088,  -6.7101,  -6.1687,  -8.1749, -11.5447,  -8.3943,  -7.8854,\n",
      "         -4.2600,  -7.2404, -11.6968])\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get model outputs (logits)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Compute log probabilities\n",
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "# Get log probability of the actual tokens in the input text\n",
    "token_ids = inputs.input_ids\n",
    "token_log_probs = log_probs[0, torch.arange(len(token_ids[0])), token_ids[0]]\n",
    "\n",
    "print(\"Log probabilities for each token in the input text:\")\n",
    "print(token_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n",
      "62.60416030883789\n"
     ]
    }
   ],
   "source": [
    "text1 = \"He married a beautiful woman.\"\n",
    "\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "print(logits.shape)  # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "# Get log probs of the actual tokens\n",
    "token_log_probs = log_probs[0, torch.arange(input_ids.shape[1]-1), input_ids[0, 1:]]\n",
    "\n",
    "# Compute perplexity\n",
    "avg_log_prob = token_log_probs.mean()  # Mean log probability per token\n",
    "perplexity = torch.exp(-avg_log_prob)  # Perplexity formula\n",
    "\n",
    "print(perplexity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['He', 'Ġmarried', 'Ġa', 'Ġbeautiful', 'Ġwoman', '.']\n",
      "Token IDs: [1544, 6405, 257, 4950, 2415, 13]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"He married a beautiful woman.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text1)  # Returns a list of tokenized words\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)  # Convert tokens to IDs\n",
    "\n",
    "# Print results\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding GPT-2 Tokenization: The `Ġ` Symbol in Tokens\n",
    "\n",
    "### **Why Does `Ġ` Appear in Some Tokens?**\n",
    "When using GPT-2's tokenizer, you may notice that some tokens have a **`Ġ`** prefix. This is because GPT-2 uses **Byte-Pair Encoding (BPE)**, where:\n",
    "- The `Ġ` symbol represents a **space before the word**.\n",
    "- GPT-2 **does not tokenize spaces separately**; instead, spaces are merged with words as a prefix.\n",
    "- The model learns word relationships better when spaces are included in the tokenization.\n",
    "\n",
    "### **Example Tokenization**\n",
    "Let's tokenize the sentence `\"He married a beautiful woman.\"`:\n",
    "\n",
    "```python\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Input text\n",
    "text = \"He married a beautiful woman.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)  \n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Print results\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```python\n",
    "Tokens: ['He', 'Ġmarried', 'Ġa', 'Ġbeautiful', 'Ġwoman', '.']\n",
    "Token IDs: [1544, 6405, 257, 4950, 2415, 13]\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- \"He\" → No Ġ because it's the first word.\n",
    "- \"Ġmarried\" → Ġ means it was preceded by a space.\n",
    "- \"Ġa\", \"Ġbeautiful\", \"Ġwoman\" → Also preceded by spaces.\n",
    "- \".\" → No Ġ because punctuation is usually attached directly.\n",
    "\n",
    "### How to Decode Token IDs Correctly?\n",
    "\n",
    "You can use `tokenizer.decode()` to automatically restore spaces:\n",
    "\n",
    "```python\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(decoded_text)\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "✅ The Ġ symbol represents a space before a word in GPT-2's tokenization.\n",
    "\n",
    "✅ GPT-2 tokenizes based on subwords, keeping spaces attached.\n",
    "\n",
    "✅ tokenizer.decode() removes Ġ and reconstructs the original text correctly.\n",
    "\n",
    "✅ You can manually clean tokens with .replace(\"Ġ\", \"\") if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "text1 = \"He married a beautiful woman.\"\n",
    "\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "print(log_probs.shape)  # (batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the probabilities of all the tokens after \"He married a beautiful\" (here it's woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probabilities = log_probs[0][4]\n",
    "log_probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the log probability of the word \" woman\" following \"He married a beautiful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.62405014038086\n"
     ]
    }
   ],
   "source": [
    "woman_token_id = 2415\n",
    "\n",
    "# # We can find this back with this piece of code:\n",
    "\n",
    "# tokens = tokenizer.tokenize(\" woman\")\n",
    "# token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# print(token_ids)\n",
    "\n",
    "log_prob = log_probabilities[woman_token_id]\n",
    "print(log_prob.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look if woman was the most likely word to come after \"He married a beautiful\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "# find position of max probability\n",
    "print(torch.argmax(log_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.7113)\n"
     ]
    }
   ],
   "source": [
    "print(log_probabilities[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID 11 corresponds to: ','\n"
     ]
    }
   ],
   "source": [
    "token_id = 11\n",
    "\n",
    "# Convert token ID to its corresponding word\n",
    "decoded_token = tokenizer.decode([token_id])\n",
    "\n",
    "print(f\"Token ID {token_id} corresponds to: '{decoded_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the most likely word/token after \"He married a beautiful\" is a comma. But we wrote this sentence so it makes sense. \" woman\" can also be likely. Let's see for the token \" car\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.70089340209961\n"
     ]
    }
   ],
   "source": [
    "car_token = tokenizer.tokenize(\" car\")\n",
    "car_token_id = tokenizer.convert_tokens_to_ids(car_token)[0]\n",
    "\n",
    "car_log_prob = log_probabilities[car_token_id]\n",
    "\n",
    "print(car_log_prob.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \" car\" is logically less likely (10,000 less) to follow than \" woman\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits and log_probs shape: torch.Size([1, 6, 50257])\n",
      "token_log_probs: tensor([ -9.4455,  -1.9388,  -4.6002, -10.0217,  -3.0335])\n",
      "avg_log_prob: tensor(-5.8080)\n",
      "332.9397277832031\n"
     ]
    }
   ],
   "source": [
    "text1 = \"He married a beautiful plane.\"\n",
    "\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "# logits.shape == (batch_size, sequence_length, vocab_size)      # here (1, 6, 50257)\n",
    "\n",
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "print(\"logits and log_probs shape:\", logits.shape)\n",
    "\n",
    "# Get log probs of the actual tokens\n",
    "token_log_probs = log_probs[0, torch.arange(input_ids.shape[1]-1), input_ids[0, 1:]]\n",
    "print(\"token_log_probs:\", token_log_probs)\n",
    "\n",
    "# Compute perplexity\n",
    "avg_log_prob = token_log_probs.mean()  # Mean log probability per token\n",
    "print(\"avg_log_prob:\", avg_log_prob)\n",
    "perplexity = torch.exp(-avg_log_prob)  # Perplexity formula\n",
    "\n",
    "print(perplexity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8.8688,  -8.6120,  -7.8501,  -5.0303,  -7.6311, -11.0583])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs[0, torch.arange(input_ids.shape[1]), input_ids[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6405,  257, 4950, 6614,   13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of first token: -8.868807792663574\n",
      "Log probabilities of predicted tokens: tensor([ -9.4455,  -1.9388,  -4.6002, -10.0217,  -3.0335])\n",
      "All log probabilities: tensor([ -8.8688,  -9.4455,  -1.9388,  -4.6002, -10.0217,  -3.0335])\n",
      "\n",
      "Perplexity: 554.5197143554688\n"
     ]
    }
   ],
   "source": [
    "text1 = \"He married a beautiful plane.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "# Compute logits (predictions)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Convert logits to log probabilities\n",
    "log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "# 1️⃣ First token: log probability of appearing as first token\n",
    "first_token_log_prob = log_probs[0, 0, input_ids[0, 0]]\n",
    "\n",
    "# 2️⃣ Rest of the tokens: log probabilities of being predicted\n",
    "predicted_log_probs = log_probs[0, torch.arange(input_ids.shape[1] - 1), input_ids[0, 1:]]\n",
    "\n",
    "# Combine them into one tensor\n",
    "all_log_probs = torch.cat([first_token_log_prob.unsqueeze(0), predicted_log_probs])\n",
    "\n",
    "# Print results\n",
    "print(\"Log probability of first token:\", first_token_log_prob.item())\n",
    "print(\"Log probabilities of predicted tokens:\", predicted_log_probs)\n",
    "print(\"All log probabilities:\", all_log_probs)\n",
    "print()\n",
    "\n",
    "\n",
    "# Compute perplexity\n",
    "avg_log_prob = all_log_probs.mean()  # Mean log probability per token\n",
    "perplexity = torch.exp(-avg_log_prob)  # Perplexity formula\n",
    "print(\"Perplexity:\", perplexity.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity computation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of first sentence: 62.60\n",
      "Perplexity of second sentence: 306.76\n"
     ]
    }
   ],
   "source": [
    "def compute_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Get log probs of the actual tokens\n",
    "    token_log_probs = log_probs[0, torch.arange(input_ids.shape[1]-1), input_ids[0, 1:]]\n",
    "\n",
    "    # Compute perplexity\n",
    "    avg_log_prob = token_log_probs.mean()  # Mean log probability per token\n",
    "    perplexity = torch.exp(-avg_log_prob)  # Perplexity formula\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "# Example usage\n",
    "text1 = \"He married a beautiful woman.\"\n",
    "text2 = \"He married a beautiful car.\"\n",
    "\n",
    "ppl1 = compute_perplexity(text1)\n",
    "ppl2 = compute_perplexity(text2)\n",
    "\n",
    "print(f\"Perplexity of first sentence: {ppl1:.2f}\")\n",
    "print(f\"Perplexity of second sentence: {ppl2:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
