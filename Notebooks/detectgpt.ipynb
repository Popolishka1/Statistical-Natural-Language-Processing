{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model list (all tested)\n",
    "# gpt2\n",
    "# gpt2-large\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "MODEL_NAME = \"gpt2-large\" \n",
    "\n",
    "device,_,_ = get_backend()\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Output AI-generated text using the chosen model\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): the maximum length (~ number of words) of the generated text\n",
    "\n",
    "    Returns:\n",
    "        str: generated text\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the log prob of a given text under the chosen model\n",
    "\n",
    "    Args:\n",
    "        text (str): input text for which to compute the log prob\n",
    "\n",
    "    Returns:\n",
    "        float: input text log prob\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    n_tokens = input_ids.shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, labels=input_ids)\n",
    "        # negative of the NLL per token = log prob\n",
    "        log_prob = -outputs.loss.item() * n_tokens # total NLL\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the perplexity score of a given text using the chosen model\n",
    "\n",
    "    Args:\n",
    "        text (str): input text for which to compute perplexity\n",
    "\n",
    "    Returns:\n",
    "        float: text perplexity score \n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, labels=input_ids)\n",
    "        log_prob = outputs.loss # NLL per token\n",
    "        \n",
    "    perplexity = torch.exp(log_prob) if log_prob < 100 else float(\"inf\") # overflow possible\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_perturbation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Randomly swaps two adjacent words to create a perturbed version of the text\n",
    "\n",
    "    Args:\n",
    "        text (str): the input text to be perturbed\n",
    "\n",
    "    Returns:\n",
    "        str: the perturbed text \n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) > 3:\n",
    "        i = random.randint(0, len(words)-2)\n",
    "        words[i], words[i + 1] = words[i + 1], words[i]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement the T5 perturbation of the paper\n",
    "\n",
    "def perturbation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    T5 perturbation\n",
    "\n",
    "    Args:\n",
    "        text (str): the input text to be perturbed\n",
    "\n",
    "    Returns:\n",
    "        str: the perturbed text \n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectgpt_score(text: str, num_perturbations: int) -> bool:\n",
    "    \"\"\"\n",
    "    Implement DetectGPT algorithm 1\n",
    "\n",
    "    Args:\n",
    "        text (str): input text to be analyzed\n",
    "        num_perturbations (int): number of perturbed versions of the text to generate\n",
    "\n",
    "    Returns:\n",
    "        bool: true if the text is likely model-generated\n",
    "    \"\"\"\n",
    "    original_log_prob = compute_log_prob(text) # log prob of the original text\n",
    "\n",
    "    # Generate perturbed texts + compute their log prob\n",
    "    perturbed_texts = [dummy_perturbation(text) for _ in range(num_perturbations)]\n",
    "    perturbed_log_probs = [compute_log_prob(pt) for pt in perturbed_texts]\n",
    "\n",
    "    mu = sum(perturbed_log_probs) / num_perturbations # mean log probability of the perturbed texts\n",
    "\n",
    "    d = original_log_prob - mu # estimate perturbation discrepancy d\n",
    "\n",
    "    variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (num_perturbations - 1) # variance of the log probabilities\n",
    "    sigma = variance ** 0.5  # standard deviation\n",
    "\n",
    "    score = d / sigma if sigma > 0 else 0 \n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "max_length = 60\n",
    "prompt = \"In a faraway galaxy, where no humans exist\"\n",
    "\n",
    "\n",
    "ai_text = generate_text(prompt,max_length)\n",
    "\n",
    "# Human text from CNN\n",
    "human_text = \"But Bhaduri found it increasingly hard to secure work after more women began partaking in jatra productions in the 1960s and 1970s. By the time he met Kishore, who was running a theater publication at the time, the actor was in his 60s and only performing a handful of times a year for the equivalent of $1 a night.\"\n",
    "\n",
    "print(\"AI text:\", ai_text)\n",
    "print(\"human text:\", human_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_perturbations = 10\n",
    "ai_text_score = detectgpt_score(ai_text,num_perturbations)\n",
    "human_text_score = detectgpt_score(human_text,num_perturbations)\n",
    "\n",
    "print(\"AI text DetectGPT score:\", ai_text_score)\n",
    "print(\"Human text DetectGPT score:\", human_text_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perplexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity for AI-generated text and human text\n",
    "\n",
    "max_length = 60\n",
    "prompt = \"In a faraway galaxy, where no humans exist\"\n",
    "ai_text = generate_text(prompt, max_length)\n",
    "\n",
    "human_text = \"But Bhaduri found it increasingly hard to secure work after more women began partaking in jatra productions in the 1960s and 1970s. By the time he met Kishore, who was running a theater publication at the time, the actor was in his 60s and only performing a handful of times a year for the equivalent of $1 a night.\"\n",
    "\n",
    "# Compute perplexity for AI-generated text\n",
    "perplexity_ai = compute_perplexity(ai_text)\n",
    "perplexity_ai_perturbed = compute_perplexity(dummy_perturbation(ai_text))\n",
    "\n",
    "# Compute perplexity for human-written text\n",
    "perplexity_human = compute_perplexity(human_text)\n",
    "perplexity_human_perturbed = compute_perplexity(dummy_perturbation(human_text))\n",
    "\n",
    "print(f\"Perplexity AI: {perplexity_ai}\")\n",
    "print(f\"Perplexity AI (perturbed): {perplexity_ai_perturbed}\")\n",
    "print(f\"Perplexity human: {perplexity_human}\")\n",
    "print(f\"Perplexity human (perturbed): {perplexity_human_perturbed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot analysis\n",
    "num_samples = 100\n",
    "max_length = 60\n",
    "prompts = [\"In a faraway galaxy, where no humans exist\" for _ in range(num_samples)]\n",
    "\n",
    "# Generate AI-generated texts based on the prompts\n",
    "ai_texts = [generate_text(prompt, max_length) for prompt in prompts]\n",
    "\n",
    "# Compute perplexity before and after perturbation\n",
    "perplexities_before = [compute_perplexity(text) for text in ai_texts]\n",
    "perplexities_after = [compute_perplexity(dummy_perturbation(text)) for text in ai_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.scatter(perplexities_before, perplexities_after,marker='x',c='r')\n",
    "\n",
    "plt.xlabel(\"Perplexity before perturbation\")\n",
    "plt.ylabel(\"Perplexity after perturbation\")\n",
    "plt.plot([0,30],[0,30],'b--')\n",
    "plt.title(f\"Perplexity before vs after perturbation for {num_samples} AI-generated texts\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
