{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT**: Identifying AI-generated text\n",
    "This notebook implements the **DetectGPT** method from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- **Model setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a simple setup of different transformer based models that will be needed to:\n",
    "1. produce the AI-generated text - ``generation_model``\n",
    "2. compute the log-probablities - ``computation_model``\n",
    "3. perturb the text with the T5 perturbation - ``t5_model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "\n",
    "GENERATION_MODEL_NAME = \"openai-community/gpt2-xl\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "generation_model_kwargs = {}\n",
    "if 'gpt-j' in GENERATION_MODEL_NAME or 'neox' in GENERATION_MODEL_NAME:\n",
    "    generation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in GENERATION_MODEL_NAME:\n",
    "    generation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(GENERATION_MODEL_NAME, **generation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "generation_tokenizer.model_max_length = 1024 # what is the impact for generation ??\n",
    "\n",
    "if generation_tokenizer.pad_token is None:\n",
    "    generation_tokenizer.pad_token = generation_tokenizer.eos_token\n",
    "generation_tokenizer.pad_token_id = generation_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"openai-community/gpt2-xl\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "PERTURBATION_MODEL_NAME = \"google-t5/t5-3b\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# google-t5/t5-large - used for the experiment assessing impact of n_perturbations\n",
    "# google-t5/t5-3b - general case use\n",
    "# google-t5/t5-11b - use for gpt-neox or gpt3\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "# Load model\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(PERTURBATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(PERTURBATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "t5_tokenizer.model_max_length = 512 \n",
    "\n",
    "t5_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- **Code setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **🔀 Text perturbation** *NEW VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the **T5-based perturbation function**, which modifies the input text slightly while preserving its meaning. \n",
    "\n",
    "- **Why is perturbation needed?** AI-generated text often sits in **low-curvature** probability regions, meaning slight perturbations can significantly change their log probabilities\n",
    "- **How does it work?** The **T5 model** introduces variations to the text and helps in detecting AI-generated content\n",
    "\n",
    "These perturbed texts will later be compared to their original versions to compute the discrepancy scores d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text: str, span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Tokenizes a text and applies masking by replacing certain spans with placeholder tokens\n",
    "\n",
    "    Args:\n",
    "        text (str): input text to be tokenized and masked\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool - optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Return:\n",
    "        text (str): masked text with placeholder token\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    # Calculate number of masked spans\n",
    "    n_spans = pct * len(tokens) / (span_length + buffer_size * 2)\n",
    "\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - buffer_size)\n",
    "        search_end = min(len(tokens), end + buffer_size)\n",
    "\n",
    "        # Ensure no overlapping masks in buffer region\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # Replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def count_masks(texts: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Counts the number of mask tokens in each text\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts containing mask tokens (format: \"<extra_id_N>\" where N is any int)\n",
    "\n",
    "    Returns:\n",
    "        n_masks (list): list where each element represents the number of mask tokens in the corresponding text\n",
    "    \"\"\"\n",
    "    n_masks = [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "    return n_masks\n",
    "\n",
    "def replace_masks(texts: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Replaces masked spans in texts with generated text using a T5 model\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts containing mask tokens (format: \"<extra_id_N>\" where N is any int)\n",
    "\n",
    "    Returns:\n",
    "        texts_replaced (list): list of texts where masked spans have been replaced with generated content\n",
    "    \"\"\"\n",
    "    n_expected = count_masks(texts) # Count number of masks per text\n",
    "    stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0] # Define stopping condition\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    tokens = t5_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(DEVICE)\n",
    "    \n",
    "    # Generate replacements for the masks using T5\n",
    "    outputs = t5_model.generate(\n",
    "        **tokens, \n",
    "        max_length=150, \n",
    "        do_sample=True, \n",
    "        top_p=1, \n",
    "        num_return_sequences=1, \n",
    "        eos_token_id=stop_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    texts_replaced = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    return texts_replaced\n",
    "\n",
    "# Define a regex pattern to match all placeholder tokens in the format <extra_id_N> (where N is any int)\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "def extract_fills(texts: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts the generated text fills from masked texts\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts where masked spans have been replaced with generated text\n",
    "\n",
    "    Returns:\n",
    "        extracted_fills (list): a list of lists, where each inner list contains the extracted fills for the corresponding input text\n",
    "    \"\"\"\n",
    "    # Remove \"<pad>\" and \"</s>\" tokens from the beginning/end of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # Extract the text between mask tokens (pattern should be defined elsewhere)\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # Trim whitespace from each extracted fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "def apply_extracted_fills(masked_texts: list[str], extracted_fills: list[list[str]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Replaces mask tokens in masked texts with corresponding extracted fills\n",
    "\n",
    "    Args:\n",
    "        masked_texts (list): list of texts containing mask tokens\n",
    "        extracted_fills (list): list of lists, where each inner list contains the extracted fill text for the corresponding masked text\n",
    "\n",
    "    Returns:\n",
    "        texts (list): list of texts with all masks replaced by their corresponding extracted fills\n",
    "    \"\"\"\n",
    "    # Split masked text into tokens, keeping spaces intact\n",
    "    tokens = [x.split(' ') for x in masked_texts]\n",
    "\n",
    "    # Count expected number of masks per text\n",
    "    n_expected = count_masks(masked_texts)\n",
    "\n",
    "    # Replace each mask token with the corresponding extracted fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = [] # Empty text if not enough fills are available\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                if f\"<extra_id_{fill_idx}>\" in text:\n",
    "                    text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # Join tokens back into text format\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts\n",
    "\n",
    "def perturb_texts_(texts: list[str], span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies T5-based perturbation to a list of texts by masking spans, generating replacements,and applying the generated fills\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of input texts to be perturbed\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool, optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        perturbed_texts (list): list of perturbed texts\n",
    "    \"\"\"\n",
    "    # Step 1: Mask spans in the input texts\n",
    "    masked_texts = [tokenize_and_mask(x, span_length, pct, buffer_size, ceil_pct) for x in texts]\n",
    "    print(f\"Masked texts: {masked_texts}\")\n",
    "\n",
    "    # Step 2: Generate replacement texts\n",
    "    raw_fills = replace_masks(masked_texts)\n",
    "    print(f\"Raw fills: {raw_fills}\")\n",
    "\n",
    "    # Step 3: Extract only the generated fills\n",
    "    extracted_fills = extract_fills(raw_fills)\n",
    "    print(f\"Extracted fills: {extracted_fills}\")\n",
    "\n",
    "    # Step 4: Apply the extracted fills to reconstruct the perturbed texts\n",
    "    perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "    print(f\"Perturbed texts: {perturbed_texts}\")\n",
    "    print(f\"Original texts: {texts}\")\n",
    "\n",
    "    # Handle cases where the model doesn't generate the correct number of fills\n",
    "    attempts = 1\n",
    "    while '' in perturbed_texts:\n",
    "        idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
    "        print(f'WARNING: {len(idxs)} texts have no fills. Retrying [attempt {attempts}].')\n",
    "\n",
    "        # Retry perturbation for failed cases\n",
    "        masked_texts = [tokenize_and_mask(texts[idx], span_length, pct, buffer_size, ceil_pct) for idx in idxs]\n",
    "        raw_fills = replace_masks(masked_texts)\n",
    "        extracted_fills = extract_fills(raw_fills)\n",
    "        new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "\n",
    "        # Update perturbed texts\n",
    "        for idx, new_text in zip(idxs, new_perturbed_texts):\n",
    "            perturbed_texts[idx] = new_text\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    return perturbed_texts\n",
    "\n",
    "def perturb_texts(texts: list[str], batch_size: int, span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies T5-based perturbation to a list of texts (in chunks for efficiency)\n",
    "\n",
    "    Note: wrapper function around `perturb_texts_`\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of input texts to be perturbed\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool, optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        outputs (list): list of perturbed texts\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size): # Process texts in batches of for efficiency\n",
    "        batch = texts[i:i + batch_size]\n",
    "        perturbed_batch = perturb_texts_(batch, span_length, pct, buffer_size, ceil_pct=ceil_pct)\n",
    "        outputs.extend(perturbed_batch)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll(text):\n",
    "    with torch.no_grad():\n",
    "        tokenized = computation_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
    "        labels = tokenized.input_ids\n",
    "        output = computation_model(**tokenized, labels=labels)\n",
    "        avg_ll = -output.loss.item() # Loss is already averaged over tokens\n",
    "    return avg_ll\n",
    "\n",
    "def get_lls(texts):\n",
    "    return [get_ll(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(text):\n",
    "    with torch.no_grad():\n",
    "        tokenized = computation_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
    "        labels = tokenized.input_ids\n",
    "        loss = computation_model(**tokenized, labels=labels).loss.item() \n",
    "    return float(np.exp(loss))\n",
    "\n",
    "def get_ppls(texts):\n",
    "    return [get_ppl(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list,\n",
    "                                log_probs_per_text_transformed: list,\n",
    "                                normalization: bool=False) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts. Computed for n_perturbations perturbations.\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the n_perturbations perturbed log probs\n",
    "        normalization (bool): True if you want to normalize the discrepancy scores, False otherwise\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texts\n",
    "    \"\"\"\n",
    "    n_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "        n_perturbations = len(perturbed_log_probs) # Number of perturbations\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = sum(perturbed_log_probs) / n_perturbations  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (n_perturbations - 1)\n",
    "            sigma = variance ** 0.5\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else 0\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **🔍 Main functions: *DetectGPT* Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the **DetectGPT method**.\n",
    "\n",
    "- **Key idea:** once again, AI-generated texts often **reside in low-curvature probability regions**.\n",
    "- **How does it work?**\n",
    "  - We perturb the text multiple times (``num_perturbation``). We will use ``n_samples`` texts with ``max_length`` words.\n",
    "  - Compute log probabilities for both **original** and **perturbed** texts\n",
    "  - Measure the **discrepancy score** (a higher score suggests AI-generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_processing(data: list, \n",
    "                         n_samples: int,\n",
    "                         batch_size: int, \n",
    "                         max_length: int, \n",
    "                         n_perturbations: int, \n",
    "                         span_length: int, \n",
    "                         pct: float, \n",
    "                         buffer_size: int)-> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Compute log probabilities for original and perturbed texts.\n",
    "    \n",
    "    This function processes multiple text samples, computes their log probabilities, \n",
    "    applies perturbations to the texts, and then computes the log probabilities \n",
    "    of the perturbed versions.\n",
    "\n",
    "    Args:\n",
    "        data (list): list of dictionaries containing text (e.g. [{\"text\": \"sample text\"}, ...])\n",
    "        n_samples (int): number of texts to process\n",
    "        batch_size (int): batch to group and proceed the perturbations (code speed up)\n",
    "        max_length (int): maximum number of words to consider in each text\n",
    "        n_perturbations (int): number of perturbations applied to each text\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: \n",
    "            - log_probs_per_text_base (list): log probs of the original texts\n",
    "            - log_probs_per_text_transformed (list of lists): log probs of the perturbed texts,\n",
    "              structured as a list where each element corresponds to a text and contains \n",
    "              a list of its perturbed log probs\n",
    "    \"\"\"\n",
    "    log_probs_per_text_transformed = []\n",
    "\n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "    # Calculate log probabilities of the original texts\n",
    "    log_probs_per_text_base = get_lls(texts=original_texts)\n",
    "\n",
    "    for perturbation_idx in tqdm(range(n_perturbations), desc=f\"Processing {n_perturbations} perturbations for {n_samples} texts. Perturbation number\"):\n",
    "        # Apply perturbation\n",
    "        all_perturbed_texts = perturb_texts(texts=original_texts,\n",
    "                                            batch_size=batch_size,\n",
    "                                            span_length=span_length,\n",
    "                                            pct=pct,\n",
    "                                            buffer_size=buffer_size,\n",
    "                                            ceil_pct=False)\n",
    "        \n",
    "        # Calculate log probabilities of the perturbed texts\n",
    "        all_log_probs = get_lls(texts=all_perturbed_texts)\n",
    "\n",
    "        # Organize results\n",
    "        for j in range(n_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "\n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to optimized_processing function that allows you to save down the perturbed texts\n",
    "\n",
    "def gen_perturbed(data: list,\n",
    "                  n_samples: int=200,\n",
    "                  max_length: int=512,\n",
    "                  n_perturbations: int=100,\n",
    "                  batch_size: int=128,\n",
    "                  span_length: int=2,\n",
    "                  pct: float=0.3,\n",
    "                  buffer_size: int=1):\n",
    "    \"\"\"\n",
    "    Generates pertubations for texts in data. \n",
    "    Returns list of length n_pertubations, each entry being a JSON object with perturbed text.\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        The output is a list of length n_pertubations, each entry being a list of length num_samples, \n",
    "        each entry being a string. The outer list is over the perturbations, the inner list is over the samples, \n",
    "        and the string is the perturbed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise list to store all perturbed JSON\n",
    "    all_perturbed_texts = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)] # Truncate text to max_length, returns list of strings as before\n",
    "    \n",
    "    # Iterate for length num_pertubation\n",
    "    for _ in tqdm(range(n_perturbations), desc=f\"Processing {n_perturbations} perturbations for {n_samples} texts. Perturbation number\"):\n",
    "\n",
    "        # Randomly select 15% of text to mask, creates pertubations\n",
    "        perturbed_texts = perturb_texts(texts=original_texts,\n",
    "                                        batch_size=batch_size,\n",
    "                                        span_length=span_length,\n",
    "                                        pct=pct,\n",
    "                                        buffer_size=buffer_size,\n",
    "                                        ceil_pct=False\n",
    "                                        )\n",
    "\n",
    "        # Store in list\n",
    "        all_perturbed_texts.append(perturbed_texts)\n",
    "    \n",
    "    return all_perturbed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_log_prob(original_texts, all_perturbed_texts, n_perturbation=100):\n",
    "    \"\"\"\n",
    "    Compares log probs of original text vs list of JSONs perturbed texts\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        log_probs_per_text_base is a list of length num_samples, each entry being the log prob of the corresponding original text\n",
    "        log_probs_per_text_pert is a list of length num_samples, each entry being a list of length num_pertubations, each entry being the log prob of the corresponding perturbed text\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialise list to store log prob of each perturbed JSON\n",
    "    \n",
    "    log_probs_per_text_pert = []\n",
    "\n",
    "    # Get num_samples\n",
    "    num_samples = len(original_texts)\n",
    "\n",
    "    # Calculate log prob for original text in batch\n",
    "    log_probs_per_text_base = get_lls(texts=original_texts)\n",
    "\n",
    "    # Iterate \n",
    "    for perturbation_idx in tqdm(range(n_perturbation)):\n",
    "\n",
    "        # Get the JSON file\n",
    "        perturbed_texts = all_perturbed_texts[perturbation_idx]\n",
    "\n",
    "        # Calculate log prob\n",
    "        log_probs_per_text_tran = get_lls(texts=perturbed_texts)\n",
    "\n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_pert.append([])\n",
    "            log_probs_per_text_pert[j].append(log_probs_per_text_tran[j])\n",
    "        \n",
    "    return log_probs_per_text_base, log_probs_per_text_pert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- **Data loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📌 Dataset format guidelines**\n",
    "\n",
    "All datasets (human-written and AI-generated) must follow this format:\n",
    "\n",
    "- Stored as a **`.jsonl`** where each line is a dictionary.\n",
    "- Each entry contains (minimum requirement):\n",
    "  - `\"text\"`: the text content\n",
    "  - `\"model\"`: for human text please label it as `\"human\"` and for AI-generated texts, please specify the model used (e.g. ``\"gpt2-large\"``)\n",
    "  - `\"source\"`: the origin of the text (e.g., `\"wikihow\"`, `\"reddit\"`, `\"news articles\"`)\n",
    "\n",
    "#### Exemple (as in ``subtaskB_train.jsonl`` located in `Datasets\\SemEval2024-Task8`):\n",
    "```json\n",
    "{\"text\": \"A groundbreaking discovery in physics was made today.\", \"model\": \"human\", \"source\": \"news articles\"}\n",
    "{\"text\": \"The AI revolution is shaping the future of work.\", \"model\": \"chatGPT\", \"source\": \"AI Generated\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Human texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemEval dataset\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_human_semeval = []\n",
    "\n",
    "# Efficiently process the file line by line\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") == \"human\":\n",
    "            data_human_semeval.append(record)\n",
    "\n",
    "# Display first few rows of the list\n",
    "display(data_human_semeval[:5]) # Display first 5 rows as sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XSum dataset\n",
    "\n",
    "n_samples = 2000\n",
    "FILE_PATH = \"EdinburghNLP/xsum\"\n",
    "\n",
    "# Load dataset using Hugging Face's datasets library\n",
    "dataset = load_dataset(FILE_PATH, split=\"train\")\n",
    "\n",
    "# Keep only the first 200 rows\n",
    "dataset = dataset.select(range(n_samples)) # CHANGE THIS TO CONTROL THE NUMBER OF DATAPOINTS TO USE\n",
    "\n",
    "# Convert into a list of dictionaries \n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Adapt the dataframe to the desired style\n",
    "df = df.rename(columns={\"document\": \"text\"}).drop(columns=[\"summary\"])  # Rename and drop columns\n",
    "df[\"model\"] = \"human\"   # Add a new column to track that all these documents are human-generated\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "data_human_xsum = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Display first few rows of the list\n",
    "display(data_human_xsum[:5]) # Display first 5 rows as sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **AI-generated texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. *Option 1: produce own AI-generated texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate AI text from a given prompt\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): max length of generated text\n",
    "\n",
    "    Returns:\n",
    "        cleaned_text (str): cleaned generated text\n",
    "    \"\"\"\n",
    "    inputs = generation_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = generation_model.generate(**inputs, max_length=max_length, do_sample=True, temperature=1)\n",
    "        # extra arguemnts to think about: \n",
    "        # min_length=150\n",
    "        # max_length=200\n",
    "        # top_p=0.96\n",
    "        # top_k=40\n",
    "        # pad_token_id=generation_model.eos_token_id\n",
    "        # eos_token_id=generation_model.eos_token_id\n",
    "\n",
    "    generated_text = generation_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text # The output is a string of the generated text\n",
    "\n",
    "def generate_prompt(prompt_list: list, n_tokens: int=30) -> list:\n",
    "    \"\"\"\n",
    "    Truncate the 'text' field in each dictionary to the first 30 tokens.\n",
    "\n",
    "    Args:\n",
    "        prompt_list (list): list of dictionaries with keys 'text', 'id', and 'model'\n",
    "\n",
    "    Returns:\n",
    "        truncated_prompts (list): updated list with truncated 'text' values\n",
    "    \"\"\"\n",
    "    truncated_prompts = []\n",
    "    for entry in prompt_list:\n",
    "        tokens = generation_tokenizer.tokenize(entry['text'])\n",
    "        truncated_text = generation_tokenizer.convert_tokens_to_string(tokens[:n_tokens])\n",
    "        truncated_prompts.append({\n",
    "            \"text\": truncated_text,\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"model\": entry[\"model\"]\n",
    "        })\n",
    "    return truncated_prompts # The output is a list of dictionaries with keys 'text', 'id', and 'model'\n",
    "\n",
    "\n",
    "def generate_dataset(prompt_list: list, max_length: int, n_tokens: int, generation_model_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a dataset of AI-generated texts based on the given original prompt list.\n",
    "\n",
    "    Args:\n",
    "        prompt_list (list): Original list of dictionaries with full 'text'.\n",
    "        max_length (int): Maximum length of each generated text.\n",
    "\n",
    "    Returns:\n",
    "        list: Dataset of AI-generated texts (list of dictionaries).\n",
    "    \"\"\"\n",
    "    truncated_prompts = generate_prompt(prompt_list=prompt_list,n_tokens=n_tokens) # Apply generate_prompt within generate_dataset\n",
    "\n",
    "    data_ai = [\n",
    "        {\n",
    "            \"text\": generate_text(prompt=entry[\"text\"], max_length=max_length),\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"model\": generation_model_name\n",
    "        }\n",
    "        for entry in truncated_prompts\n",
    "    ]\n",
    "    return data_ai\n",
    "    # The output is a list of dictionaries, each dictionary has keys 'text', 'id', and 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model.to(DEVICE)\n",
    "print(DEVICE)\n",
    "\n",
    "n_samples_ai_generation = 500\n",
    "PROMPT = data_human_xsum[:n_samples_ai_generation] # data_human_semeval[:n_samples_ai_generation]\n",
    "max_length = 250\n",
    "n_tokens = 30\n",
    "\n",
    "clear_cuda_cache()\n",
    "data_ai_generated = generate_dataset(prompt_list=PROMPT, max_length=max_length, n_tokens=n_tokens, generation_model_name=GENERATION_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the AI-generated dataset in the correct .jsonl format\n",
    "\n",
    "def save_ai_dataset_jsonl(data, file_name_base, generation_model_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in JSONL format in a structured directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of dictionaries to be saved.\n",
    "        file_name_base (str): Base name for the file.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_generated_by_{generation_model_name}_{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Data\", \"AI-Generated\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and check the AI-generated dataset\n",
    "\n",
    "# Save the AI-generated dataset\n",
    "save_ai_dataset_jsonl(data_ai_generated, f\"XSUM_{n_samples_ai_generation}_samples\", GENERATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_generated[0] if data_ai_generated else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. *Option 2: load AI-generated texts from a dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemEval dataset\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_ai_dataset = []\n",
    "\n",
    "# Read entire file and parse as JSON list\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") != \"human\":\n",
    "            data_ai_dataset.append(record)\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_dataset[0] if data_ai_dataset else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Sanity check: test the log probs and perplexity scores computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human_xsum[67:] # data_human_semeval\n",
    "n_samples = 100\n",
    "max_length = 250\n",
    "\n",
    "original_texts = [\" \".join(DATA[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "clear_cuda_cache()\n",
    "lls = get_lls(texts=original_texts)\n",
    "ppls = get_ppls(texts=original_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Sanity check: test the perturbations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human_semeval # data_human_xsum\n",
    "n_samples = 100\n",
    "batch_size = 20\n",
    "max_length = 250\n",
    "\n",
    "original_texts = [\" \".join(DATA[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "clear_cuda_cache()\n",
    "perturbed_texts = perturb_texts(texts=original_texts, batch_size=batch_size, span_length=2,pct=0.3,buffer_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- **Exemple usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Gian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the perturbations in the correct .jsonl format\n",
    "\n",
    "def save_perturbations_jsonl(all_perturbed_texts, file_name_base, generation_model_name, perturbation_model_name):\n",
    "    \"\"\"\n",
    "    Saves perturbed texts in JSONL format, where each line is a list of perturbed texts (one perturbation batch).\n",
    "\n",
    "    Parameters:\n",
    "        all_perturbed_texts (list of lists): List of lists containing perturbed texts.\n",
    "        file_name_base (str): Base name for the file.\n",
    "        generation_model_name (str): Name of the model that generated the original texts.\n",
    "        perturbation_model_name (str): Name of the model that generated the perturbations.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_generated_by_{generation_model_name}_perturbed_by_{perturbation_model_name}_{timestamp}.jsonl\"\n",
    "    file_relative_path = os.path.join(\"Data\", \"Perturbations\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for perturbed_texts in all_perturbed_texts:\n",
    "            f.write(json.dumps(perturbed_texts) + \"\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the results in the correct .jsonl format\n",
    "\n",
    "def save_results_jsonl(data, file_name_base, generation_model_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in JSONL format in a structured directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of results to be saved.\n",
    "        file_name_base (str): Base name for the file.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_{generation_model_name}_{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Results\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Generate perturbations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATA = data_human_xsum\n",
    "\n",
    "# Experiment setup\n",
    "n_samples = 500\n",
    "batch_size = 20\n",
    "max_length = 250\n",
    "\n",
    "# Perturbation setup\n",
    "n_perturbations = 100\n",
    "span_length = 2\n",
    "pct = 0.3\n",
    "buffer_size = 1\n",
    "\n",
    "clear_cuda_cache()\n",
    "\n",
    "# Compute log probs before and after perturbation\n",
    "log_probs_base_human, log_probs_transformed_human = optimized_processing(\n",
    "    data=DATA,\n",
    "    n_samples=n_samples,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    n_perturbations=n_perturbations,\n",
    "    span_length=span_length,\n",
    "    pct=pct,\n",
    "    buffer_size=buffer_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = False # True if you want to normalize the discrepancy scores\n",
    "\n",
    "# Compute discrepancy scores\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=normalization)\n",
    "\n",
    "# Store all results\n",
    "results_human = {}\n",
    "results_human[\"log_probs_base\"] = log_probs_base_human\n",
    "results_human[\"log_probs_transformed\"] = log_probs_transformed_human\n",
    "results_human[\"discrepancy_scores\"] = discrepancy_scores_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_human, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATA = data_ai_dataset # DATA = data_ai_generated\n",
    "\n",
    "# Experiment setup\n",
    "n_samples = 500\n",
    "batch_size = 20\n",
    "max_length = 250\n",
    "\n",
    "# Perturbation setup\n",
    "n_perturbations = 100\n",
    "span_length = 2\n",
    "pct = 0.3\n",
    "buffer_size = 1\n",
    "\n",
    "# Compute log probs before and after perturbation\n",
    "clear_cuda_cache()\n",
    "log_probs_base_ai, log_probs_transformed_ai = optimized_processing(\n",
    "    data=DATA,\n",
    "    n_samples=n_samples,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    n_perturbations=n_perturbations,\n",
    "    span_length=span_length,\n",
    "    pct=pct,\n",
    "    buffer_size=buffer_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = False # True if you want to normalize the discrepancy scores\n",
    "\n",
    "# Compute discrepancy scores\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=normalization)\n",
    "\n",
    "# Store all results\n",
    "results_ai = {}\n",
    "results_ai[\"log_probs_base\"] = log_probs_base_ai\n",
    "results_ai[\"log_probs_transformed\"] = log_probs_transformed_ai\n",
    "results_ai[\"discrepancy_scores\"] = discrepancy_scores_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_ai, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Histograms of the discrepancy scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION = False # True if you want to normalize the discrepancy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI texts results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\\\results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    data_ai = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base_ai = data_ai[\"log_probs_base\"]\n",
    "log_probs_transformed_ai = data_ai[\"log_probs_transformed\"]\n",
    "\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human texts results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\\\results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    data_human = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base_human = data_human[\"log_probs_base\"]\n",
    "log_probs_transformed_human = data_human[\"log_probs_transformed\"]\n",
    "\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def optimal_threshold(list1, list2):\n",
    "    X = np.concatenate([list1, list2]).reshape(-1, 1)\n",
    "    y = np.concatenate([np.zeros(len(list1)),np.ones(len(list2))])\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    best_threshold = -clf.intercept_[0] / clf.coef_[0][0]\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = (clf.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    return best_threshold, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold,auroc = optimal_threshold(discrepancy_scores_human, discrepancy_scores_ai)\n",
    "print(f\"Optimal threshold: {threshold:.2f}\")\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "\n",
    "plt.hist(discrepancy_scores_human, bins=15, alpha=0.5, label='Human', edgecolor='black', density=True)\n",
    "plt.hist(discrepancy_scores_ai, bins=15, alpha=0.5, label='AI', edgecolor='black', density=True)\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "plt.xlabel(f'Discrepancy scores (normalization={normalization})')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Computation model: {COMPUTATION_MODEL_NAME}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
