{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT for dummies**: Identifying AI-generated text\n",
    "This notebook implements the **DetectGPT** method from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- **Model setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a simple setup of different transformer based models that will be needed to:\n",
    "1. produce the AI-generated text - ``generation_model``\n",
    "2. compute the log-probablities - ``computation_model``\n",
    "3. perturb the text with the T5 perturbation - ``t5_model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "\n",
    "GENERATION_MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "generation_model_kwargs = {}\n",
    "if 'gpt-j' in GENERATION_MODEL_NAME or 'neox' in GENERATION_MODEL_NAME:\n",
    "    generation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in GENERATION_MODEL_NAME:\n",
    "    generation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(GENERATION_MODEL_NAME, **generation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "generation_tokenizer.pad_token_id = generation_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "PERTURBATION_MODEL_NAME = \"google-t5/t5-3b\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# google-t5/t5-large - used for the experiment assessing impact of n_perturbations\n",
    "# google-t5/t5-3b - general case use\n",
    "# google-t5/t5-11b - use for gpt-neox or gpt3\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "# Load model\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(PERTURBATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "n_position = 512\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(PERTURBATION_MODEL_NAME, model_max_length=n_position, cache_dir=CACHE_DIR)\n",
    "\n",
    "t5_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- **Code setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **🔀 Text perturbation** *NEW VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the **T5-based perturbation function**, which modifies the input text slightly while preserving its meaning. \n",
    "\n",
    "- **Why is perturbation needed?** AI-generated text often sits in **low-curvature** probability regions, meaning slight perturbations can significantly change their log probabilities\n",
    "- **How does it work?** The **T5 model** introduces variations to the text and helps in detecting AI-generated content\n",
    "\n",
    "These perturbed texts will later be compared to their original versions to compute the discrepancy scores d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text: str, span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Tokenizes a text and applies masking by replacing certain spans with placeholder tokens\n",
    "\n",
    "    Args:\n",
    "        text (str): input text to be tokenized and masked\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool - optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Return:\n",
    "        text (str): masked text with placeholder token\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    # Calculate number of masked spans\n",
    "    n_spans = pct * len(tokens) / (span_length + buffer_size * 2)\n",
    "\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - buffer_size)\n",
    "        search_end = min(len(tokens), end + buffer_size)\n",
    "\n",
    "        # Ensure no overlapping masks in buffer region\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # Replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def count_masks(texts: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Counts the number of mask tokens in each text\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts containing mask tokens (format: \"<extra_id_N>\" where N is any int)\n",
    "\n",
    "    Returns:\n",
    "        n_masks (list): list where each element represents the number of mask tokens in the corresponding text\n",
    "    \"\"\"\n",
    "    n_masks = [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "    return n_masks\n",
    "\n",
    "def replace_masks(texts: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Replaces masked spans in texts with generated text using a T5 model\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts containing mask tokens (format: \"<extra_id_N>\" where N is any int)\n",
    "\n",
    "    Returns:\n",
    "        (list): list of texts where masked spans have been replaced with generated content\n",
    "    \"\"\"\n",
    "    n_expected = count_masks(texts) # Count number of masks per text\n",
    "    stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0] # Define stopping condition\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    tokens = t5_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "    # Generate replacements for the masks using T5\n",
    "    outputs = t5_model.generate(\n",
    "        **tokens, \n",
    "        max_length=150, \n",
    "        do_sample=True, \n",
    "        top_p=1, \n",
    "        num_return_sequences=1, \n",
    "        eos_token_id=stop_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    texts_replaced = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    return texts_replaced\n",
    "\n",
    "# Define a regex pattern to match all placeholder tokens in the format <extra_id_N> (where N is any int)\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "def extract_fills(texts: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts the generated text fills from masked texts\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts where masked spans have been replaced with generated text\n",
    "\n",
    "    Returns:\n",
    "        extracted_fills (list): a list of lists, where each inner list contains the extracted fills for the corresponding input text\n",
    "    \"\"\"\n",
    "    # Remove \"<pad>\" and \"</s>\" tokens from the beginning/end of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # Extract the text between mask tokens (pattern should be defined elsewhere)\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # Trim whitespace from each extracted fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "def apply_extracted_fills(masked_texts: list[str], extracted_fills: list[list[str]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Replaces mask tokens in masked texts with corresponding extracted fills\n",
    "\n",
    "    Args:\n",
    "        masked_texts (list): list of texts containing mask tokens\n",
    "        extracted_fills (list): list of lists, where each inner list contains the extracted fill text for the corresponding masked text\n",
    "\n",
    "    Returns:\n",
    "        texts (list): list of texts with all masks replaced by their corresponding extracted fills\n",
    "    \"\"\"\n",
    "    # Split masked text into tokens, keeping spaces intact\n",
    "    tokens = [x.split(' ') for x in masked_texts]\n",
    "\n",
    "    # Count expected number of masks per text\n",
    "    n_expected = count_masks(masked_texts)\n",
    "\n",
    "    # Replace each mask token with the corresponding extracted fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = []  # Empty text if not enough fills are available\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                if f\"<extra_id_{fill_idx}>\" in text:\n",
    "                    text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # Join tokens back into text format\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts\n",
    "\n",
    "def perturb_texts_(texts: list[str], span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies T5-based perturbation to a list of texts by masking spans, generating replacements,and applying the generated fills\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of input texts to be perturbed\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool, optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        perturbed_texts (list): list of perturbed texts\n",
    "    \"\"\"\n",
    "    # Step 1: Mask spans in the input texts\n",
    "    masked_texts = [tokenize_and_mask(x, span_length, pct, buffer_size, ceil_pct) for x in texts]\n",
    "    print(f\"Masked texts: {masked_texts}\")\n",
    "\n",
    "    # Step 2: Generate replacement texts\n",
    "    raw_fills = replace_masks(masked_texts)\n",
    "    print(f\"Raw fills: {raw_fills}\")\n",
    "\n",
    "    # Step 3: Extract only the generated fills\n",
    "    extracted_fills = extract_fills(raw_fills)\n",
    "    print(f\"Extracted fills: {extracted_fills}\")\n",
    "\n",
    "    # Step 4: Apply the extracted fills to reconstruct the perturbed texts\n",
    "    perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "    print(f\"Perturbed texts: {perturbed_texts}\")\n",
    "    print(f\"Original texts: {texts}\")\n",
    "\n",
    "    # Handle cases where the model doesn't generate the correct number of fills\n",
    "    attempts = 1\n",
    "    while '' in perturbed_texts:\n",
    "        idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
    "        print(f'WARNING: {len(idxs)} texts have no fills. Retrying [attempt {attempts}].')\n",
    "\n",
    "        # Retry perturbation for failed cases\n",
    "        masked_texts = [tokenize_and_mask(texts[idx], span_length, pct, buffer_size, ceil_pct) for idx in idxs]\n",
    "        raw_fills = replace_masks(masked_texts)\n",
    "        extracted_fills = extract_fills(raw_fills)\n",
    "        new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "\n",
    "        # Update perturbed texts\n",
    "        for idx, new_text in zip(idxs, new_perturbed_texts):\n",
    "            perturbed_texts[idx] = new_text\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    return perturbed_texts\n",
    "\n",
    "def perturb_texts(texts: list[str], batch_size: int, span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies T5-based perturbation to a list of texts (in chunks for efficiency)\n",
    "\n",
    "    Note: wrapper function around `perturb_texts_`\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of input texts to be perturbed\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool, optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        outputs (list): list of perturbed texts\n",
    "    \"\"\"\n",
    "    batch_size = batch_size # Process texts in batches of 20 for efficiency\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        perturbed_batch = perturb_texts_(batch, span_length, pct, buffer_size, ceil_pct=ceil_pct)\n",
    "        outputs.extend(perturbed_batch)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **🔍 Main functions: *DetectGPT* Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the **DetectGPT method**.\n",
    "\n",
    "- **Key idea:** once again, AI-generated texts often **reside in low-curvature probability regions**.\n",
    "- **How does it work?**\n",
    "  - We perturb the text multiple times (``num_perturbation``). We will use ``n_samples`` texts with ``max_length`` words.\n",
    "  - Compute log probabilities for both **original** and **perturbed** texts\n",
    "  - Measure the **discrepancy score** (a higher score suggests AI-generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lls(texts: list[str], batch_size: int=20) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute log prob for multiple texts (in batches)\n",
    "    \n",
    "    Args:\n",
    "        texts (list): list of input texts\n",
    "        batch_size (int): size of each batch (default=20)\n",
    "\n",
    "    Returns:\n",
    "        log_probs (list): list of log probs for each text\n",
    "    \"\"\"\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Tokenize the batch\n",
    "            tokenized = computation_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "            labels = tokenized.input_ids\n",
    "            \n",
    "            # Compute loss (NLL per token)\n",
    "            loss = computation_model(**tokenized, labels=labels).loss\n",
    "            \n",
    "            # Ensure loss is correctly shaped\n",
    "            if loss.dim() == 0:  # If it's a scalar loss (single value), expand to match batch size\n",
    "                loss = loss.repeat(len(batch))\n",
    "            \n",
    "            # Compute log-probs (negative loss) and convert properly\n",
    "            batch_log_probs = (-loss).cpu().numpy().tolist()\n",
    "            log_probs.extend(batch_log_probs)\n",
    "    \n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppls(texts: list[str], batch_size: int=20) -> list[float]:\n",
    "    \"\"\"\n",
    "    Compute perplexity for multiple texts (in batches)\n",
    "    \n",
    "    Args:\n",
    "        texts (list): list of input texts\n",
    "        batch_size (int): size of each batch (default=20)\n",
    "\n",
    "    Returns:\n",
    "        perplexity_scores (list): list of perplexity scores for each text\n",
    "    \"\"\"\n",
    "    perplexity_scores = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Tokenize the batch\n",
    "            tokenized = computation_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "            labels = tokenized.input_ids\n",
    "            \n",
    "            # Compute loss (NLL per token)\n",
    "            loss = computation_model(**tokenized, labels=labels).loss\n",
    "            \n",
    "            # Compute perplexity\n",
    "            batch_perplexity_scores = torch.exp(loss).cpu().numpy().tolist()\n",
    "            perplexity_scores.extend(batch_perplexity_scores)\n",
    "    \n",
    "    return perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list,\n",
    "                                log_probs_per_text_transformed: list,\n",
    "                                normalization: bool=False) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts. Computed for n_perturbations perturbations.\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the n_perturbations perturbed log probs\n",
    "        normalization (bool): True if you want to normalize the discrepancy scores, False otherwise\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texts\n",
    "    \"\"\"\n",
    "    n_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "        n_perturbations = len(perturbed_log_probs) # Number of perturbations\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = sum(perturbed_log_probs) / n_perturbations  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (n_perturbations - 1)\n",
    "            sigma = variance ** 0.5\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else 0\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_processing(data: list, \n",
    "                         n_samples: int,\n",
    "                         batch_size: int, \n",
    "                         max_length: int, \n",
    "                         n_perturbations: int, \n",
    "                         span_length: int, \n",
    "                         pct: float, \n",
    "                         buffer_size: int)-> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Compute log probabilities for original and perturbed texts.\n",
    "    \n",
    "    This function processes multiple text samples, computes their log probabilities, \n",
    "    applies perturbations to the texts, and then computes the log probabilities \n",
    "    of the perturbed versions.\n",
    "\n",
    "    Args:\n",
    "        data (list): list of dictionaries containing text (e.g. [{\"text\": \"sample text\"}, ...])\n",
    "        n_samples (int): number of texts to process\n",
    "        max_length (int): maximum number of words to consider in each text\n",
    "        n_perturbations (int): number of perturbations applied to each text\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: \n",
    "            - log_probs_per_text_base (list): log probs of the original texts\n",
    "            - log_probs_per_text_transformed (list of lists): log probs of the perturbed texts,\n",
    "              structured as a list where each element corresponds to a text and contains \n",
    "              a list of its perturbed log probs\n",
    "    \"\"\"\n",
    "    log_probs_per_text_transformed = []\n",
    "\n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "    # Calculate log probabilities of the original texts\n",
    "    log_probs_per_text_base = get_lls(texts=original_texts,batch_size=batch_size)\n",
    "\n",
    "    for perturbation_idx in tqdm(range(n_perturbations), desc=f\"Processing {n_perturbations} perturbations for {n_samples} texts. Perturbation number\"):\n",
    "        # Apply perturbation\n",
    "        all_perturbed_texts = perturb_texts(texts=original_texts,\n",
    "                                            batch_size=batch_size,\n",
    "                                            span_length=span_length,\n",
    "                                            pct=pct,\n",
    "                                            buffer_size=buffer_size,\n",
    "                                            ceil_pct=False)\n",
    "        \n",
    "        # Calculate log probabilities of the perturbed texts\n",
    "        all_log_probs = get_lls(texts=all_perturbed_texts,batch_size=batch_size)\n",
    "\n",
    "        # Organize results\n",
    "        for j in range(n_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "\n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to optimized_processing function that allows you to save down the perturbed texts\n",
    "\n",
    "def gen_perturbed(data: list,\n",
    "                  n_samples: int=200,\n",
    "                  max_length: int=512,\n",
    "                  n_perturbations: int=100,\n",
    "                  batch_size: int=128,\n",
    "                  span_length: int=2,\n",
    "                  pct: float=0.3,\n",
    "                  buffer_size: int=1):\n",
    "    \"\"\"Generates pertubations for texts in data. Returns list of length n_pertubations, each entry being a JSON object with perturbed text\"\"\"\n",
    "    # Initialise list to store all perturbed JSON\n",
    "    all_perturbed_texts = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)] # Truncate text to max_length, returns list of strings as before\n",
    "    \n",
    "    # Iterate for length num_pertubation\n",
    "    for _ in tqdm(range(n_perturbations), desc=f\"Processing {n_perturbations} perturbations for {n_samples} texts. Perturbation number\"):\n",
    "\n",
    "        # Randomly select 15% of text to mask, creates pertubations\n",
    "        perturbed_texts = perturb_texts(texts=original_texts,\n",
    "                                        batch_size=batch_size,\n",
    "                                        span_length=span_length,\n",
    "                                        pct=pct,\n",
    "                                        buffer_size=buffer_size,\n",
    "                                        ceil_pct=False)\n",
    "\n",
    "        # Store in list\n",
    "        all_perturbed_texts.append(perturbed_texts)\n",
    "    \n",
    "    return all_perturbed_texts\n",
    "    # The output is a list of length n_pertubations, each entry being a list of length num_samples, each entry being a string\n",
    "    # (so the outer list is over the perturbations, the inner list is over the samples, and the string is the perturbed text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_log_prob(original_texts, all_perturbed_texts, n_perturbation=100, batch_size=128):\n",
    "    \"\"\"Compares log probs of original text vs list of JSONs perturbed texts\"\"\"\n",
    "    # Initialise list to store log prob of each perturbed JSON\n",
    "    \n",
    "    log_probs_per_text_pert = []\n",
    "\n",
    "    # Get num_samples\n",
    "    num_samples = len(original_texts)\n",
    "\n",
    "    # Calculate log prob for original text in batch\n",
    "    log_probs_per_text_base = get_lls(texts=original_texts, batch_size=batch_size)\n",
    "\n",
    "    # Iterate \n",
    "    for perturbation_idx in tqdm(range(n_perturbation)):\n",
    "\n",
    "        # Get the JSON file\n",
    "        perturbed_texts = all_perturbed_texts[perturbation_idx]\n",
    "\n",
    "        # Calculate log prob\n",
    "        log_probs_per_text_tran = get_lls(texts=perturbed_texts, batch_size=batch_size)\n",
    "\n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_pert.append([])\n",
    "            log_probs_per_text_pert[j].append(log_probs_per_text_tran[j])\n",
    "        \n",
    "    return log_probs_per_text_base, log_probs_per_text_pert\n",
    "    # log_probs_per_text_base is a list of length num_samples, each entry being the log prob of the corresponding original text\n",
    "    # log_probs_per_text_pert is a list of length num_samples, each entry being a list of length num_pertubations, each entry being the log prob of the corresponding perturbed text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- **Data loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📌 Dataset format guidelines**\n",
    "\n",
    "All datasets (human-written and AI-generated) must follow this format:\n",
    "\n",
    "- Stored as a **`.jsonl`** where each line is a dictionary.\n",
    "- Each entry contains (minimum requirement):\n",
    "  - `\"text\"`: the text content\n",
    "  - `\"model\"`: for human text please label it as `\"human\"` and for AI-generated texts, please specify the model used (e.g. ``\"gpt2-large\"``)\n",
    "  - `\"source\"`: the origin of the text (e.g., `\"wikihow\"`, `\"reddit\"`, `\"news articles\"`)\n",
    "\n",
    "#### Exemple (as in ``subtaskB_train.jsonl`` located in `Datasets\\SemEval2024-Task8`):\n",
    "```json\n",
    "{\"text\": \"A groundbreaking discovery in physics was made today.\", \"model\": \"human\", \"source\": \"news articles\"}\n",
    "{\"text\": \"The AI revolution is shaping the future of work.\", \"model\": \"chatGPT\", \"source\": \"AI Generated\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Human texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "# if not os.path.exists(FILE_PATH):\n",
    "#     raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "# data_human = []\n",
    "\n",
    "# # Efficiently process the file line by line\n",
    "# with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#     for line in file:\n",
    "#         record = json.loads(line)  # Parse JSON once\n",
    "#         if record.get(\"model\") == \"human\":\n",
    "#             data_human.append(record)\n",
    "\n",
    "# # Print first human record\n",
    "# print(\"First human text record:\", data_human[0] if data_human else \"No human data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For XSUM dataset\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Data/EdinburghNLP___xsum/default/1.2.0/40db7604fedb616a9d2b0673d11838fa5be8451c/xsum-test.arrow\"  # CHANGE THIS TO THE PATH OF THE HUMAN-GENERATED TEXT FILE\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "\n",
    "# For XSUM data - convert into a list of dictionaries\n",
    "# Load dataset using Hugging Face's datasets library\n",
    "dataset = Dataset.from_file(FILE_PATH)\n",
    "# Keep only the first 200 rows\n",
    "dataset = dataset.select(range(3))    # CHANGE THIS TO CONTROL THE NUMBER OF DATAPOINTS TO USE\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "# Adapt the dataframe to the desired style\n",
    "df = df.rename(columns={\"document\": \"text\"}).drop(columns=[\"summary\"])  # Rename and drop columns\n",
    "df[\"model\"] = \"human\"   # Add a new column to track that all these documents are human-generated\n",
    "\n",
    "# Truncate text to 400 tokens - CHANGE THIS TO CONTROL THE MAXIMUM NUMBER OF TOKENS TO USE\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: generation_tokenizer.convert_tokens_to_string(generation_tokenizer.tokenize(x)[:400]) if len(generation_tokenizer.tokenize(x)) > 400 else x)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "data_human = df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **AI-generated texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. *Option 1: produce own AI-generated texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate AI text from a given prompt\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): max length of generated text\n",
    "\n",
    "    Returns:\n",
    "        cleaned_text (str): cleaned generated text\n",
    "    \"\"\"\n",
    "    inputs = generation_tokenizer(prompt, return_tensors=\"pt\").to(generation_model.device)\n",
    "    with torch.no_grad():\n",
    "        output = generation_model.generate(**inputs, max_length=max_length, do_sample=True, temperature=1.0)    # CHANGE THE GENERATION MODEL TEMPERATURE HERE (originally 0.7)\n",
    "    \n",
    "    generated_text = generation_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "    # The output is a string of the generated text\n",
    "\n",
    "\n",
    "def generate_prompt(prompt_list: list) -> list:\n",
    "    \"\"\"\n",
    "    Truncate the 'text' field in each dictionary to the first 30 tokens.\n",
    "\n",
    "    Args:\n",
    "        prompt_list (list): List of dictionaries with keys 'text', 'id', and 'model'.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated list with truncated 'text' values.\n",
    "    \"\"\"\n",
    "    truncated_prompts = []\n",
    "    for entry in prompt_list:\n",
    "        tokens = generation_tokenizer.tokenize(entry['text'])\n",
    "        truncated_text = generation_tokenizer.convert_tokens_to_string(tokens[:30]) # CHANGE THIS TO CONTROL THE NUMBER OF TOKENS THE GENERATION MODEL IS GIVEN AS A PROMPT\n",
    "        truncated_prompts.append({\n",
    "            \"text\": truncated_text,\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"model\": entry[\"model\"]\n",
    "        })\n",
    "    return truncated_prompts\n",
    "    # The output is a list of dictionaries with keys 'text', 'id', and 'model'\n",
    "\n",
    "\n",
    "def generate_dataset(prompt_list: list, max_length: int, generation_model_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a dataset of AI-generated texts based on the given original prompt list.\n",
    "\n",
    "    Args:\n",
    "        prompt_list (list): Original list of dictionaries with full 'text'.\n",
    "        max_length (int): Maximum length of each generated text.\n",
    "\n",
    "    Returns:\n",
    "        list: Dataset of AI-generated texts (list of dictionaries).\n",
    "    \"\"\"\n",
    "    truncated_prompts = generate_prompt(prompt_list)  # Apply generate_prompt within generate_dataset\n",
    "\n",
    "    data_ai = [\n",
    "        {\n",
    "            \"text\": generate_text(entry[\"text\"], max_length),\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"model\": generation_model_name\n",
    "        }\n",
    "        for entry in truncated_prompts\n",
    "    ]\n",
    "    return data_ai\n",
    "    # The output is a list of dictionaries, each dictionary has keys 'text', 'id', and 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model.to(DEVICE)\n",
    "print(DEVICE)\n",
    "\n",
    "max_length = 400  # CHANGE THIS TO CONTROL THE MAXIMUM LENGTH OF THE GENERATED TEXT\n",
    "\n",
    "clear_cuda_cache()\n",
    "data_ai_generated = generate_dataset(data_human, max_length, GENERATION_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the AI-generated dataset in the correct .jsonl format\n",
    "\n",
    "def save_ai_dataset_jsonl(data, file_name_base, generation_model_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in JSONL format in a structured directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of dictionaries to be saved.\n",
    "        file_name_base (str): Base name for the file.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_generated_by_{generation_model_name}_{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Data\", \"AI-Generated\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and check the AI-generated dataset\n",
    "\n",
    "# Save the AI-generated dataset\n",
    "save_ai_dataset_jsonl(data_ai_generated, \"XSUM_200_Samples\", GENERATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_generated[0] if data_ai_generated else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. *Option 2: load AI-generated texts from a dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "# # FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "# if not os.path.exists(FILE_PATH):\n",
    "#     raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "# data_ai_dataset = []\n",
    "\n",
    "# # Read entire file and parse as JSON list\n",
    "# with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#     for line in file:\n",
    "#         record = json.loads(line)  # Parse JSON once\n",
    "#         if record.get(\"model\") != \"human\":\n",
    "#             data_ai_dataset.append(record)\n",
    "\n",
    "# # Print first AI-generated text record\n",
    "# print(\"First AI-generated text record:\", data_ai_dataset[0] if data_ai_dataset else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Texts perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = data_human\n",
    "# n_samples = 2\n",
    "# max_length = 10\n",
    "\n",
    "# original_texts = [\" \".join(DATA[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "# clear_cuda_cache()\n",
    "# perplexity_scores = get_ppls(texts=original_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- **Exemple usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the perturbations in the correct .jsonl format\n",
    "\n",
    "def save_perturbations_jsonl(all_perturbed_texts, file_name_base, n_samples, max_length, n_perturbations, generation_model_name, perturbation_model_name):\n",
    "    \"\"\"\n",
    "    Saves perturbed texts in JSONL format, where each line is a list of perturbed texts (one perturbation batch).\n",
    "\n",
    "    Parameters:\n",
    "        all_perturbed_texts (list of lists): List of lists containing perturbed texts.\n",
    "        file_name_base (str): Base name for the file.\n",
    "        generation_model_name (str): Name of the model that generated the original texts.\n",
    "        perturbation_model_name (str): Name of the model that generated the perturbations.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}__{n_samples}_Samples__{max_length}_Max_Length__{n_perturbations}_Perturbations__Generated_By_{generation_model_name}__Perturbed_By_{perturbation_model_name}__{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Data\", \"Perturbations\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for perturbed_texts in all_perturbed_texts:\n",
    "            f.write(json.dumps(perturbed_texts) + \"\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the results in the correct .jsonl format\n",
    "\n",
    "def save_results_jsonl(data, file_name_base, n_samples, max_length, n_perturbations, generation_model_name, perturbation_model_name, computation_model_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in JSONL format in a structured directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of results to be saved.\n",
    "        file_name_base (str): Base name for the file.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}__{n_samples}_Samples__{max_length}_Max_Length__{n_perturbations}_Perturbations__Generated_By_{generation_model_name}__Perturbed_By_{perturbation_model_name}__Scored_By_{computation_model_name}__{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Results\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATA = data_human\n",
    "\n",
    "# Experiment setup\n",
    "NUM_SAMPLES = 3\n",
    "MAX_LENGTH = 400\n",
    "\n",
    "# Perturbation setup\n",
    "NUM_PERTURBATIONS = 100\n",
    "SPAN_LENGTH = 2\n",
    "PCT = 0.3\n",
    "BUFFER_SIZE = 1\n",
    "\n",
    "clear_cuda_cache()\n",
    "\n",
    "# # Compute log probs before and after perturbation\n",
    "# log_probs_base_human, log_probs_transformed_human = optimized_processing(\n",
    "#     data=DATA,\n",
    "#     n_samples=n_samples,\n",
    "#     max_length=max_length,\n",
    "#     n_perturbations=n_perturbations,\n",
    "#     span_length=span_length,\n",
    "#     pct=pct,\n",
    "#     buffer_size=buffer_size\n",
    "#     )\n",
    "\n",
    "perturbed_texts_human = gen_perturbed(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, SPAN_LENGTH, PCT, BUFFER_SIZE)\n",
    "save_perturbations_jsonl(perturbed_texts_human, \"XSUM_Human\", NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, \"Human\", PERTURBATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "log_probs_base_human, log_probs_transformed_human = compare_log_prob(DATA, perturbed_texts_human, NUM_PERTURBATIONS)\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human, log_probs_transformed_human, normalization=True)\n",
    "\n",
    "results_human = {}\n",
    "results_human[\"log_probs_base\"] = log_probs_base_human\n",
    "results_human[\"log_probs_transformed\"] = log_probs_transformed_human\n",
    "results_human[\"discrepancy_scores\"] = discrepancy_scores_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving results\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Results\\experiment_0_results_human.json\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "# with open(FILE_PATH, \"w\") as f:\n",
    "#     json.dump(results_human, f, indent=2)\n",
    "\n",
    "# Save the results\n",
    "\n",
    "save_results_jsonl(results_human, \"XSUM_Human\", NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, \"Human\", PERTURBATION_MODEL_NAME, GENERATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATA = data_ai_generated\n",
    "\n",
    "# Experiment setup\n",
    "NUM_SAMPLES = 3\n",
    "MAX_LENGTH = 400\n",
    "\n",
    "# Perturbation setup\n",
    "NUM_PERTURBATIONS = 100\n",
    "SPAN_LENGTH = 2\n",
    "PCT = 0.3\n",
    "BUFFER_SIZE = 1\n",
    "\n",
    "# # Compute log probs before and after perturbation\n",
    "# clear_cuda_cache()\n",
    "# log_probs_base_ai, log_probs_transformed_ai = optimized_processing(\n",
    "#     data=DATA,\n",
    "#     n_samples=n_samples,\n",
    "#     max_length=max_length,\n",
    "#     n_perturbations=n_perturbations,\n",
    "#     span_length=span_length,\n",
    "#     pct=pct,\n",
    "#     buffer_size=buffer_size\n",
    "#     )\n",
    "\n",
    "perturbed_texts_ai = gen_perturbed(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, SPAN_LENGTH, PCT, BUFFER_SIZE)\n",
    "save_perturbations_jsonl(perturbed_texts_ai, \"XSUM_AI\", NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, GENERATION_MODEL_NAME, PERTURBATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "log_probs_base_ai, log_probs_transformed_ai = compare_log_prob(DATA, perturbed_texts_ai, NUM_PERTURBATIONS)\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai, log_probs_transformed_ai, normalization=True)\n",
    "\n",
    "results_ai = {}\n",
    "results_ai[\"log_probs_base\"] = log_probs_base_ai\n",
    "results_ai[\"log_probs_transformed\"] = log_probs_transformed_ai\n",
    "results_ai[\"discrepancy_scores\"] = discrepancy_scores_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving results\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Results\\experiment_0_results_ai.json\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "# with open(FILE_PATH, \"w\") as f:\n",
    "#     json.dump(results_ai, f, indent=2)\n",
    "\n",
    "# Save the results\n",
    "\n",
    "save_results_jsonl(results_ai, \"XSUM_AI\", NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, GENERATION_MODEL_NAME, PERTURBATION_MODEL_NAME, COMPUTATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sense-Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first perturbation of the first text of the AI-generated dataset\n",
    "print(\"First perturbation of the first text of the AI-generated dataset:\")\n",
    "print(perturbed_texts_ai[0][0])\n",
    "# Print the first perturbation of the second text of the AI-generated dataset\n",
    "print(\"First perturbation of the second text of the AI-generated dataset:\")\n",
    "print(perturbed_texts_ai[2][1])\n",
    "\n",
    "# Print the first perturbation of the first text of the human dataset\n",
    "print(\"First perturbation of the first text of the human dataset:\")\n",
    "print(perturbed_texts_human[0][0])\n",
    "# Print the first perturbation of the second text of the human dataset\n",
    "print(\"First perturbation of the second text of the human dataset:\")\n",
    "print(perturbed_texts_human[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Histograms of the discrepancy scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION = True # True if you want to normalize the discrepancy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AI texts results\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Results\\\\results_ai.json\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "# with open(FILE_PATH, 'r') as file:\n",
    "#     data_ai = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_probs_base_ai = data_ai[\"log_probs_base\"]\n",
    "# log_probs_transformed_ai = data_ai[\"log_probs_transformed\"]\n",
    "\n",
    "# discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Human texts results\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Results\\\\results_human.json\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "# with open(FILE_PATH, 'r') as file:\n",
    "#     data_human = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_probs_base_human = data_human[\"log_probs_base\"]\n",
    "# log_probs_transformed_human = data_human[\"log_probs_transformed\"]\n",
    "\n",
    "# discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def optimal_threshold(list1, list2):\n",
    "    X = np.concatenate([list1, list2]).reshape(-1, 1)\n",
    "    y = np.concatenate([np.zeros(len(list1)),np.ones(len(list2))])\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    best_threshold = -clf.intercept_[0] / clf.coef_[0][0]\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = (clf.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    return best_threshold, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold,auroc = optimal_threshold(discrepancy_scores_human, discrepancy_scores_ai)\n",
    "print(f\"Optimal threshold: {threshold:.2f}\")\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "\n",
    "plt.hist(discrepancy_scores_human, bins=15, alpha=0.5, label='Human', edgecolor='black', density=True)\n",
    "plt.hist(discrepancy_scores_ai, bins=15, alpha=0.5, label='AI', edgecolor='black', density=True)\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "plt.xlabel(f'Discrepancy scores (normalization={NORMALIZATION})')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Computation model: {COMPUTATION_MODEL_NAME}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
