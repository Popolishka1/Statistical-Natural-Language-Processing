{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT**: Identifying AI-generated text\n",
    "This notebook implements the DetectGPT algorithm from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Code setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model for text generation and probabilities/perplexity computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-large\" \n",
    "\n",
    "# Model list (all tested)\n",
    "# gpt2\n",
    "# gpt2-large\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model for perturbation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "PERTURBATION_MODEL_NAME = \"t5-large\"\n",
    "\n",
    "# Load model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(PERTURBATION_MODEL_NAME,torch_dtype=torch.float16,device_map=\"auto\")\n",
    "\n",
    "# Load tokenizer \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "t5_model.eval()\n",
    "\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Output AI-generated text using the chosen model\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): the maximum length (~ number of words) of the generated text\n",
    "\n",
    "    Returns:\n",
    "        str: generated text\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=max_length, do_sample=True, temperature=0.7)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the log prob of a given text under the chosen model\n",
    "\n",
    "    Args:\n",
    "        text (str): input text for which to compute the log prob\n",
    "\n",
    "    Returns:\n",
    "        float: input text log prob\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    n_tokens = input_ids.shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, labels=input_ids)\n",
    "        # negative of the NLL per token = log prob\n",
    "        log_prob = -outputs.loss.item() * n_tokens # total NLL\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the perplexity score of a given text using the chosen model\n",
    "\n",
    "    Args:\n",
    "        text (str): input text for which to compute perplexity\n",
    "\n",
    "    Returns:\n",
    "        float: text perplexity score \n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, labels=input_ids)\n",
    "        log_prob = outputs.loss # NLL per token\n",
    "        \n",
    "    perplexity = torch.exp(log_prob) if log_prob < 100 else float(\"inf\") # overflow possible\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perturbation\n",
    "We define a perturbation function that slightly modifies the text. This allows us to analyze how variations in text influence log probabilities. Different perturbation methods can be used, such as:\n",
    "- `word_swap_perturbation`: basic word swap function\n",
    "- `t5_perturbation`: a more sophisticated perturbation method using a transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word swap perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_swap_perturbation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Randomly swaps two adjacent words to create a perturbed version of the text\n",
    "\n",
    "    Args:\n",
    "        text (str): the input text to be perturbed\n",
    "\n",
    "    Returns:\n",
    "        str: the perturbed text \n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) > 3:\n",
    "        i = random.randint(0, len(words)-2)\n",
    "        words[i], words[i + 1] = words[i + 1], words[i]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5 perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_text(text, mask_ratio=0.15, max_words=370):\n",
    "    words = text.split()\n",
    "\n",
    "    # Truncate text\n",
    "    if len(words) > max_words:\n",
    "        words = words[:max_words]\n",
    "\n",
    "    num_masks = int(len(words) * mask_ratio)\n",
    "\n",
    "    # Randomly select spans to mask\n",
    "    mask_indices = sorted(random.sample(range(len(words) - 1), num_masks))\n",
    "    for i, idx in enumerate(mask_indices):\n",
    "        words[idx] = f\"<extra_id_{i}>\"\n",
    "        if idx + 1 < len(words): # Ensure a 2-word span\n",
    "            words[idx + 1] = \"\"\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def replace_masks(texts):\n",
    "    \"\"\"Generate T5 model outputs for masked texts.\"\"\"\n",
    "    n_expected = [text.count(\"<extra_id_\") for text in texts]\n",
    "    stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "\n",
    "    tokens = t5_tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move input tensors to model's device just before passing to model\n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(\n",
    "            input_ids=tokens[\"input_ids\"].to(t5_model.device),  \n",
    "            attention_mask=tokens[\"attention_mask\"].to(t5_model.device),  \n",
    "            max_length=150,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=stop_id\n",
    "        )\n",
    "        \n",
    "    outputs = outputs.detach().cpu() # Move tensors to CPU and detach\n",
    "\n",
    "    return t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "def extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "\n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "def apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            masked_text = masked_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "\n",
    "        filled_texts.append(masked_text)\n",
    "\n",
    "    return filled_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_perturbation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    T5 perturbation\n",
    "\n",
    "    Args:\n",
    "        text (str): the input text to be perturbed\n",
    "\n",
    "    Returns:\n",
    "        str: the perturbed text \n",
    "    \"\"\"\n",
    "    masked_text = mask_text(text)\n",
    "    raw_fills = replace_masks([masked_text])\n",
    "    extracted_fills = extract_fills(raw_fills)\n",
    "    perturbed_text = apply_extracted_fills([masked_text], extracted_fills)[0]\n",
    "    return perturbed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Paper algo\n",
    "The DetectGPT algorithm works by computing:\n",
    "- The mean log probability of perturbed texts\n",
    "- The difference between the original text's log probability and the mean perturbed probability\n",
    "The final score indicates whether the text is likely AI-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectgpt_score(text: str, num_perturbations: int, perturbation_function) -> bool:\n",
    "    \"\"\"\n",
    "    Implement DetectGPT algorithm 1\n",
    "\n",
    "    Args:\n",
    "        text (str): input text to be analyzed\n",
    "        num_perturbations (int): number of perturbed versions of the text to generate\n",
    "        perturbation_function (function): function to generate perturbed versions of the text (word swap or T5)\n",
    "\n",
    "    Returns:\n",
    "        bool: true if the text is likely model-generated\n",
    "    \"\"\"\n",
    "    original_log_prob = compute_log_prob(text) # log prob of the original text\n",
    "\n",
    "    # Generate perturbed texts + compute their log prob\n",
    "    perturbed_texts = [perturbation_function(text) for _ in range(num_perturbations)]\n",
    "    perturbed_log_probs = [compute_log_prob(pt) for pt in perturbed_texts]\n",
    "\n",
    "    mu = sum(perturbed_log_probs) / num_perturbations # mean log probability of the perturbed texts\n",
    "\n",
    "    d = original_log_prob - mu # estimate perturbation discrepancy d\n",
    "\n",
    "    variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (num_perturbations - 1) # variance of the log probabilities\n",
    "    sigma = variance ** 0.5 # standard deviation\n",
    "\n",
    "    score = d / sigma if sigma > 0 else 0 \n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI text: In a faraway galaxy, where no humans exist, the world is in a terrible state. The space station, which was originally built to serve the planet, has broken down and the crew is trapped in the ship. The only hope for survival is a young girl known as \"Sue\" who\n",
      "human text: But Bhaduri found it increasingly hard to secure work after more women began partaking in jatra productions in the 1960s and 1970s. By the time he met Kishore, who was running a theater publication at the time, the actor was in his 60s and only performing a handful of times a year for the equivalent of $1 a night.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "max_length = 60\n",
    "prompt = \"In a faraway galaxy, where no humans exist\"\n",
    "\n",
    "ai_text = generate_text(prompt,max_length)\n",
    "\n",
    "# Human text from CNN\n",
    "human_text = \"But Bhaduri found it increasingly hard to secure work after more women began partaking in jatra productions in the 1960s and 1970s. By the time he met Kishore, who was running a theater publication at the time, the actor was in his 60s and only performing a handful of times a year for the equivalent of $1 a night.\"\n",
    "\n",
    "print(\"AI text:\", ai_text)\n",
    "print(\"human text:\", human_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI text DetectGPT score: 5.055410139882069\n",
      "Human text DetectGPT score: 3.9946030108610695\n"
     ]
    }
   ],
   "source": [
    "num_perturbations = 100\n",
    "perturbation = t5_perturbation\n",
    "\n",
    "ai_text_score = detectgpt_score(ai_text,num_perturbations,perturbation)\n",
    "human_text_score = detectgpt_score(human_text,num_perturbations,perturbation)\n",
    "\n",
    "print(\"AI text DetectGPT score:\", ai_text_score)\n",
    "print(\"Human text DetectGPT score:\", human_text_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perplexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity AI: 11.486611366271973\n",
      "Perplexity AI (perturbed): 54.88127517700195\n",
      "Perplexity human: 22.94808578491211\n",
      "Perplexity human (perturbed): 42.53069305419922\n"
     ]
    }
   ],
   "source": [
    "# Compute perplexity for AI-generated text and human text\n",
    "perturbation = t5_perturbation\n",
    "\n",
    "max_length = 60\n",
    "prompt = \"In a faraway galaxy, where no humans exist\"\n",
    "ai_text = generate_text(prompt, max_length)\n",
    "\n",
    "human_text = \"But Bhaduri found it increasingly hard to secure work after more women began partaking in jatra productions in the 1960s and 1970s. By the time he met Kishore, who was running a theater publication at the time, the actor was in his 60s and only performing a handful of times a year for the equivalent of $1 a night.\"\n",
    "\n",
    "# Compute perplexity for AI-generated text\n",
    "perplexity_ai = compute_perplexity(ai_text)\n",
    "perplexity_ai_perturbed = compute_perplexity(perturbation(ai_text))\n",
    "\n",
    "# Compute perplexity for human-written text\n",
    "perplexity_human = compute_perplexity(human_text)\n",
    "perplexity_human_perturbed = compute_perplexity(perturbation(human_text))\n",
    "\n",
    "print(f\"Perplexity AI: {perplexity_ai}\")\n",
    "print(f\"Perplexity AI (perturbed): {perplexity_ai_perturbed}\")\n",
    "print(f\"Perplexity human: {perplexity_human}\")\n",
    "print(f\"Perplexity human (perturbed): {perplexity_human_perturbed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot analysis: perplexity before and after perturbation for a wide variety of sample\n",
    "\n",
    "perturbation = t5_perturbation\n",
    "\n",
    "num_samples = 100\n",
    "max_length = 60\n",
    "prompts = [\"In a faraway galaxy, where no humans exist\" for _ in range(num_samples)]\n",
    "\n",
    "# Generate AI-generated texts based on the prompts\n",
    "ai_texts = [generate_text(prompt, max_length) for prompt in prompts]\n",
    "\n",
    "# Compute perplexity before and after perturbation\n",
    "perplexities_before = [compute_perplexity(text) for text in ai_texts]\n",
    "perplexities_after = [compute_perplexity(perturbation(text)) for text in ai_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.scatter(perplexities_before, perplexities_after,marker='x',c='r')\n",
    "\n",
    "plt.xlabel(\"Perplexity before perturbation\")\n",
    "plt.ylabel(\"Perplexity after perturbation\")\n",
    "plt.plot([0,30],[0,30],'b--')\n",
    "plt.title(f\"Perplexity before vs after perturbation for {num_samples} AI-generated texts\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
