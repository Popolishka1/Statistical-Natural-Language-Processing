{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected keys and structure\n",
    "EXPECTED_KEYS = {\"log_probs_base\", \"log_probs_transformed\", \"discrepancy_scores\"}\n",
    "\n",
    "def load_and_validate(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    # Check that the JSON object has exactly the three expected keys\n",
    "    if set(data.keys()) != EXPECTED_KEYS:\n",
    "        raise ValueError(f\"Expected keys {EXPECTED_KEYS}, but got {set(data.keys())}\")\n",
    "    # Validate lengths for each list\n",
    "    if len(data[\"log_probs_base\"]) != 200:\n",
    "        raise ValueError(\"log_probs_base should contain 200 floats\")\n",
    "    if len(data[\"log_probs_transformed\"]) != 200:\n",
    "        raise ValueError(\"log_probs_transformed should contain 200 lists\")\n",
    "    if len(data[\"discrepancy_scores\"]) != 200:\n",
    "        raise ValueError(\"discrepancy_scores should contain 200 floats\")\n",
    "    # Validate each inner list in log_probs_transformed has 100 floats\n",
    "    for idx, inner_list in enumerate(data[\"log_probs_transformed\"]):\n",
    "        if len(inner_list) != 100:\n",
    "            raise ValueError(f\"Inner list at index {idx} in log_probs_transformed does not contain 100 floats\")\n",
    "    return data\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Generated by humans\n",
    "human_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250319_103854.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250319_114542.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250319_120520.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  Generated by GPT-J\n",
    "ai_generated_by_gpt_j_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-j-6B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250318_175629.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-j-6B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250318_183320.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-j-6B__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250318_183732.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  Generated by GPT-Neo\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-neo-2.7B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250318_202111.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-neo-2.7B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250318_205825.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-neo-2.7B__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250318_210233.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  Generated by GPT-2\n",
    "ai_generated_by_gpt_2_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_openai-community/gpt2__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250319_112531.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_openai-community/gpt2__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250319_120321.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_openai-community/gpt2__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250319_120717.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate DetectGPT Discrepancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DetectGPT Discrepancy Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list,\n",
    "                                log_probs_per_text_transformed: list,\n",
    "                                normalization: bool=False) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts. Computed for n_perturbations perturbations.\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the n_perturbations perturbed log probs\n",
    "        normalization (bool): True if you want to normalize the discrepancy scores, False otherwise\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texts\n",
    "    \"\"\"\n",
    "    n_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = np.mean(perturbed_log_probs)  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            sigma = np.std(perturbed_log_probs)\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else discrepancy_score_unormalized\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Discrepancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) GPT-J as Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_computed_by_gpt_j = compute_detectgpt_discrepancy(human_computed_by_gpt_j_dct[\"log_probs_base\"], human_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_j = compute_detectgpt_discrepancy(ai_generated_by_gpt_j_computed_by_gpt_j_dct[\"log_probs_base\"], ai_generated_by_gpt_j_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_j = compute_detectgpt_discrepancy(ai_generated_by_gpt_neo_computed_by_gpt_j_dct[\"log_probs_base\"], ai_generated_by_gpt_neo_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_j = compute_detectgpt_discrepancy(ai_generated_by_gpt_2_computed_by_gpt_j_dct[\"log_probs_base\"], ai_generated_by_gpt_2_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) GPT-Neo as Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"EleutherAI/gpt-neo-2.7B\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_computed_by_gpt_neo = compute_detectgpt_discrepancy(human_computed_by_gpt_neo_dct[\"log_probs_base\"], human_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_neo = compute_detectgpt_discrepancy(ai_generated_by_gpt_j_computed_by_gpt_neo_dct[\"log_probs_base\"], ai_generated_by_gpt_j_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_neo = compute_detectgpt_discrepancy(ai_generated_by_gpt_neo_computed_by_gpt_neo_dct[\"log_probs_base\"], ai_generated_by_gpt_neo_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_neo = compute_detectgpt_discrepancy(ai_generated_by_gpt_2_computed_by_gpt_neo_dct[\"log_probs_base\"], ai_generated_by_gpt_2_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) GPT-2 as Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"openai-community/gpt2\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_computed_by_gpt_2 = compute_detectgpt_discrepancy(human_computed_by_gpt_2_dct[\"log_probs_base\"], human_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_2 = compute_detectgpt_discrepancy(ai_generated_by_gpt_j_computed_by_gpt_2_dct[\"log_probs_base\"], ai_generated_by_gpt_j_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_2 = compute_detectgpt_discrepancy(ai_generated_by_gpt_neo_computed_by_gpt_2_dct[\"log_probs_base\"], ai_generated_by_gpt_neo_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_2 = compute_detectgpt_discrepancy(ai_generated_by_gpt_2_computed_by_gpt_2_dct[\"log_probs_base\"], ai_generated_by_gpt_2_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise Discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = [\n",
    "    human_computed_by_gpt_j, ai_generated_by_gpt_j_computed_by_gpt_j,   # Top-left\n",
    "    human_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_neo, # Top-middle\n",
    "    human_computed_by_gpt_2, ai_generated_by_gpt_j_computed_by_gpt_2,   # Top-right\n",
    "    human_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_j, # Middle-left\n",
    "    human_computed_by_gpt_neo, ai_generated_by_gpt_neo_computed_by_gpt_neo, # Middle\n",
    "    human_computed_by_gpt_2, ai_generated_by_gpt_neo_computed_by_gpt_2, # Middle-right\n",
    "    human_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_j,   # Bottom-left\n",
    "    human_computed_by_gpt_neo, ai_generated_by_gpt_2_computed_by_gpt_neo, # Bottom-middle\n",
    "    human_computed_by_gpt_2, ai_generated_by_gpt_2_computed_by_gpt_2    # Bottom-right\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUROC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_metrics(discrepancy_scores_human, discrepancy_scores_ai):\n",
    "    fpr, tpr, _ = roc_curve(\n",
    "    np.array([0] * len(discrepancy_scores_human) + [1] * len(discrepancy_scores_ai)),\n",
    "    np.concatenate([discrepancy_scores_human, discrepancy_scores_ai])\n",
    "    )\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr.tolist(), tpr.tolist(), float(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_grid():\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    fig.suptitle('Histograms of Discrepancy Scores', fontsize=24, fontweight='bold', y=1.04)  # Title further away\n",
    "\n",
    "    row_titles = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "    col_titles = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        human_scores = scores_list[i * 2]\n",
    "        ai_scores = scores_list[i * 2 + 1]\n",
    "\n",
    "        fpr,tpr,roc_auc = get_roc_metrics(human_scores, ai_scores)\n",
    "\n",
    "        ax.hist(human_scores, bins='auto', alpha=0.5, label='Human', edgecolor='black')\n",
    "        ax.hist(ai_scores, bins='auto', alpha=0.5, label='AI', edgecolor='black')\n",
    "\n",
    "\n",
    "        ax.set_title(f'AUROC {roc_auc:.2f}')\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.text(-0.25, 0.5, row_titles[i // 3], transform=ax.transAxes, fontsize=16, fontweight = 'bold',\n",
    "                    rotation=90, verticalalignment='center', horizontalalignment='center')\n",
    "\n",
    "        if i < 3:\n",
    "            ax.text(0.5, 1.15, col_titles[i], transform=ax.transAxes, fontsize=16, fontweight = 'bold',\n",
    "                    rotation='horizontal', verticalalignment='bottom', horizontalalignment='center')\n",
    "\n",
    "        if i >= 6:\n",
    "            ax.set_xlabel('Discrepancy Score')\n",
    "        \n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    fig.text(0.5, 0.95, 'Scoring Model', ha='center', fontsize=20, fontweight = 'bold')  # Centered relative to grid\n",
    "    fig.text(-0.06, 0.5, 'Generating Model', va='center', rotation='vertical', fontsize=20, fontweight = 'bold')  # Equidistant from grid\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUROC Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auroc_grid():\n",
    "    aurocs = [get_roc_metrics(scores_list[i * 2], scores_list[i * 2 + 1])[2] for i in range(9)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    auroc_matrix = np.array(aurocs).reshape(3, 3)\n",
    "    sns.heatmap(auroc_matrix, annot=True, fmt='.2f', cmap='Blues', linewidths=0.5, ax=ax, cbar_kws={'label': 'AUROC'})\n",
    "\n",
    "    # Set row and column labels\n",
    "    row_titles = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "    col_titles = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "\n",
    "    ax.set_xticklabels(col_titles, fontsize=12, fontweight='bold', ha='center', rotation=0)\n",
    "    ax.xaxis.set_label_position('top')  # Move x-axis label (column titles) to the top\n",
    "    ax.xaxis.tick_top()  # Ensure ticks and labels are on top\n",
    "\n",
    "    ax.set_yticklabels(row_titles, fontsize=12, fontweight='bold', va='center', rotation=90)\n",
    "    \n",
    "    # Set main title\n",
    "    fig.suptitle('Grid of AUROCs', fontsize=16, fontweight='bold', y=1.08)\n",
    "\n",
    "    # Set 'Scoring Model' and 'Generating Model' labels\n",
    "    fig.text(0.5, 0.96, 'Scoring Model', ha='center', fontsize=14, fontweight='bold')\n",
    "    fig.text(0, 0.5, 'Generating Model', va='center', rotation='vertical', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram_grid()\n",
    "plot_auroc_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the arithmetic mean of the scores for the human text\n",
    "human_scores = np.array(scores_list[0::2])\n",
    "human_scores_mean = np.mean(human_scores)\n",
    "\n",
    "# Check whether any of the lists of scores for the GPT-J generated texts are the same (note that each entry is a list)\n",
    "assert not np.allclose(scores_list[1], scores_list[3], scores_list[5])\n",
    "# Calculate the arithmetic mean of the scores for the GPT-J generated text (entries 1, 3, 5)\n",
    "gpt_j_scores = np.array([scores_list[1], scores_list[3], scores_list[5]])\n",
    "gpt_j_scores_mean = np.mean(gpt_j_scores)\n",
    "\n",
    "# Check whether any of the lists of scores for the GPT-Neo generated texts are the same (note that each entry is a list)\n",
    "assert not np.allclose(scores_list[7], scores_list[9], scores_list[11])\n",
    "# Calculate the arithmetic mean of the scores for the GPT-Neo generated text (entries 7, 9, 11)\n",
    "gpt_neo_scores = np.array([scores_list[7], scores_list[9], scores_list[11]])\n",
    "gpt_neo_scores_mean = np.mean(gpt_neo_scores)\n",
    "\n",
    "# Check whether any of the lists of scores for the GPT-2 generated texts are the same (note that each entry is a list)\n",
    "assert not np.allclose(scores_list[13], scores_list[15], scores_list[17])\n",
    "# Calculate the arithmetic mean of the scores for the GPT-2 generated text (entries 13, 15, 17)\n",
    "gpt_2_scores = np.array([scores_list[13], scores_list[15], scores_list[17]])\n",
    "gpt_2_scores_mean = np.mean(gpt_2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculate the mean, ignoring the results where the generating model is used as a scoring model\n",
    "\n",
    "human_scores_no_base = human_scores\n",
    "human_scores_no_base_mean = np.mean(human_scores_no_base)\n",
    "\n",
    "gpt_j_scores_no_base = np.array([scores_list[3], scores_list[5]])\n",
    "gpt_j_scores_no_base_mean = np.mean(gpt_j_scores_no_base)\n",
    "\n",
    "gpt_neo_scores_no_base = np.array([scores_list[7], scores_list[11]])\n",
    "gpt_neo_scores_no_base_mean = np.mean(gpt_neo_scores_no_base)\n",
    "\n",
    "gpt_2_scores_no_base = np.array([scores_list[13], scores_list[15])\n",
    "gpt_2_scores_no_base_mean = np.mean(gpt_2_scores_no_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above code, but find the median instead of the mean\n",
    "\n",
    "human_scores_median = np.median(human_scores)\n",
    "\n",
    "gpt_j_scores_median = np.median(gpt_j_scores)\n",
    "\n",
    "gpt_neo_scores_median = np.median(gpt_neo_scores)\n",
    "\n",
    "gpt_2_scores_median = np.median(gpt_2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculate the median, ignoring the results where the generating model is used as a scoring model\n",
    "\n",
    "human_scores_no_base_mean = np.median(human_scores_no_base)\n",
    "\n",
    "gpt_j_scores_no_base_mean = np.median(gpt_j_scores_no_base)\n",
    "\n",
    "gpt_neo_scores_no_base_mean = np.median(gpt_neo_scores_no_base)\n",
    "\n",
    "gpt_2_scores_no_base_mean = np.median(gpt_2_scores_no_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the above code, but find the maximum instead of the mean\n",
    "\n",
    "human_scores_max = np.max(human_scores)\n",
    "\n",
    "gpt_j_scores_max = np.max(gpt_j_scores)\n",
    "\n",
    "gpt_neo_scores_max = np.max(gpt_neo_scores)\n",
    "\n",
    "gpt_2_scores_max = np.max(gpt_2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculate the mean, ignoring the results where the generating model is used as a scoring model\n",
    "\n",
    "human_scores_no_base_mean = np.max(human_scores_no_base)\n",
    "\n",
    "gpt_j_scores_no_base_mean = np.max(gpt_j_scores_no_base)\n",
    "\n",
    "gpt_neo_scores_no_base_mean = np.max(gpt_neo_scores_no_base)\n",
    "\n",
    "gpt_2_scores_no_base_mean = np.max(gpt_2_scores_no_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect_gpt_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
