{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DetectGPT Discrepancy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list,\n",
    "                                log_probs_per_text_transformed: list,\n",
    "                                normalization: bool=False) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts. Computed for n_perturbations perturbations.\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the n_perturbations perturbed log probs\n",
    "        normalization (bool): True if you want to normalize the discrepancy scores, False otherwise\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texts\n",
    "    \"\"\"\n",
    "    n_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = np.mean(perturbed_log_probs)  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            sigma = np.std(perturbed_log_probs)\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else discrepancy_score_unormalized\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUROC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_metrics(discrepancy_scores_human, discrepancy_scores_ai):\n",
    "    fpr, tpr, _ = roc_curve(\n",
    "    np.array([0] * len(discrepancy_scores_human) + [1] * len(discrepancy_scores_ai)),\n",
    "    np.concatenate([discrepancy_scores_human, discrepancy_scores_ai])\n",
    "    )\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr.tolist(), tpr.tolist(), float(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_grid(list_of_scores, title, row_titles, col_titles, row_heading, col_heading):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    fig.suptitle(title, fontsize=24, fontweight='bold', y=1.04)  # Title further away\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        human_scores = list_of_scores[i * 2]\n",
    "        ai_scores = list_of_scores[i * 2 + 1]\n",
    "\n",
    "        fpr,tpr,roc_auc = get_roc_metrics(human_scores, ai_scores)\n",
    "\n",
    "        ax.hist(human_scores, bins='auto', alpha=0.5, label='Human', edgecolor='black')\n",
    "        ax.hist(ai_scores, bins='auto', alpha=0.5, label='AI', edgecolor='black')\n",
    "\n",
    "\n",
    "        ax.set_title(f'AUROC {roc_auc:.2f}')\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.text(-0.25, 0.5, row_titles[i // 3], transform=ax.transAxes, fontsize=16, fontweight = 'bold',\n",
    "                    rotation=90, verticalalignment='center', horizontalalignment='center')\n",
    "\n",
    "        if i < 3:\n",
    "            ax.text(0.5, 1.15, col_titles[i], transform=ax.transAxes, fontsize=16, fontweight = 'bold',\n",
    "                    rotation='horizontal', verticalalignment='bottom', horizontalalignment='center')\n",
    "\n",
    "        if i >= 6:\n",
    "            ax.set_xlabel('Discrepancy Score')\n",
    "        \n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    fig.text(-0.06, 0.5, row_heading, va='center', rotation='vertical', fontsize=20, fontweight = 'bold')  # Equidistant from grid\n",
    "    fig.text(0.5, 0.95, col_heading, ha='center', fontsize=20, fontweight = 'bold')  # Centered relative to grid\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUROC Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auroc_grid(list_of_scores, title, row_titles, col_titles, row_heading, col_heading):\n",
    "    aurocs = [get_roc_metrics(list_of_scores[i * 2], list_of_scores[i * 2 + 1])[2] for i in range(9)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    auroc_matrix = np.array(aurocs).reshape(3, 3)\n",
    "    sns.heatmap(auroc_matrix, annot=True, fmt='.2f', cmap='Blues', linewidths=0.5, ax=ax, cbar_kws={'label': 'AUROC'})\n",
    "\n",
    "    ax.set_yticklabels(row_titles, fontsize=12, fontweight='bold', va='center', rotation=90)\n",
    "    \n",
    "    ax.set_xticklabels(col_titles, fontsize=12, fontweight='bold', ha='center', rotation=0)\n",
    "    ax.xaxis.set_label_position('top')  # Move x-axis label (column titles) to the top\n",
    "    ax.xaxis.tick_top()  # Ensure ticks and labels are on top\n",
    "    \n",
    "    # Set main title\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold', y=1.08)\n",
    "\n",
    "    # Set 'Scoring Model' and 'Generating Model' labels\n",
    "    fig.text(0, 0.5, row_heading, va='center', rotation='vertical', fontsize=14, fontweight='bold')\n",
    "    fig.text(0.5, 0.96, col_heading, ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected keys and structure\n",
    "EXPECTED_KEYS = {\"log_probs_base\", \"log_probs_transformed\", \"discrepancy_scores\"}\n",
    "\n",
    "def load_and_validate(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    # Check that the JSON object has exactly the three expected keys\n",
    "    if set(data.keys()) != EXPECTED_KEYS:\n",
    "        raise ValueError(f\"Expected keys {EXPECTED_KEYS}, but got {set(data.keys())}\")\n",
    "    # Validate lengths for each list\n",
    "    if len(data[\"log_probs_base\"]) != 200:\n",
    "        raise ValueError(\"log_probs_base should contain 200 floats\")\n",
    "    if len(data[\"log_probs_transformed\"]) != 200:\n",
    "        raise ValueError(\"log_probs_transformed should contain 200 lists\")\n",
    "    if len(data[\"discrepancy_scores\"]) != 200:\n",
    "        raise ValueError(\"discrepancy_scores should contain 200 floats\")\n",
    "    # Validate each inner list in log_probs_transformed has 100 floats\n",
    "    for idx, inner_list in enumerate(data[\"log_probs_transformed\"]):\n",
    "        if len(inner_list) != 100:\n",
    "            raise ValueError(f\"Inner list at index {idx} in log_probs_transformed does not contain 100 floats\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HUMAN TEXTS\n",
    "# -------------------------------------------------------------------\n",
    "# Generated by humans - GPT-J run\n",
    "human_computed_by_gpt_j_gpt_j_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250318_171708.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_neo_gpt_j_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250318_181503.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_2_gpt_j_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250318_183528.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Generated by humans - GPT-Neo run\n",
    "human_computed_by_gpt_j_gpt_neo_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250318_193811.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_neo_gpt_neo_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250318_203938.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_2_gpt_neo_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250318_210029.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Generated by humans - GPT-2 run\n",
    "human_computed_by_gpt_j_gpt_2_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250319_103854.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_neo_gpt_2_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250319_114542.jsonl\"\n",
    ")\n",
    "\n",
    "human_computed_by_gpt_2_gpt_2_run_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_Human__200_Samples__200_Max_Length__100_Perturbations__Generated_By_Human__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250319_120520.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# -------------------------------------------------------------------\n",
    "#  Generated by GPT-J\n",
    "ai_generated_by_gpt_j_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-j-6B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250318_175629.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-j-6B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250318_183320.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_j_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-j-6B__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250318_183732.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  Generated by GPT-Neo\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-neo-2.7B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250318_202111.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-neo-2.7B__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250318_205825.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_EleutherAI/gpt-neo-2.7B__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250318_210233.jsonl\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "#  Generated by GPT-2\n",
    "ai_generated_by_gpt_2_computed_by_gpt_j_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_openai-community/gpt2__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-j-6B__20250319_112531.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_neo_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_openai-community/gpt2__Perturbed_By_google-t5/t5-3b__Scored_By_EleutherAI/gpt-neo-2.7B__20250319_120321.jsonl\"\n",
    ")\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_2_dct = load_and_validate(\n",
    "    \"/workspace/Results/XSUM_AI__200_Samples__200_Max_Length__100_Perturbations__Generated_By_openai-community/gpt2__Perturbed_By_google-t5/t5-3b__Scored_By_openai-community/gpt2__20250319_120717.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate DetectGPT Discrepancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) GPT-J as Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN TEXTS\n",
    "human_computed_by_gpt_j_gpt_j_run = compute_detectgpt_discrepancy(human_computed_by_gpt_j_gpt_j_run_dct[\"log_probs_base\"], human_computed_by_gpt_j_gpt_j_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "human_computed_by_gpt_j_gpt_neo_run = compute_detectgpt_discrepancy(human_computed_by_gpt_j_gpt_neo_run_dct[\"log_probs_base\"], human_computed_by_gpt_j_gpt_neo_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "human_computed_by_gpt_j_gpt_2_run = compute_detectgpt_discrepancy(human_computed_by_gpt_j_gpt_2_run_dct[\"log_probs_base\"], human_computed_by_gpt_j_gpt_2_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "ai_generated_by_gpt_j_computed_by_gpt_j = compute_detectgpt_discrepancy(ai_generated_by_gpt_j_computed_by_gpt_j_dct[\"log_probs_base\"], ai_generated_by_gpt_j_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_j = compute_detectgpt_discrepancy(ai_generated_by_gpt_neo_computed_by_gpt_j_dct[\"log_probs_base\"], ai_generated_by_gpt_neo_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_j = compute_detectgpt_discrepancy(ai_generated_by_gpt_2_computed_by_gpt_j_dct[\"log_probs_base\"], ai_generated_by_gpt_2_computed_by_gpt_j_dct[\"log_probs_transformed\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) GPT-Neo as Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"EleutherAI/gpt-neo-2.7B\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN TEXTS\n",
    "human_computed_by_gpt_neo_gpt_j_run = compute_detectgpt_discrepancy(human_computed_by_gpt_neo_gpt_j_run_dct[\"log_probs_base\"], human_computed_by_gpt_neo_gpt_j_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "human_computed_by_gpt_neo_gpt_neo_run = compute_detectgpt_discrepancy(human_computed_by_gpt_neo_gpt_neo_run_dct[\"log_probs_base\"], human_computed_by_gpt_neo_gpt_neo_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "human_computed_by_gpt_neo_gpt_2_run = compute_detectgpt_discrepancy(human_computed_by_gpt_neo_gpt_2_run_dct[\"log_probs_base\"], human_computed_by_gpt_neo_gpt_2_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "ai_generated_by_gpt_j_computed_by_gpt_neo = compute_detectgpt_discrepancy(ai_generated_by_gpt_j_computed_by_gpt_neo_dct[\"log_probs_base\"], ai_generated_by_gpt_j_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_neo = compute_detectgpt_discrepancy(ai_generated_by_gpt_neo_computed_by_gpt_neo_dct[\"log_probs_base\"], ai_generated_by_gpt_neo_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_neo = compute_detectgpt_discrepancy(ai_generated_by_gpt_2_computed_by_gpt_neo_dct[\"log_probs_base\"], ai_generated_by_gpt_2_computed_by_gpt_neo_dct[\"log_probs_transformed\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) GPT-2 as Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"openai-community/gpt2\"\n",
    "# Model list (all tested)\n",
    "\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-medium\n",
    "# openai-community/gpt2-large\n",
    "# openai-community/gpt2-xl\n",
    "\n",
    "# EleutherAI/gpt-neo-2.7B\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "computation_model_kwargs = {}\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME or 'neox' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "if 'gpt-j' in COMPUTATION_MODEL_NAME:\n",
    "    computation_model_kwargs.update(dict(revision='float16'))\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, **computation_model_kwargs, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "computation_tokenizer.model_max_length = 1024 \n",
    "\n",
    "if computation_tokenizer.pad_token is None:\n",
    "    computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "computation_tokenizer.pad_token_id = computation_tokenizer.eos_token_id\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN TEXTS\n",
    "human_computed_by_gpt_2_gpt_j_run = compute_detectgpt_discrepancy(human_computed_by_gpt_2_gpt_j_run_dct[\"log_probs_base\"], human_computed_by_gpt_2_gpt_j_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "human_computed_by_gpt_2_gpt_neo_run = compute_detectgpt_discrepancy(human_computed_by_gpt_2_gpt_neo_run_dct[\"log_probs_base\"], human_computed_by_gpt_2_gpt_neo_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "human_computed_by_gpt_2_gpt_2_run = compute_detectgpt_discrepancy(human_computed_by_gpt_2_gpt_2_run_dct[\"log_probs_base\"], human_computed_by_gpt_2_gpt_2_run_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "ai_generated_by_gpt_j_computed_by_gpt_2 = compute_detectgpt_discrepancy(ai_generated_by_gpt_j_computed_by_gpt_2_dct[\"log_probs_base\"], ai_generated_by_gpt_j_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_neo_computed_by_gpt_2 = compute_detectgpt_discrepancy(ai_generated_by_gpt_neo_computed_by_gpt_2_dct[\"log_probs_base\"], ai_generated_by_gpt_neo_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)\n",
    "\n",
    "ai_generated_by_gpt_2_computed_by_gpt_2 = compute_detectgpt_discrepancy(ai_generated_by_gpt_2_computed_by_gpt_2_dct[\"log_probs_base\"], ai_generated_by_gpt_2_computed_by_gpt_2_dct[\"log_probs_transformed\"], normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organise Discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = [\n",
    "    human_computed_by_gpt_j_gpt_j_run, ai_generated_by_gpt_j_computed_by_gpt_j,   # Top-left\n",
    "    human_computed_by_gpt_neo_gpt_j_run, ai_generated_by_gpt_j_computed_by_gpt_neo, # Top-middle\n",
    "    human_computed_by_gpt_2_gpt_j_run, ai_generated_by_gpt_j_computed_by_gpt_2,   # Top-right\n",
    "    human_computed_by_gpt_j_gpt_neo_run, ai_generated_by_gpt_neo_computed_by_gpt_j, # Middle-left\n",
    "    human_computed_by_gpt_neo_gpt_neo_run, ai_generated_by_gpt_neo_computed_by_gpt_neo, # Middle\n",
    "    human_computed_by_gpt_2_gpt_neo_run, ai_generated_by_gpt_neo_computed_by_gpt_2, # Middle-right\n",
    "    human_computed_by_gpt_j_gpt_2_run, ai_generated_by_gpt_2_computed_by_gpt_j,   # Bottom-left\n",
    "    human_computed_by_gpt_neo_gpt_2_run, ai_generated_by_gpt_2_computed_by_gpt_neo, # Bottom-middle\n",
    "    human_computed_by_gpt_2_gpt_2_run, ai_generated_by_gpt_2_computed_by_gpt_2    # Bottom-right\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Visualisation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = 'Histograms of Discrepancy Scores'\n",
    "ROW_TITLES = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "COL_TITLES = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "ROW_HEADING = \"Generating Model\"\n",
    "COL_HEADING = \"Scoring Model\"\n",
    "\n",
    "TITLE_AUROC = 'Grid of AUROCs'\n",
    "\n",
    "plot_histogram_grid(scores_list, TITLE, ROW_TITLES, COL_TITLES, ROW_HEADING, COL_HEADING)\n",
    "plot_auroc_grid(scores_list, TITLE_AUROC, ROW_TITLES, COL_TITLES, ROW_HEADING, COL_HEADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics\n",
    "\n",
    "For every entry in a scores list, you replace the entry with a summary statistic of that entry across multiple lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_lists(list1, list2, list3):\n",
    "    return [(x + y + z) / 3 for x, y, z in zip(list1, list2, list3)]\n",
    "\n",
    "def average_lists_without_base(list1, list2):\n",
    "    return [(x + y) / 2 for x, y in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN TEXTS\n",
    "# Calculate the arithmetic mean of the scores for the human text in the GPT-J run\n",
    "human_scores_for_gpt_j_run_mean = average_lists(human_computed_by_gpt_j_gpt_j_run, human_computed_by_gpt_neo_gpt_j_run, human_computed_by_gpt_2_gpt_j_run)\n",
    "\n",
    "# Calculate the arithmetic mean of the scores for the human text in the GPT-Neo run\n",
    "human_scores_for_gpt_neo_run_mean = average_lists(human_computed_by_gpt_j_gpt_neo_run, human_computed_by_gpt_neo_gpt_neo_run, human_computed_by_gpt_2_gpt_neo_run)\n",
    "\n",
    "# Calculate the arithmetic mean of the scores for the human text in the GPT-2 run\n",
    "human_scores_for_gpt_2_run_mean = average_lists(human_computed_by_gpt_j_gpt_2_run, human_computed_by_gpt_neo_gpt_2_run, human_computed_by_gpt_2_gpt_2_run)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# Check whether any of the lists of scores for the GPT-J generated texts are the same (note that each entry is a list)\n",
    "assert not np.allclose(scores_list[1], scores_list[3], scores_list[5])\n",
    "# Calculate the arithmetic mean of the scores for the GPT-J generated text (entries 1, 3, 5)\n",
    "gpt_j_scores_mean = average_lists(ai_generated_by_gpt_j_computed_by_gpt_j, ai_generated_by_gpt_j_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_2)\n",
    "\n",
    "# Check whether any of the lists of scores for the GPT-Neo generated texts are the same (note that each entry is a list)\n",
    "assert not np.allclose(scores_list[7], scores_list[9], scores_list[11])\n",
    "# Calculate the arithmetic mean of the scores for the GPT-Neo generated text (entries 7, 9, 11)\n",
    "gpt_neo_scores_mean = average_lists(ai_generated_by_gpt_neo_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_neo, ai_generated_by_gpt_neo_computed_by_gpt_2)\n",
    "\n",
    "# Check whether any of the lists of scores for the GPT-2 generated texts are the same (note that each entry is a list)\n",
    "assert not np.allclose(scores_list[13], scores_list[15], scores_list[17])\n",
    "# Calculate the arithmetic mean of the scores for the GPT-2 generated text (entries 13, 15, 17)\n",
    "gpt_2_scores_mean = average_lists(ai_generated_by_gpt_2_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_neo, ai_generated_by_gpt_2_computed_by_gpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculate the mean, ignoring the results where the generating model is used as a scoring model\n",
    "\n",
    "# HUMAN TEXTS\n",
    "# Calculate the arithmetic mean of the scores for the human text in the GPT-J run, ignoring the GPT-J model\n",
    "human_scores_for_gpt_j_run_mean_no_base = average_lists_without_base(human_computed_by_gpt_neo_gpt_j_run, human_computed_by_gpt_2_gpt_j_run)\n",
    "\n",
    "# Calculate the arithmetic mean of the scores for the human text in the GPT-Neo run, ignoring the GPT-Neo model\n",
    "human_scores_for_gpt_neo_run_mean_no_base = average_lists_without_base(human_computed_by_gpt_j_gpt_neo_run, human_computed_by_gpt_2_gpt_neo_run)\n",
    "\n",
    "# Calculate the arithmetic mean of the scores for the human text in the GPT-2 run, ignoring the GPT-2 model\n",
    "human_scores_for_gpt_2_run_mean_no_base = average_lists_without_base(human_computed_by_gpt_j_gpt_2_run, human_computed_by_gpt_neo_gpt_2_run)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# Calculate the arithmetic mean of the scores for the GPT-J generated text, ignoring the GPT-J model\n",
    "gpt_j_scores_mean_no_base = average_lists_without_base(ai_generated_by_gpt_j_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the arithmetic mean of the scores for the GPT-Neo generated text, ignoring the GPT-Neo model\n",
    "gpt_neo_scores_mean_no_base = average_lists_without_base(ai_generated_by_gpt_neo_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the arithmetic mean of the scores for the GPT-2 generated text, ignoring the GPT-2 model\n",
    "gpt_2_scores_mean_no_base = average_lists_without_base(ai_generated_by_gpt_2_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_neo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_lists(list1, list2, list3):\n",
    "    return [np.median([x, y, z]) for x, y, z in zip(list1, list2, list3)]\n",
    "\n",
    "def median_lists_without_base(list1, list2):\n",
    "    return [np.median([x, y]) for x, y in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN TEXTS\n",
    "# Calculate the median of the scores for the human text in the GPT-J run\n",
    "human_scores_for_gpt_j_run_median= median_lists(human_computed_by_gpt_j_gpt_j_run, human_computed_by_gpt_neo_gpt_j_run, human_computed_by_gpt_2_gpt_j_run)\n",
    "\n",
    "# Calculate the median of the scores for the human text in the GPT-Neo run\n",
    "human_scores_for_gpt_neo_run_median = median_lists(human_computed_by_gpt_j_gpt_neo_run, human_computed_by_gpt_neo_gpt_neo_run, human_computed_by_gpt_2_gpt_neo_run)\n",
    "\n",
    "# Calculate the median of the scores for the human text in the GPT-2 run\n",
    "human_scores_for_gpt_2_run_median = median_lists(human_computed_by_gpt_j_gpt_2_run, human_computed_by_gpt_neo_gpt_2_run, human_computed_by_gpt_2_gpt_2_run)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# Calculate the median of the scores for the GPT-J generated text (entries 1, 3, 5)\n",
    "gpt_j_scores_median = median_lists(ai_generated_by_gpt_j_computed_by_gpt_j, ai_generated_by_gpt_j_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the median of the scores for the GPT-Neo generated text (entries 7, 9, 11)\n",
    "gpt_neo_scores_median = median_lists(ai_generated_by_gpt_neo_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_neo, ai_generated_by_gpt_neo_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the median of the scores for the GPT-2 generated text (entries 13, 15, 17)\n",
    "gpt_2_scores_median = median_lists(ai_generated_by_gpt_2_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_neo, ai_generated_by_gpt_2_computed_by_gpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculate the median, ignoring the results where the generating model is used as a scoring model\n",
    "\n",
    "# HUMAN TEXTS\n",
    "# Calculate the median of the scores for the human text in the GPT-J run, ignoring the GPT-J model\n",
    "human_scores_for_gpt_j_run_median_no_base = median_lists_without_base(human_computed_by_gpt_neo_gpt_j_run, human_computed_by_gpt_2_gpt_j_run)\n",
    "\n",
    "# Calculate the median of the scores for the human text in the GPT-Neo run, ignoring the GPT-Neo model\n",
    "human_scores_for_gpt_neo_run_median_no_base = median_lists_without_base(human_computed_by_gpt_j_gpt_neo_run, human_computed_by_gpt_2_gpt_neo_run)\n",
    "\n",
    "# Calculate the median of the scores for the human text in the GPT-2 run, ignoring the GPT-2 model\n",
    "human_scores_for_gpt_2_run_median_no_base = median_lists_without_base(human_computed_by_gpt_j_gpt_2_run, human_computed_by_gpt_neo_gpt_2_run)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# Calculate the median of the scores for the GPT-J generated text, ignoring the GPT-J model\n",
    "gpt_j_scores_median_no_base = median_lists_without_base(ai_generated_by_gpt_j_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the median of the scores for the GPT-Neo generated text, ignoring the GPT-Neo model\n",
    "gpt_neo_scores_median_no_base = median_lists_without_base(ai_generated_by_gpt_neo_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the median of the scores for the GPT-2 generated text, ignoring the GPT-2 model\n",
    "gpt_2_scores_median_no_base = median_lists_without_base(ai_generated_by_gpt_2_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_neo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_lists(list1, list2, list3):\n",
    "    return [max(x, y, z) for x, y, z in zip(list1, list2, list3)]\n",
    "\n",
    "def max_lists_without_base(list1, list2):\n",
    "    return [max(x, y) for x, y in zip(list1, list2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN TEXTS\n",
    "# Calculate the maximum of the scores for the human text in the GPT-J run\n",
    "human_scores_for_gpt_j_run_max = max_lists(human_computed_by_gpt_j_gpt_j_run, human_computed_by_gpt_neo_gpt_j_run, human_computed_by_gpt_2_gpt_j_run)\n",
    "\n",
    "# Calculate the maximum of the scores for the human text in the GPT-Neo run\n",
    "human_scores_for_gpt_neo_run_max = max_lists(human_computed_by_gpt_j_gpt_neo_run, human_computed_by_gpt_neo_gpt_neo_run, human_computed_by_gpt_2_gpt_neo_run)\n",
    "\n",
    "# Calculate the maximum of the scores for the human text in the GPT-2 run\n",
    "human_scores_for_gpt_2_run_max = max_lists(human_computed_by_gpt_j_gpt_2_run, human_computed_by_gpt_neo_gpt_2_run, human_computed_by_gpt_2_gpt_2_run)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# Calculate the maximum of the scores for the GPT-J generated text (entries 1, 3, 5)\n",
    "gpt_j_scores_max = max_lists(ai_generated_by_gpt_j_computed_by_gpt_j, ai_generated_by_gpt_j_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the maximum of the scores for the GPT-Neo generated text (entries 7, 9, 11)\n",
    "gpt_neo_scores_max = max_lists(ai_generated_by_gpt_neo_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_neo, ai_generated_by_gpt_neo_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the maximum of the scores for the GPT-2 generated text (entries 13, 15, 17)\n",
    "gpt_2_scores_max = max_lists(ai_generated_by_gpt_2_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_neo, ai_generated_by_gpt_2_computed_by_gpt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculate the maximum, ignoring the results where the generating model is used as a scoring model\n",
    "\n",
    "# HUMAN TEXTS\n",
    "# Calculate the maximum of the scores for the human text in the GPT-J run, ignoring the GPT-J model\n",
    "human_scores_for_gpt_j_run_max_no_base = max_lists_without_base(human_computed_by_gpt_neo_gpt_j_run, human_computed_by_gpt_2_gpt_j_run)\n",
    "\n",
    "# Calculate the maximum of the scores for the human text in the GPT-Neo run, ignoring the GPT-Neo model\n",
    "human_scores_for_gpt_neo_run_max_no_base = max_lists_without_base(human_computed_by_gpt_j_gpt_neo_run, human_computed_by_gpt_2_gpt_neo_run)\n",
    "\n",
    "# Calculate the maximum of the scores for the human text in the GPT-2 run, ignoring the GPT-2 model\n",
    "human_scores_for_gpt_2_run_max_no_base = max_lists_without_base(human_computed_by_gpt_j_gpt_2_run, human_computed_by_gpt_neo_gpt_2_run)\n",
    "\n",
    "\n",
    "# AI TEXTS\n",
    "# Calculate the maximum of the scores for the GPT-J generated text, ignoring the GPT-J model\n",
    "gpt_j_scores_max_no_base = max_lists_without_base(ai_generated_by_gpt_j_computed_by_gpt_neo, ai_generated_by_gpt_j_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the maximum of the scores for the GPT-Neo generated text, ignoring the GPT-Neo model\n",
    "gpt_neo_scores_max_no_base = max_lists_without_base(ai_generated_by_gpt_neo_computed_by_gpt_j, ai_generated_by_gpt_neo_computed_by_gpt_2)\n",
    "\n",
    "# Calculate the maximum of the scores for the GPT-2 generated text, ignoring the GPT-2 model\n",
    "gpt_2_scores_max_no_base = max_lists_without_base(ai_generated_by_gpt_2_computed_by_gpt_j, ai_generated_by_gpt_2_computed_by_gpt_neo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organise Ensemble Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list_ensemble = [\n",
    "    human_scores_for_gpt_j_run_mean, gpt_j_scores_mean,   # Top-left\n",
    "    human_scores_for_gpt_j_run_median, gpt_j_scores_median,   # Top-middle\n",
    "    human_scores_for_gpt_j_run_max, gpt_j_scores_max,   # Top-right\n",
    "    human_scores_for_gpt_neo_run_mean, gpt_neo_scores_mean, # Middle-left\n",
    "    human_scores_for_gpt_neo_run_median, gpt_neo_scores_median, # Middle\n",
    "    human_scores_for_gpt_neo_run_max, gpt_neo_scores_max, # Middle-right\n",
    "    human_scores_for_gpt_2_run_mean, gpt_2_scores_mean,   # Bottom-left\n",
    "    human_scores_for_gpt_2_run_median, gpt_2_scores_median,   # Bottom-middle\n",
    "    human_scores_for_gpt_2_run_max, gpt_2_scores_max    # Bottom-right\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list_ensemble_no_base = [\n",
    "    human_scores_for_gpt_j_run_mean_no_base, gpt_j_scores_mean_no_base,   # Top-left\n",
    "    human_scores_for_gpt_j_run_median_no_base, gpt_j_scores_median_no_base,   # Top-middle\n",
    "    human_scores_for_gpt_j_run_max_no_base, gpt_j_scores_max_no_base,   # Top-right\n",
    "    human_scores_for_gpt_neo_run_mean_no_base, gpt_neo_scores_mean_no_base, # Middle-left\n",
    "    human_scores_for_gpt_neo_run_median_no_base, gpt_neo_scores_median_no_base, # Middle\n",
    "    human_scores_for_gpt_neo_run_max_no_base, gpt_neo_scores_max_no_base, # Middle-right\n",
    "    human_scores_for_gpt_2_run_mean_no_base, gpt_2_scores_mean_no_base,   # Bottom-left\n",
    "    human_scores_for_gpt_2_run_median_no_base, gpt_2_scores_median_no_base,   # Bottom-middle\n",
    "    human_scores_for_gpt_2_run_max_no_base, gpt_2_scores_max_no_base    # Bottom-right\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rows should be the generating models and the columns should be the ensemble methods (mean, median, max)\n",
    "\n",
    "TITLE_ENSEMBLE = 'Histograms of Discrepancy Scores for Ensemble Methods'\n",
    "ROW_TITLES_ENSEMBLE = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "COL_TITLES_ENSEMBLE = [\"Mean\", \"Median\", \"Maximum\"]\n",
    "ROW_HEADING_ENSEMBLE = \"Generating Model\"\n",
    "COL_HEADING_ENSEMBLE = \"Ensemble Method\"\n",
    "\n",
    "TITLE_ENSEMBLE_AUROC = 'Grid of AUROCs for Ensemble Methods'\n",
    "\n",
    "plot_histogram_grid(scores_list_ensemble, TITLE_ENSEMBLE, ROW_TITLES_ENSEMBLE, COL_TITLES_ENSEMBLE, ROW_HEADING_ENSEMBLE, COL_HEADING_ENSEMBLE)\n",
    "plot_auroc_grid(scores_list_ensemble, TITLE_ENSEMBLE_AUROC, ROW_TITLES_ENSEMBLE, COL_TITLES_ENSEMBLE, ROW_HEADING_ENSEMBLE, COL_HEADING_ENSEMBLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles Excluding Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rows should be the generating models and the columns should be the ensemble methods (mean, median, max)\n",
    "\n",
    "TITLE_ENSEMBLE_NO_BASE = 'Histograms of Discrepancy Scores for Ensemble Methods (Ignoring Base Model)'\n",
    "ROW_TITLES_ENSEMBLE_NO_BASE = [\"GPT-J\", \"GPT-Neo\", \"GPT-2\"]\n",
    "COL_TITLES_ENSEMBLE_NO_BASE = [\"Mean\", \"Median\", \"Maximum\"]\n",
    "ROW_HEADING_ENSEMBLE_NO_BASE = \"Generating Model\"\n",
    "COL_HEADING_ENSEMBLE_NO_BASE = \"Ensemble Method\"\n",
    "\n",
    "TITLE_ENSEMBLE_AUROC_NO_BASE = 'Grid of AUROCs for Ensemble Methods (Ignoring Base Model)'\n",
    "plot_histogram_grid(scores_list_ensemble_no_base, TITLE_ENSEMBLE_NO_BASE, ROW_TITLES_ENSEMBLE_NO_BASE, COL_TITLES_ENSEMBLE_NO_BASE, ROW_HEADING_ENSEMBLE_NO_BASE, COL_HEADING_ENSEMBLE_NO_BASE)\n",
    "plot_auroc_grid(scores_list_ensemble_no_base, TITLE_ENSEMBLE_AUROC_NO_BASE, ROW_TITLES_ENSEMBLE_NO_BASE, COL_TITLES_ENSEMBLE_NO_BASE, ROW_HEADING_ENSEMBLE_NO_BASE, COL_HEADING_ENSEMBLE_NO_BASE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNLP_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
