{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-large\", torch_dtype=torch.float16, \n",
    "                                             device_map=\"auto\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t5_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./SubtaskB/subtaskB_train.jsonl\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data_human = [json.loads(line) for line in file if json.loads(line).get(\"model\") == \"human\"]\n",
    "\n",
    "# Print first 3 records\n",
    "print(data_human[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_text(texts, mask_ratio=0.15, max_words=370):\n",
    "    \"\"\"Mask multiple texts at once.\"\"\"\n",
    "    masked_texts = []\n",
    "    mask_indices_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        \n",
    "        # Truncate text\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        \n",
    "        num_masks = int(len(words) * mask_ratio)\n",
    "        \n",
    "        # Randomly select spans to mask\n",
    "        mask_indices = sorted(random.sample(range(len(words) - 1), num_masks))\n",
    "        mask_indices_list.append(mask_indices)\n",
    "        \n",
    "        for i, idx in enumerate(mask_indices):\n",
    "            words[idx] = f\"<extra_id_{i}>\"\n",
    "            if idx + 1 < len(words):  # Ensure a 2-word span\n",
    "                words[idx + 1] = \"\"\n",
    "        \n",
    "        masked_texts.append(\" \".join(words))\n",
    "    \n",
    "    return masked_texts, mask_indices_list\n",
    "\n",
    "def batch_replace_masks(texts, batch_size=8):\n",
    "    \"\"\"Generate T5 model outputs for masked texts in batches.\"\"\"\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        n_expected = [text.count(\"<extra_id_\") for text in batch_texts]\n",
    "        stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "        \n",
    "        tokens = t5_tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move input tensors to model's device\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=tokens[\"input_ids\"].to(t5_model.device),\n",
    "                attention_mask=tokens[\"attention_mask\"].to(t5_model.device),\n",
    "                max_length=150,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=stop_id\n",
    "            )\n",
    "            \n",
    "        # Move outputs back to CPU to save GPU memory\n",
    "        outputs = outputs.detach().cpu()\n",
    "        batch_decoded = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        all_outputs.extend(batch_decoded)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def batch_extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output for multiple texts.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "        \n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "    \n",
    "    return extracted_fills\n",
    "\n",
    "def batch_apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "        \n",
    "        filled_text = masked_text\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            filled_text = filled_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "        \n",
    "        filled_texts.append(filled_text)\n",
    "    \n",
    "    return filled_texts\n",
    "\n",
    "def batch_average_log_prob(texts, batch_size=8):\n",
    "    \"\"\"Calculate average log probability for multiple texts in batches.\"\"\"\n",
    "    all_log_probs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # For batch processing, we need to compute loss per sample\n",
    "        if hasattr(outputs, \"loss\") and outputs.loss.dim() == 0:\n",
    "            # If model returns a single loss value for the batch\n",
    "            avg_log_prob = -outputs.loss.item()\n",
    "            all_log_probs.extend([avg_log_prob] * len(batch_texts))\n",
    "        else:\n",
    "            # If we need to calculate per-sample loss\n",
    "            # This is a simplification - you might need to adjust based on your model's output\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "            loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                                       shift_labels.view(-1))\n",
    "            \n",
    "            # Reshape back to [batch_size, sequence_length]\n",
    "            loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "            \n",
    "            # Calculate average loss per sample by considering attention mask\n",
    "            sample_losses = []\n",
    "            for j in range(loss_per_token.size(0)):\n",
    "                # Use attention mask to identify real tokens\n",
    "                mask = attention_mask[j, 1:].bool()  # Shift to align with targets\n",
    "                if mask.sum() > 0:\n",
    "                    sample_loss = loss_per_token[j][mask].mean().item()\n",
    "                    sample_losses.append(-sample_loss)  # Negative loss is log probability\n",
    "                else:\n",
    "                    sample_losses.append(0.0)\n",
    "            \n",
    "            all_log_probs.extend(sample_losses)\n",
    "    \n",
    "    return all_log_probs\n",
    "\n",
    "# Main optimized processing loop\n",
    "def optimized_processing(data_human, num_samples=50, iterations=25, batch_size=8):\n",
    "    log_probs_per_text_base = []\n",
    "    log_probs_per_text_transformed = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data_human[j][\"text\"].split()[:50]) for j in range(num_samples)]\n",
    "    base_log_probs = batch_average_log_prob(original_texts, batch_size)\n",
    "    \n",
    "    # For each iteration, process all texts together in batches\n",
    "    for iter_idx in range(iterations):\n",
    "        # Step 1: Mask all texts at once\n",
    "        all_masked_texts, _ = batch_mask_text(original_texts)\n",
    "        \n",
    "        # Step 2: Generate replacements in batches\n",
    "        all_raw_fills = batch_replace_masks(all_masked_texts, batch_size)\n",
    "        \n",
    "        # Step 3: Extract fills\n",
    "        all_extracted_fills = batch_extract_fills(all_raw_fills)\n",
    "        \n",
    "        # Step 4: Apply fills\n",
    "        all_perturbed_texts = batch_apply_extracted_fills(all_masked_texts, all_extracted_fills)\n",
    "        \n",
    "        # Step 5: Calculate log probs in batches\n",
    "        all_log_probs = batch_average_log_prob(all_perturbed_texts, batch_size)\n",
    "        \n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if iter_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "    \n",
    "    # Store base log probs\n",
    "    log_probs_per_text_base = base_log_probs\n",
    "    \n",
    "    # Print results\n",
    "    for j in range(num_samples):\n",
    "        avg_log_prob_not = log_probs_per_text_base[j]\n",
    "        log_probs = log_probs_per_text_transformed[j]\n",
    "        \n",
    "        print(f\"Average per-token log probability for base sentence {j + 1}: {avg_log_prob_not:.4f}\")\n",
    "        print(f\"Average per-token log probability for transformed sentence {j + 1}: {(sum(log_probs) / len(log_probs)):.4f}, \"\n",
    "              f\"the minimum is {min(log_probs)} and the maximum is {max(log_probs)}\")\n",
    "    \n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed\n",
    "\n",
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Add caching for tokenization\n",
    "@lru_cache(maxsize=1024)\n",
    "def cached_tokenize(text, is_t5=False):\n",
    "    \"\"\"Cache tokenization results to avoid repeated work.\"\"\"\n",
    "    if is_t5:\n",
    "        return t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Example usage\n",
    "# log_probs_base, log_probs_transformed = optimized_processing(data_human, num_samples=50, iterations=25, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base, log_probs_transformed = optimized_processing(data_human, num_samples=10, iterations=25, batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
