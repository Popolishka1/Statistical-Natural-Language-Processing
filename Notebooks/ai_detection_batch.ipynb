{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"/tmp/huggingface\"\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-large\", torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", cache_dir=cache_dir)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t5_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, cache_dir=cache_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First human record: {'text': ' It will work with the Forza Motorsport disc in. Simply it will load but this problem will occur if your disc is dirty or damaged. Clean it with water on the back of the disc. There can be scratches because discs are fragile and can break easily. So always clean it with a soft, lint-free cloth.;\\n, If you have the game that includes the bonus feature, Xbox Live Arcade, You will get the dashboard menu that has \"Forza Motorsport\" and \"Xbox Live Arcade\". Choose Xbox Live Arcade if you want to play some arcade games, If you want to play Forza Motorsport, Simply press \"A\" on the Forza Motorsport Feature.\\n\\n\\nThe Main Menu features one of those choices after loading or creating a profile on the game.\\n\\nArcade Race: This is simply the quick race mode. You can race at a circuit, sprint, or a very interesting race. Just simply choose a class and choose a car and then choose a race and the race will load, Also the cars are featured in multiplayer, career, and free run mode.\\nCareer Mode: Actually this mode is the only mode to unlock cars and customize cars. It will be at the garage. Once you win a race, you may unlock a car and it will be added to your garage. The first car is a Lexus One. Unlock more cars and complete more races but if you did all of it, You are a winner.\\nMultiplayer, It\\'s the mode where a friend or a parent can join as a 2-player. It\\'s a 2-player mode, Go to head-to-head and you will get the split-screen that shows your car on the screen and your 2-player car.\\nTime Trial, Show your best to see how fast your car will complete in 1 minute or 5 minutes on a record.\\nFree Run, A mode where no operands are there but your the only person that can be featured in the Free Run mode, you can race by yourself and pretend that it\\'s a time trial race or go fast without any opponents trying to win.\\nOptions, Simply use this to setup your settings, sound, sfx, and more.\\n\\n\\n\\n, The first three tracks are Maple Valley II, Silverstone II, Tsukuba Circuit. You must complete all of them if you want to get more races like for an example, Road Atlanta II.\\n\\n\\nThe Career Mode has a short menu and a short things but it\\'s like the main menu. The choices are:\\n\\nGo Race, the Go Race mode is where you race to earn credits (money) and you unlock more tracks and stuff like cars. Simply if you race all of them, You\\'re winner.\\nThe Garage, Nothing special but the garage is where you keep cars, Customize a car or go into another car for the race. You must race in tracks if you want to get all of the cars.\\nBuy Cars, The mode is the where you can buy cars anytime you want but only if you have enough amount of credits, You can get cars from any country, the pictures are the flags of their country. And simply in the middle left corner of the screen, It will tell you the country (i.e. Sweden). And you get a car and it will be added into your garage.\\nTrain Drivatar, You know that you can easily train a drivatar (a driving avatar), which you can train by selecting a drivatar\\'s profile or creating one, and there is a menu. You can train it anytime you want to.\\nSet Difficulty, It will set the difficulty to the easiest setting to the hardest setting.\\n\\n\\nThe Multiplayer menu will feature the choices that will usually do with a Xbox account:\\n\\nXbox Live, Compete with other players around the world who play Forza Motorsport and you can also join a Car Club if you like.\\nSplit Screen, You can see yourself and your 2-player in this mode. It will split the screen into half. Simply, your facing into your screen and the other player faces into the 2nd player screen. Your racing into head-to-head.\\nSystem Link, Only for people who only signed in a Xbox Live Account. You can race with other players in the network or a hub.\\nScoreboards, Go to see who was fastest in the race you have.\\nXbox Live Sign In, Nothing special but to just login or sign up. Sign up means to create a account and join. Login means to sign in into your account.\\n\\n\\nFree Run features three races to choose from, here\\'s are the choices:\\n\\nHot Lap, It only features circuit races. Choose it to race around the track and there is unlimited laps.\\nAutocross, This is a very tricky race, Race with cones and you must stay on the street. The time will be penalized if you have a missing gate or knocking the cones down.\\nPoint-to-Point, Sprint tracks only, You can practice with any car on a sprint race where you must not go around a track but finished into a location-to-location.\\n\\n\\n\\n,, It will show your cars you got.\\n\\n,, To reverse, Press the Left Trigger Button. Press the B button to hand brake, Or use a control manual or so.\\n\\n, Once at the beginning of the race, Instead of a \"3, 2, 1, Go!\" or \"Ready, Set, Go!\" They have those blue circle things. There are three of them and make some kind of noise, They disappear at each second. When every thing is gone and when you hear the \"eerrr\". You start to race and you can go fast as you want as long as you don\\'t hit anything but you do this by an accident.\\n\\n, You can see those flat arrows on the road, They feature in almost every race except for Autocross race which makes it more trickier. The green arrows mean that you can turn it easily and the yellow arrows means it\\'s normal but be careful with the roads and the red arrows means a hard and sharp turn. You must slow down speed when you see the red arrows. That\\'s how the game works.\\n\\n, The right thumbstick will be used as a camera.\\n\\n,,, If you like this game and if you have a Xbox 360, you can get the sequel \"Forza Motorsport 2\", The sequel is only for Xbox 360 players. The new game, Forza Motorsport 3 is coming to release in 2009.\\n\\n', 'model': 'human', 'source': 'wikihow', 'label': 0, 'id': 2000}\n",
      "First AI record: {'text': 'Forza Motorsport is a popular racing game that provides players with the ability to race on various tracks and in different vehicles. Whether you\\'re a seasoned racer or a newbie, playing Forza Motorsport can be a fun experience. In this article, we will take you through the different steps on how to play Forza Motorsport.\\n\\nStep 1. Insert The Game Disc\\n\\nThe first step is to insert the game disc into your console or computer. Follow the instructions to set up the game.\\n\\nStep 2. Choose Your Game\\n\\nOnce the game is set up, choose the game you\\'d like to play. Forza Motorsport has different modes: Career, Free Play, and Arcade. In this article, we will focus on the Arcade mode.\\n\\nStep 3. Just Make A Quick Race By The Arcade Mode\\n\\nOnce the Arcade mode is selected, choose \"Quick Race\" to get started quickly.\\n\\nStep 4. Pick A Racetrack\\n\\nPick a racetrack from the different ones available like Road Atlanta, New York, Rio de Janeiro, Maple Valley, or anything to choose from.\\n\\nStep 5. Pick A Class And A Car\\n\\nForza Motorsport has different car classes, from Classes A, B, C, D, S to R. Pick a class that suits your gaming style and choose a car of your liking.\\n\\nStep 6. The Options Panel\\n\\nThe Options Panel opens automatically after choosing a car. Press \"OK\", and be patient while the race is loading. It will show what time, wind direction, wind heading, and miles.\\n\\nStep 7. Accelerate\\n\\nTo accelerate, press the Right trigger to make the car engine running.\\n\\nStep 8. Go!\\n\\nOnce acceleration is complete, let go of the brakes and hit the gas to go!\\n\\nStep 9. The Arrows\\n\\nTo steer, use the left thumbstick and to turn to the left, move the thumbstick left, and to turn right, move the thumbstick right.\\n\\nStep 10. Pause The Game\\n\\nTo pause the game, just press the \"start\" button. If you feel lonely and wanted an operant, choose the \"Ghost car\" setting and turn it on.\\n\\nStep 11. Replay the Race\\n\\nIf you want to see how well you did, use this feature to see a video of the race, and it will show you turns for some reason.\\n\\nStep 12. Have Fun And Enjoy The Game!\\n\\nIn conclusion, Forza Motorsport can be a fun game with its impressive graphics, user-friendly interface, and thrilling gameplay. Follow these steps to improve your gameplay and have fun.', 'model': 'chatGPT', 'source': 'wikihow', 'label': 1, 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"subtaskB_train.jsonl\"\n",
    "\n",
    "data_human = []\n",
    "data_ai = []\n",
    "\n",
    "# Efficiently process the file line by line\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") == \"human\":\n",
    "            data_human.append(record)\n",
    "        else:\n",
    "            data_ai.append(record)\n",
    "\n",
    "# Print the first record of each category\n",
    "print(\"First human record:\", data_human[0] if data_human else \"No human data found.\")\n",
    "print(\"First AI record:\", data_ai[0] if data_ai else \"No AI data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_text(texts, mask_ratio=0.15, max_words=370):\n",
    "    \"\"\"Mask multiple texts at once.\"\"\"\n",
    "    masked_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        \n",
    "        # Truncate text\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        \n",
    "        num_masks = int(len(words) * mask_ratio)\n",
    "        \n",
    "        # Randomly select spans to mask (sorted in reverse to avoid index shifts)\n",
    "        mask_indices = sorted(random.sample(range(len(words) - 1), num_masks), reverse=True)\n",
    "        \n",
    "        for i, idx in enumerate(mask_indices):\n",
    "            words[idx] = f\"<extra_id_{i}>\"\n",
    "            if idx + 1 < len(words):  # Ensure a 2-word span\n",
    "                del words[idx + 1]  # Remove instead of replacing with \"\"\n",
    "        \n",
    "        masked_texts.append(\" \".join(words))\n",
    "    \n",
    "    return masked_texts\n",
    "\n",
    "\n",
    "def batch_replace_masks(texts, batch_size=8):\n",
    "    \"\"\"Generate T5 model outputs for masked texts in batches.\"\"\"\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        n_expected = [text.count(\"<extra_id_\") for text in batch_texts]\n",
    "        stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "        \n",
    "        tokens = t5_tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move input tensors to model's device\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=tokens[\"input_ids\"].to(t5_model.device),\n",
    "                attention_mask=tokens[\"attention_mask\"].to(t5_model.device),\n",
    "                max_length=150,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=stop_id\n",
    "            )\n",
    "            \n",
    "        # Move outputs back to CPU to save GPU memory\n",
    "        outputs = outputs.detach().cpu()\n",
    "        batch_decoded = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        all_outputs.extend(batch_decoded)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def batch_extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output for multiple texts.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "        \n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "    \n",
    "    return extracted_fills\n",
    "\n",
    "def batch_apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "        \n",
    "        filled_text = masked_text\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            filled_text = filled_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "        \n",
    "        filled_texts.append(filled_text)\n",
    "    \n",
    "    return filled_texts\n",
    "\n",
    "def batch_average_log_prob(texts, batch_size=8):\n",
    "    \"\"\"Calculate average log probability for multiple texts in batches.\"\"\"\n",
    "    \n",
    "    all_log_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract logits\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Shift logits and labels to align\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()  # Ensure mask aligns\n",
    "\n",
    "        # Compute per-token loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=tokenizer.pad_token_id)\n",
    "        loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Reshape to [batch_size, seq_length - 1]\n",
    "        loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "        # Compute per-sample log prob\n",
    "        sample_losses = []\n",
    "        for j in range(loss_per_token.size(0)):\n",
    "            mask = shift_mask[j].bool()  # Use shift_mask for actual tokens\n",
    "            if mask.sum() > 0:\n",
    "                sample_loss = loss_per_token[j][mask].mean().item()\n",
    "                sample_losses.append(-sample_loss)  # Negative loss as log prob\n",
    "            else:\n",
    "                sample_losses.append(float('-inf'))  # Avoid zero prob bias\n",
    "\n",
    "        all_log_probs.extend(sample_losses)\n",
    "\n",
    "    return all_log_probs\n",
    "\n",
    "\n",
    "# Main optimized processing loop\n",
    "def optimized_processing(data_human, num_samples=50, iterations=25, batch_size=8):\n",
    "    log_probs_per_text_base = []\n",
    "    log_probs_per_text_transformed = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data_human[j][\"text\"].split()[:50]) for j in range(num_samples)]\n",
    "    base_log_probs = batch_average_log_prob(original_texts, batch_size)\n",
    "    \n",
    "    # Inside the loop in optimized_processing()\n",
    "    for iter_idx in range(iterations):\n",
    "        all_masked_texts = batch_mask_text(original_texts)\n",
    "        all_raw_fills = batch_replace_masks(all_masked_texts, batch_size)\n",
    "        all_extracted_fills = batch_extract_fills(all_raw_fills)\n",
    "        all_perturbed_texts = batch_apply_extracted_fills(all_masked_texts, all_extracted_fills)\n",
    "\n",
    "        all_log_probs = batch_average_log_prob(all_perturbed_texts, batch_size)\n",
    "        \n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if iter_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "    \n",
    "    return base_log_probs, log_probs_per_text_transformed\n",
    "\n",
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Add caching for tokenization\n",
    "@lru_cache(maxsize=1024)\n",
    "def cached_tokenize(text, is_t5=False):\n",
    "    \"\"\"Cache tokenization results to avoid repeated work.\"\"\"\n",
    "    if is_t5:\n",
    "        return t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 300\n",
    "iterations = 100\n",
    "batch_size = 128\n",
    "\n",
    "log_probs_base, log_probs_transformed = optimized_processing(data_human, num_samples=num_samples, iterations=iterations, batch_size=batch_size)\n",
    "\n",
    "results = {}\n",
    "results[\"log_probs_base\"] = log_probs_base\n",
    "results[\"log_probs_transformed\"] = log_probs_transformed\n",
    "\n",
    "with open(\"results_human.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "log_probs_base, log_probs_transformed = optimized_processing(data_ai, num_samples=num_samples, iterations=iterations, batch_size=batch_size)\n",
    "\n",
    "results = {}\n",
    "results[\"log_probs_base\"] = log_probs_base\n",
    "results[\"log_probs_transformed\"] = log_probs_transformed\n",
    "\n",
    "with open(\"results_ai.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
