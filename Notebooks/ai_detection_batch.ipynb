{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 3.06MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 2.95G/2.95G [00:34<00:00, 86.6MB/s]\n",
      "Downloading generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 431kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 3.53MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 4.61MB/s]\n",
      "/workspace/.miniconda3/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"/tmp/huggingface\"\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-large\", torch_dtype=torch.float16, cache_dir=cache_dir)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-large\", cache_dir=cache_dir)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "t5_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 67.1kB/s]\n",
      "Downloading config.json: 100%|██████████| 666/666 [00:00<00:00, 1.73MB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.65MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.04MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.50MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 3.25G/3.25G [00:39<00:00, 82.7MB/s]\n",
      "Downloading generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 390kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, cache_dir=cache_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' It will work with the Forza Motorsport disc in. Simply it will load but this problem will occur if your disc is dirty or damaged. Clean it with water on the back of the disc. There can be scratches because discs are fragile and can break easily. So always clean it with a soft, lint-free cloth.;\\n, If you have the game that includes the bonus feature, Xbox Live Arcade, You will get the dashboard menu that has \"Forza Motorsport\" and \"Xbox Live Arcade\". Choose Xbox Live Arcade if you want to play some arcade games, If you want to play Forza Motorsport, Simply press \"A\" on the Forza Motorsport Feature.\\n\\n\\nThe Main Menu features one of those choices after loading or creating a profile on the game.\\n\\nArcade Race: This is simply the quick race mode. You can race at a circuit, sprint, or a very interesting race. Just simply choose a class and choose a car and then choose a race and the race will load, Also the cars are featured in multiplayer, career, and free run mode.\\nCareer Mode: Actually this mode is the only mode to unlock cars and customize cars. It will be at the garage. Once you win a race, you may unlock a car and it will be added to your garage. The first car is a Lexus One. Unlock more cars and complete more races but if you did all of it, You are a winner.\\nMultiplayer, It\\'s the mode where a friend or a parent can join as a 2-player. It\\'s a 2-player mode, Go to head-to-head and you will get the split-screen that shows your car on the screen and your 2-player car.\\nTime Trial, Show your best to see how fast your car will complete in 1 minute or 5 minutes on a record.\\nFree Run, A mode where no operands are there but your the only person that can be featured in the Free Run mode, you can race by yourself and pretend that it\\'s a time trial race or go fast without any opponents trying to win.\\nOptions, Simply use this to setup your settings, sound, sfx, and more.\\n\\n\\n\\n, The first three tracks are Maple Valley II, Silverstone II, Tsukuba Circuit. You must complete all of them if you want to get more races like for an example, Road Atlanta II.\\n\\n\\nThe Career Mode has a short menu and a short things but it\\'s like the main menu. The choices are:\\n\\nGo Race, the Go Race mode is where you race to earn credits (money) and you unlock more tracks and stuff like cars. Simply if you race all of them, You\\'re winner.\\nThe Garage, Nothing special but the garage is where you keep cars, Customize a car or go into another car for the race. You must race in tracks if you want to get all of the cars.\\nBuy Cars, The mode is the where you can buy cars anytime you want but only if you have enough amount of credits, You can get cars from any country, the pictures are the flags of their country. And simply in the middle left corner of the screen, It will tell you the country (i.e. Sweden). And you get a car and it will be added into your garage.\\nTrain Drivatar, You know that you can easily train a drivatar (a driving avatar), which you can train by selecting a drivatar\\'s profile or creating one, and there is a menu. You can train it anytime you want to.\\nSet Difficulty, It will set the difficulty to the easiest setting to the hardest setting.\\n\\n\\nThe Multiplayer menu will feature the choices that will usually do with a Xbox account:\\n\\nXbox Live, Compete with other players around the world who play Forza Motorsport and you can also join a Car Club if you like.\\nSplit Screen, You can see yourself and your 2-player in this mode. It will split the screen into half. Simply, your facing into your screen and the other player faces into the 2nd player screen. Your racing into head-to-head.\\nSystem Link, Only for people who only signed in a Xbox Live Account. You can race with other players in the network or a hub.\\nScoreboards, Go to see who was fastest in the race you have.\\nXbox Live Sign In, Nothing special but to just login or sign up. Sign up means to create a account and join. Login means to sign in into your account.\\n\\n\\nFree Run features three races to choose from, here\\'s are the choices:\\n\\nHot Lap, It only features circuit races. Choose it to race around the track and there is unlimited laps.\\nAutocross, This is a very tricky race, Race with cones and you must stay on the street. The time will be penalized if you have a missing gate or knocking the cones down.\\nPoint-to-Point, Sprint tracks only, You can practice with any car on a sprint race where you must not go around a track but finished into a location-to-location.\\n\\n\\n\\n,, It will show your cars you got.\\n\\n,, To reverse, Press the Left Trigger Button. Press the B button to hand brake, Or use a control manual or so.\\n\\n, Once at the beginning of the race, Instead of a \"3, 2, 1, Go!\" or \"Ready, Set, Go!\" They have those blue circle things. There are three of them and make some kind of noise, They disappear at each second. When every thing is gone and when you hear the \"eerrr\". You start to race and you can go fast as you want as long as you don\\'t hit anything but you do this by an accident.\\n\\n, You can see those flat arrows on the road, They feature in almost every race except for Autocross race which makes it more trickier. The green arrows mean that you can turn it easily and the yellow arrows means it\\'s normal but be careful with the roads and the red arrows means a hard and sharp turn. You must slow down speed when you see the red arrows. That\\'s how the game works.\\n\\n, The right thumbstick will be used as a camera.\\n\\n,,, If you like this game and if you have a Xbox 360, you can get the sequel \"Forza Motorsport 2\", The sequel is only for Xbox 360 players. The new game, Forza Motorsport 3 is coming to release in 2009.\\n\\n', 'model': 'human', 'source': 'wikihow', 'label': 0, 'id': 2000}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"subtaskB_train.jsonl\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data_human = [json.loads(line) for line in file if json.loads(line).get(\"model\") == \"human\"]\n",
    "\n",
    "# Print first 3 records\n",
    "print(data_human[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_text(texts, mask_ratio=0.15, max_words=370):\n",
    "    \"\"\"Mask multiple texts at once.\"\"\"\n",
    "    masked_texts = []\n",
    "    mask_indices_list = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        \n",
    "        # Truncate text\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        \n",
    "        num_masks = int(len(words) * mask_ratio)\n",
    "        \n",
    "        # Randomly select spans to mask\n",
    "        mask_indices = sorted(random.sample(range(len(words) - 1), num_masks))\n",
    "        mask_indices_list.append(mask_indices)\n",
    "        \n",
    "        for i, idx in enumerate(mask_indices):\n",
    "            words[idx] = f\"<extra_id_{i}>\"\n",
    "            if idx + 1 < len(words):  # Ensure a 2-word span\n",
    "                words[idx + 1] = \"\"\n",
    "        \n",
    "        masked_texts.append(\" \".join(words))\n",
    "    \n",
    "    return masked_texts, mask_indices_list\n",
    "\n",
    "def batch_replace_masks(texts, batch_size=8):\n",
    "    \"\"\"Generate T5 model outputs for masked texts in batches.\"\"\"\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        n_expected = [text.count(\"<extra_id_\") for text in batch_texts]\n",
    "        stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "        \n",
    "        tokens = t5_tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move input tensors to model's device\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=tokens[\"input_ids\"].to(t5_model.device),\n",
    "                attention_mask=tokens[\"attention_mask\"].to(t5_model.device),\n",
    "                max_length=150,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=stop_id\n",
    "            )\n",
    "            \n",
    "        # Move outputs back to CPU to save GPU memory\n",
    "        outputs = outputs.detach().cpu()\n",
    "        batch_decoded = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        all_outputs.extend(batch_decoded)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def batch_extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output for multiple texts.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "        \n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "    \n",
    "    return extracted_fills\n",
    "\n",
    "def batch_apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "        \n",
    "        filled_text = masked_text\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            filled_text = filled_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "        \n",
    "        filled_texts.append(filled_text)\n",
    "    \n",
    "    return filled_texts\n",
    "\n",
    "def batch_average_log_prob(texts, batch_size=8):\n",
    "    \"\"\"Calculate average log probability for multiple texts in batches.\"\"\"\n",
    "    all_log_probs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # For batch processing, we need to compute loss per sample\n",
    "        if hasattr(outputs, \"loss\") and outputs.loss.dim() == 0:\n",
    "            # If model returns a single loss value for the batch\n",
    "            avg_log_prob = -outputs.loss.item()\n",
    "            all_log_probs.extend([avg_log_prob] * len(batch_texts))\n",
    "        else:\n",
    "            # If we need to calculate per-sample loss\n",
    "            # This is a simplification - you might need to adjust based on your model's output\n",
    "            logits = outputs.logits\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "            loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n",
    "                                       shift_labels.view(-1))\n",
    "            \n",
    "            # Reshape back to [batch_size, sequence_length]\n",
    "            loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "            \n",
    "            # Calculate average loss per sample by considering attention mask\n",
    "            sample_losses = []\n",
    "            for j in range(loss_per_token.size(0)):\n",
    "                # Use attention mask to identify real tokens\n",
    "                mask = attention_mask[j, 1:].bool()  # Shift to align with targets\n",
    "                if mask.sum() > 0:\n",
    "                    sample_loss = loss_per_token[j][mask].mean().item()\n",
    "                    sample_losses.append(-sample_loss)  # Negative loss is log probability\n",
    "                else:\n",
    "                    sample_losses.append(0.0)\n",
    "            \n",
    "            all_log_probs.extend(sample_losses)\n",
    "    \n",
    "    return all_log_probs\n",
    "\n",
    "# Main optimized processing loop\n",
    "def optimized_processing(data_human, num_samples=50, iterations=25, batch_size=8):\n",
    "    log_probs_per_text_base = []\n",
    "    log_probs_per_text_transformed = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data_human[j][\"text\"].split()[:50]) for j in range(num_samples)]\n",
    "    base_log_probs = batch_average_log_prob(original_texts, batch_size)\n",
    "    \n",
    "    # For each iteration, process all texts together in batches\n",
    "    for iter_idx in range(iterations):\n",
    "        # Step 1: Mask all texts at once\n",
    "        all_masked_texts, _ = batch_mask_text(original_texts)\n",
    "        \n",
    "        # Step 2: Generate replacements in batches\n",
    "        all_raw_fills = batch_replace_masks(all_masked_texts, batch_size)\n",
    "        \n",
    "        # Step 3: Extract fills\n",
    "        all_extracted_fills = batch_extract_fills(all_raw_fills)\n",
    "        \n",
    "        # Step 4: Apply fills\n",
    "        all_perturbed_texts = batch_apply_extracted_fills(all_masked_texts, all_extracted_fills)\n",
    "        \n",
    "        # Step 5: Calculate log probs in batches\n",
    "        all_log_probs = batch_average_log_prob(all_perturbed_texts, batch_size)\n",
    "        \n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if iter_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "    \n",
    "    # Store base log probs\n",
    "    log_probs_per_text_base = base_log_probs\n",
    "    \n",
    "    # Print results\n",
    "    for j in range(num_samples):\n",
    "        avg_log_prob_not = log_probs_per_text_base[j]\n",
    "        log_probs = log_probs_per_text_transformed[j]\n",
    "        \n",
    "        # print(f\"Average per-token log probability for base sentence {j + 1}: {avg_log_prob_not:.4f}\")\n",
    "        # print(f\"Average per-token log probability for transformed sentence {j + 1}: {(sum(log_probs) / len(log_probs)):.4f}, \"\n",
    "        #       f\"the minimum is {min(log_probs)} and the maximum is {max(log_probs)}\")\n",
    "    \n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed\n",
    "\n",
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Add caching for tokenization\n",
    "@lru_cache(maxsize=1024)\n",
    "def cached_tokenize(text, is_t5=False):\n",
    "    \"\"\"Cache tokenization results to avoid repeated work.\"\"\"\n",
    "    if is_t5:\n",
    "        return t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Example usage\n",
    "# log_probs_base, log_probs_transformed = optimized_processing(data_human, num_samples=50, iterations=25, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base, log_probs_transformed = optimized_processing(data_human, num_samples=100, iterations=25, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
