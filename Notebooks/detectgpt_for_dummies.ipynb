{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT for dummies**: Identifying AI-generated text\n",
    "This notebook implements the **DetectGPT** method from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- **Model setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a simple setup of different transformer based models that will be needed to:\n",
    "1. produce the AI-generated text - ``generation_model``\n",
    "2. compute the log-probablities - ``computation_model``\n",
    "3. perturb the text with the T5 perturbation - ``t5_model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulh\\miniconda3\\envs\\detectgpt_env\\lib\\site-packages\\huggingface_hub-0.28.1-py3.8.egg\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "\n",
    "GENERATION_MODEL_NAME = \"gpt2\"\n",
    "# Model list (all tested)\n",
    "# gpt2\n",
    "# gpt2-large\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(GENERATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "generation_model.eval()\n",
    "\n",
    "generation_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"openai-community/gpt2-large\"\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "\n",
    "# Set model to evaluation mode (ensures stable log prob estimation + disables dropout)\n",
    "computation_model.eval()\n",
    "\n",
    "computation_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulh\\miniconda3\\envs\\detectgpt_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:220: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "PERTURBATION_MODEL_NAME = \"t5-large\"\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(PERTURBATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(PERTURBATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Set to evaluation mode\n",
    "t5_model.eval()\n",
    "\n",
    "t5_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- **Code setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **ðŸ”€ Text perturbation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the **T5-based perturbation function**, which modifies the input text slightly while preserving its meaning. \n",
    "\n",
    "- **Why is perturbation needed?** AI-generated text often sits in **low-curvature** probability regions, meaning slight perturbations can significantly change their log probabilities\n",
    "- **How does it work?** The **T5 model** introduces variations to the text and helps in detecting AI-generated content\n",
    "\n",
    "These perturbed texts will later be compared to their original versions to compute the discrepancy scores d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_text(texts, mask_ratio=0.15, max_words=370):\n",
    "    \"\"\"Mask multiple texts at once.\"\"\"\n",
    "    masked_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        \n",
    "        # Truncate text\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        \n",
    "        num_masks = int(len(words) * mask_ratio)\n",
    "        \n",
    "        # Randomly select spans to mask (sorted in reverse to avoid index shifts)\n",
    "        mask_indices = sorted(random.sample(range(len(words) - 1), num_masks), reverse=True)\n",
    "        \n",
    "        for i, idx in enumerate(mask_indices):\n",
    "            words[idx] = f\"<extra_id_{i}>\"\n",
    "            if idx + 1 < len(words):  # Ensure a 2-word span\n",
    "                del words[idx + 1]  # Remove instead of replacing with \"\"\n",
    "        \n",
    "        masked_texts.append(\" \".join(words))\n",
    "    \n",
    "    return masked_texts\n",
    "\n",
    "def batch_replace_masks(texts, batch_size=128):\n",
    "    \"\"\"Generate T5 model outputs for masked texts in batches.\"\"\"\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        n_expected = [text.count(\"<extra_id_\") for text in batch_texts]\n",
    "        stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "        \n",
    "        tokens = t5_tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move input tensors to model's device\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=tokens[\"input_ids\"].to(t5_model.device),\n",
    "                attention_mask=tokens[\"attention_mask\"].to(t5_model.device),\n",
    "                max_length=150,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=stop_id\n",
    "            )\n",
    "            \n",
    "        # Move outputs back to CPU to save GPU memory\n",
    "        outputs = outputs.detach().cpu()\n",
    "        batch_decoded = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        all_outputs.extend(batch_decoded)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def batch_extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output for multiple texts.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "        \n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "    \n",
    "    return extracted_fills\n",
    "\n",
    "def batch_apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "        \n",
    "        filled_text = masked_text\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            filled_text = filled_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "        \n",
    "        filled_texts.append(filled_text)\n",
    "    \n",
    "    return filled_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_perturbation(text: str, batch_size: int) -> str:\n",
    "    \"\"\"\n",
    "    T5 perturbation - batch version\n",
    "\n",
    "    Args:\n",
    "        text (str): the input texts to be perturbed\n",
    "        batch_size (int): batch_size for compute\n",
    "\n",
    "    Returns:\n",
    "        all_perturbed_texts (str): the perturbed texts\n",
    "    \"\"\"\n",
    "    # Step 1: mask all texts at once\n",
    "    all_masked_texts = batch_mask_text(text)\n",
    "\n",
    "    # Step 2: generate replacements in batches\n",
    "    all_raw_fills = batch_replace_masks(all_masked_texts, batch_size)\n",
    "\n",
    "    # Step 3: extract fills\n",
    "    all_extracted_fills = batch_extract_fills(all_raw_fills)\n",
    "\n",
    "    # Step 4: apply fills\n",
    "    all_perturbed_texts = batch_apply_extracted_fills(all_masked_texts, all_extracted_fills)\n",
    "    \n",
    "    return all_perturbed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **ðŸ” Main functions: *DetectGPT* Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the **DetectGPT method**.\n",
    "\n",
    "- **Key idea:** once again, AI-generated texts often **reside in low-curvature probability regions**.\n",
    "- **How does it work?**\n",
    "  - We perturb the text multiple times (``num_perturbation``). We will use ``n_samples`` texts with ``max_length`` words.\n",
    "  - Compute log probabilities for both **original** and **perturbed** texts\n",
    "  - Measure the **discrepancy score** (a higher score suggests AI-generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_average_log_prob(texts, batch_size=128):\n",
    "    \"\"\"Calculate average log probability for multiple texts in batches.\"\"\"\n",
    "    \n",
    "    all_log_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = computation_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = computation_model(input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract logits\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Shift logits and labels to align\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()  # Ensure mask aligns\n",
    "\n",
    "        # Compute per-token loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=computation_tokenizer.pad_token_id)\n",
    "        loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Reshape to [batch_size, seq_length - 1]\n",
    "        loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "        # Compute per-sample log prob\n",
    "        sample_losses = []\n",
    "        for j in range(loss_per_token.size(0)):\n",
    "            mask = shift_mask[j].bool()  # Use shift_mask for actual tokens\n",
    "            if mask.sum() > 0:\n",
    "                sample_loss = loss_per_token[j][mask].mean().item()\n",
    "                sample_losses.append(-sample_loss)  # Negative loss as log prob\n",
    "            else:\n",
    "                sample_losses.append(float('-inf'))  # Avoid zero prob bias\n",
    "\n",
    "        all_log_probs.extend(sample_losses)\n",
    "\n",
    "    return all_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimized processing loop\n",
    "def optimized_processing(data, num_samples=300, max_length=50, num_perturbation=100, batch_size=128):\n",
    "    log_probs_per_text_transformed = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(num_samples)]\n",
    "    log_probs_per_text_base = batch_average_log_prob(original_texts, batch_size)\n",
    "    \n",
    "    # Inside the loop in optimized_processing()\n",
    "    for perturbation_idx in tqdm(range(num_perturbation), desc=f\"Processing {num_perturbation} perturbations for {num_samples} texts. Perturbation number:\"):\n",
    "        all_perturbed_texts = t5_perturbation(original_texts,batch_size)\n",
    "        all_log_probs = batch_average_log_prob(all_perturbed_texts, batch_size)\n",
    "        \n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "    \n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_perturbed(data, num_samples=300, max_length=50, num_perturbation=100, batch_size=128):\n",
    "    '''Generates pertubations for text. Returns list of length num_pertubations, each entry being a JSON object with perturbed text'''\n",
    "    # Initialise list to store all perturbed JSON\n",
    "    all_perturbed_texts = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(num_samples)]\n",
    "    \n",
    "    # Iterate for length num_pertubation\n",
    "    for perturbation_idx in tqdm(range(num_perturbation), desc=f\"Processing {num_perturbation} perturbations for {num_samples} texts. Perturbation number:\"):\n",
    "\n",
    "        # Randomly select 15% of text to mask, creates pertubations\n",
    "        perturbed_texts = t5_perturbation(original_texts,batch_size)\n",
    "\n",
    "        # Store in list\n",
    "        all_perturbed_texts.append(perturbed_texts)\n",
    "    \n",
    "    return all_perturbed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_log_prob(original_texts, all_perturbed_texts, num_perturbation=100, batch_size=128):\n",
    "    '''Compares log probs of original text vs list of JSONs perturbed texts'''\n",
    "    # Initialise list to store log prob of each perturbed JSON\n",
    "    log_probs_per_text_pert = []\n",
    "\n",
    "    # Get num_samples\n",
    "    num_samples = len(original_texts)\n",
    "\n",
    "    # Calculate log prob for original text in batch\n",
    "    log_probs_per_text_base = batch_average_log_prob(original_texts, batch_size)\n",
    "\n",
    "    # Iterate \n",
    "    for perturbation in tqdm(range(num_perturbation)):\n",
    "\n",
    "        # Get the JSON file\n",
    "        perturbed_texts = all_perturbed_texts[perturbation]\n",
    "\n",
    "        # Calculate log prob\n",
    "        log_probs_per_text_tran = batch_average_log_prob(perturbed_texts, batch_size)\n",
    "\n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if perturbation == 0:\n",
    "                log_probs_per_text_pert.append([])\n",
    "            log_probs_per_text_pert[j].append(log_probs_per_text_tran[j])\n",
    "        \n",
    "    return log_probs_per_text_base, log_probs_per_text_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base, log_probs_per_text_transformed):\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts\n",
    "    Calculated for num_perturbations perturbations\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the num_perturbations perturbed log probabilities\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texs\n",
    "    \"\"\"\n",
    "    num_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "        num_perturbations = len(perturbed_log_probs) # Number of perturbations\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = sum(perturbed_log_probs) / num_perturbations  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        d = original_log_prob - mu  \n",
    "        discrepancy_scores.append(d)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Add caching for tokenization\n",
    "@lru_cache(maxsize=1024)\n",
    "def cached_tokenize(text, is_t5=False):\n",
    "    \"\"\"Cache tokenization results to avoid repeated work.\"\"\"\n",
    "    if is_t5:\n",
    "        return t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        return computation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- **Data loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ“Œ Dataset format guidelines**\n",
    "\n",
    "All datasets (human-written and AI-generated) must follow this format:\n",
    "\n",
    "- Stored as a **`.jsonl`** where each line is a dictionary.\n",
    "- Each entry contains (minimum requirement):\n",
    "  - `\"text\"`: the text content\n",
    "  - `\"model\"`: for human text please label it as `\"human\"` and for AI-generated texts, please specify the model used (e.g. ``\"gpt2-large\"``)\n",
    "  - `\"source\"`: the origin of the text (e.g., `\"wikihow\"`, `\"reddit\"`, `\"news articles\"`)\n",
    "\n",
    "#### Exemple (as in ``subtaskB_train.jsonl`` located in `Datasets\\SemEval2024-Task8`):\n",
    "```json\n",
    "{\"text\": \"A groundbreaking discovery in physics was made today.\", \"model\": \"human\", \"source\": \"news articles\"}\n",
    "{\"text\": \"The AI revolution is shaping the future of work.\", \"model\": \"chatGPT\", \"source\": \"AI Generated\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Human texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First human text record: {'text': ' It will work with the Forza Motorsport disc in. Simply it will load but this problem will occur if your disc is dirty or damaged. Clean it with water on the back of the disc. There can be scratches because discs are fragile and can break easily. So always clean it with a soft, lint-free cloth.;\\n, If you have the game that includes the bonus feature, Xbox Live Arcade, You will get the dashboard menu that has \"Forza Motorsport\" and \"Xbox Live Arcade\". Choose Xbox Live Arcade if you want to play some arcade games, If you want to play Forza Motorsport, Simply press \"A\" on the Forza Motorsport Feature.\\n\\n\\nThe Main Menu features one of those choices after loading or creating a profile on the game.\\n\\nArcade Race: This is simply the quick race mode. You can race at a circuit, sprint, or a very interesting race. Just simply choose a class and choose a car and then choose a race and the race will load, Also the cars are featured in multiplayer, career, and free run mode.\\nCareer Mode: Actually this mode is the only mode to unlock cars and customize cars. It will be at the garage. Once you win a race, you may unlock a car and it will be added to your garage. The first car is a Lexus One. Unlock more cars and complete more races but if you did all of it, You are a winner.\\nMultiplayer, It\\'s the mode where a friend or a parent can join as a 2-player. It\\'s a 2-player mode, Go to head-to-head and you will get the split-screen that shows your car on the screen and your 2-player car.\\nTime Trial, Show your best to see how fast your car will complete in 1 minute or 5 minutes on a record.\\nFree Run, A mode where no operands are there but your the only person that can be featured in the Free Run mode, you can race by yourself and pretend that it\\'s a time trial race or go fast without any opponents trying to win.\\nOptions, Simply use this to setup your settings, sound, sfx, and more.\\n\\n\\n\\n, The first three tracks are Maple Valley II, Silverstone II, Tsukuba Circuit. You must complete all of them if you want to get more races like for an example, Road Atlanta II.\\n\\n\\nThe Career Mode has a short menu and a short things but it\\'s like the main menu. The choices are:\\n\\nGo Race, the Go Race mode is where you race to earn credits (money) and you unlock more tracks and stuff like cars. Simply if you race all of them, You\\'re winner.\\nThe Garage, Nothing special but the garage is where you keep cars, Customize a car or go into another car for the race. You must race in tracks if you want to get all of the cars.\\nBuy Cars, The mode is the where you can buy cars anytime you want but only if you have enough amount of credits, You can get cars from any country, the pictures are the flags of their country. And simply in the middle left corner of the screen, It will tell you the country (i.e. Sweden). And you get a car and it will be added into your garage.\\nTrain Drivatar, You know that you can easily train a drivatar (a driving avatar), which you can train by selecting a drivatar\\'s profile or creating one, and there is a menu. You can train it anytime you want to.\\nSet Difficulty, It will set the difficulty to the easiest setting to the hardest setting.\\n\\n\\nThe Multiplayer menu will feature the choices that will usually do with a Xbox account:\\n\\nXbox Live, Compete with other players around the world who play Forza Motorsport and you can also join a Car Club if you like.\\nSplit Screen, You can see yourself and your 2-player in this mode. It will split the screen into half. Simply, your facing into your screen and the other player faces into the 2nd player screen. Your racing into head-to-head.\\nSystem Link, Only for people who only signed in a Xbox Live Account. You can race with other players in the network or a hub.\\nScoreboards, Go to see who was fastest in the race you have.\\nXbox Live Sign In, Nothing special but to just login or sign up. Sign up means to create a account and join. Login means to sign in into your account.\\n\\n\\nFree Run features three races to choose from, here\\'s are the choices:\\n\\nHot Lap, It only features circuit races. Choose it to race around the track and there is unlimited laps.\\nAutocross, This is a very tricky race, Race with cones and you must stay on the street. The time will be penalized if you have a missing gate or knocking the cones down.\\nPoint-to-Point, Sprint tracks only, You can practice with any car on a sprint race where you must not go around a track but finished into a location-to-location.\\n\\n\\n\\n,, It will show your cars you got.\\n\\n,, To reverse, Press the Left Trigger Button. Press the B button to hand brake, Or use a control manual or so.\\n\\n, Once at the beginning of the race, Instead of a \"3, 2, 1, Go!\" or \"Ready, Set, Go!\" They have those blue circle things. There are three of them and make some kind of noise, They disappear at each second. When every thing is gone and when you hear the \"eerrr\". You start to race and you can go fast as you want as long as you don\\'t hit anything but you do this by an accident.\\n\\n, You can see those flat arrows on the road, They feature in almost every race except for Autocross race which makes it more trickier. The green arrows mean that you can turn it easily and the yellow arrows means it\\'s normal but be careful with the roads and the red arrows means a hard and sharp turn. You must slow down speed when you see the red arrows. That\\'s how the game works.\\n\\n, The right thumbstick will be used as a camera.\\n\\n,,, If you like this game and if you have a Xbox 360, you can get the sequel \"Forza Motorsport 2\", The sequel is only for Xbox 360 players. The new game, Forza Motorsport 3 is coming to release in 2009.\\n\\n', 'model': 'human', 'source': 'wikihow', 'label': 0, 'id': 2000}\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_human = []\n",
    "\n",
    "# Efficiently process the file line by line\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") == \"human\":\n",
    "            data_human.append(record)\n",
    "\n",
    "# Print first human record\n",
    "print(\"First human text record:\", data_human[0] if data_human else \"No human data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **AI-generated texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. *Option 1: produce own AI-generated texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate AI text from a given prompt\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): max length of generated text\n",
    "\n",
    "    Returns:\n",
    "        cleaned_text (str): cleaned generated text\n",
    "    \"\"\"\n",
    "    inputs = generation_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = generation_model.generate(**inputs, max_length=max_length, do_sample=True, temperature=0.7)\n",
    "    \n",
    "    generated_text = generation_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def generate_dataset(N: int, max_length: int, generation_model_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a dataset of N AI-generated texts in the required dictionary format.\n",
    "\n",
    "    Args:\n",
    "        N (int): Number of AI-generated texts\n",
    "        max_length (int): Maximum length of each generated text\n",
    "        generation_model_name (str): Name of the AI generation model\n",
    "\n",
    "    Returns:\n",
    "        data_ai (list): Dataset of AI-generated texts (list of dictionaries)\n",
    "    \"\"\"\n",
    "    prompt = \"In a faraway galaxy,\"\n",
    "    data_ai = [\n",
    "        {\n",
    "            \"text\": generate_text(prompt, max_length),\n",
    "            \"model\": generation_model_name,\n",
    "            \"source\": \"FleLLM\"\n",
    "        }\n",
    "        for _ in range(N)\n",
    "    ]\n",
    "    return data_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "max_length = 100\n",
    "data_ai_generated = generate_dataset(N, max_length, GENERATION_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset in the correct .jsonl format\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH)\n",
    "\n",
    "with open(FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data_ai_generated:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First AI-generated text record: {'text': 'In a faraway galaxy, the alien galaxy is far away, and on the surface of that distant galaxy, we see the stars. The galaxies are very dense, and this is a big difference.\\n\\nBut there are many other galaxies, and they are very dense, and we can see some of them. There are three other galaxies, and they are very dense, and they are very dark. We can look at these stars as if they are the equivalent of a telescope. We can look', 'model': 'gpt2', 'source': 'FleLLM'}\n"
     ]
    }
   ],
   "source": [
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_generated[0] if data_ai_generated else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. *Option 2: load AI-generated texts from a dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First AI-generated text record: {'text': 'Forza Motorsport is a popular racing game that provides players with the ability to race on various tracks and in different vehicles. Whether you\\'re a seasoned racer or a newbie, playing Forza Motorsport can be a fun experience. In this article, we will take you through the different steps on how to play Forza Motorsport.\\n\\nStep 1. Insert The Game Disc\\n\\nThe first step is to insert the game disc into your console or computer. Follow the instructions to set up the game.\\n\\nStep 2. Choose Your Game\\n\\nOnce the game is set up, choose the game you\\'d like to play. Forza Motorsport has different modes: Career, Free Play, and Arcade. In this article, we will focus on the Arcade mode.\\n\\nStep 3. Just Make A Quick Race By The Arcade Mode\\n\\nOnce the Arcade mode is selected, choose \"Quick Race\" to get started quickly.\\n\\nStep 4. Pick A Racetrack\\n\\nPick a racetrack from the different ones available like Road Atlanta, New York, Rio de Janeiro, Maple Valley, or anything to choose from.\\n\\nStep 5. Pick A Class And A Car\\n\\nForza Motorsport has different car classes, from Classes A, B, C, D, S to R. Pick a class that suits your gaming style and choose a car of your liking.\\n\\nStep 6. The Options Panel\\n\\nThe Options Panel opens automatically after choosing a car. Press \"OK\", and be patient while the race is loading. It will show what time, wind direction, wind heading, and miles.\\n\\nStep 7. Accelerate\\n\\nTo accelerate, press the Right trigger to make the car engine running.\\n\\nStep 8. Go!\\n\\nOnce acceleration is complete, let go of the brakes and hit the gas to go!\\n\\nStep 9. The Arrows\\n\\nTo steer, use the left thumbstick and to turn to the left, move the thumbstick left, and to turn right, move the thumbstick right.\\n\\nStep 10. Pause The Game\\n\\nTo pause the game, just press the \"start\" button. If you feel lonely and wanted an operant, choose the \"Ghost car\" setting and turn it on.\\n\\nStep 11. Replay the Race\\n\\nIf you want to see how well you did, use this feature to see a video of the race, and it will show you turns for some reason.\\n\\nStep 12. Have Fun And Enjoy The Game!\\n\\nIn conclusion, Forza Motorsport can be a fun game with its impressive graphics, user-friendly interface, and thrilling gameplay. Follow these steps to improve your gameplay and have fun.', 'model': 'chatGPT', 'source': 'wikihow', 'label': 1, 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "# FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_ai_dataset = []\n",
    "\n",
    "# Read entire file and parse as JSON list\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") != \"human\":\n",
    "            data_ai_dataset.append(record)\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_dataset[0] if data_ai_dataset else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- **Exemple usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human\n",
    "NUM_SAMPLES = 300\n",
    "MAX_LENGTH = 50\n",
    "NUM_PERTURBATIONS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "log_probs_base_human, log_probs_transformed_human = optimized_processing(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human, log_probs_transformed_human)\n",
    "\n",
    "results_human = {}\n",
    "results_human[\"log_probs_base\"] = log_probs_base_human\n",
    "results_human[\"log_probs_transformed\"] = log_probs_transformed_human\n",
    "results_human[\"discrepancy_scores\"] = discrepancy_scores_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_human, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_ai_generated # or DATA = data_ai_dataset\n",
    "NUM_SAMPLES = 300\n",
    "MAX_LENGTH = 50\n",
    "NUM_PERTURBATIONS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "log_probs_base_ai, log_probs_transformed_ai = optimized_processing(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai, log_probs_transformed_ai)\n",
    "\n",
    "results_ai = {}\n",
    "results_ai[\"log_probs_base\"] = log_probs_base_ai\n",
    "results_ai[\"log_probs_transformed\"] = log_probs_transformed_ai\n",
    "results_ai[\"discrepancy_scores\"] = discrepancy_scores_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_ai, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do whatever you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
