{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT for dummies**: Identifying AI-generated text\n",
    "This notebook implements the **DetectGPT** method from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- **Model setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a simple setup of different transformer based models that will be needed to:\n",
    "1. produce the AI-generated text - ``generation_model``\n",
    "2. compute the log-probablities - ``computation_model``\n",
    "3. perturb the text with the T5 perturbation - ``t5_model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "\n",
    "GENERATION_MODEL_NAME = \"openai-community/gpt2-large\"\n",
    "# Model list (all tested)\n",
    "# gpt2\n",
    "# gpt2-large\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(GENERATION_MODEL_NAME, device_map=\"auto\", torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "generation_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"openai-community/gpt2-large\"\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, device_map=\"auto\", torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "\n",
    "# Set model to evaluation mode (ensures stable log prob estimation + disables dropout)\n",
    "computation_model.eval()\n",
    "\n",
    "computation_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "PERTURBATION_MODEL_NAME = \"t5-large\"\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(PERTURBATION_MODEL_NAME, device_map=\"auto\", torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(PERTURBATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Set to evaluation mode\n",
    "t5_model.eval()\n",
    "\n",
    "t5_model.to(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- **Code setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **🔀 Text perturbation** *NEW VERSION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the **T5-based perturbation function**, which modifies the input text slightly while preserving its meaning. \n",
    "\n",
    "- **Why is perturbation needed?** AI-generated text often sits in **low-curvature** probability regions, meaning slight perturbations can significantly change their log probabilities\n",
    "- **How does it work?** The **T5 model** introduces variations to the text and helps in detecting AI-generated content\n",
    "\n",
    "These perturbed texts will later be compared to their original versions to compute the discrepancy scores d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text: str, span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Tokenizes a text and applies masking by replacing certain spans with placeholder tokens\n",
    "\n",
    "    Args:\n",
    "        text (str): input text to be tokenized and masked\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool - optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Return:\n",
    "        text (str): masked text with placeholder token\n",
    "    \"\"\"\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    # Calculate number of masked spans\n",
    "    n_spans = pct * len(tokens) / (span_length + buffer_size * 2)\n",
    "\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - buffer_size)\n",
    "        search_end = min(len(tokens), end + buffer_size)\n",
    "\n",
    "        # Ensure no overlapping masks in buffer region\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # Replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def count_masks(texts: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Counts the number of mask tokens in each text\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts containing mask tokens (format: \"<extra_id_N>\" where N is any int)\n",
    "\n",
    "    Returns:\n",
    "        n_masks (list): list where each element represents the number of mask tokens in the corresponding text\n",
    "    \"\"\"\n",
    "    n_masks = [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "    return n_masks\n",
    "\n",
    "def replace_masks(texts: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Replaces masked spans in texts with generated text using a T5 model\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts containing mask tokens (format: \"<extra_id_N>\" where N is any int)\n",
    "\n",
    "    Returns:\n",
    "        (list): list of texts where masked spans have been replaced with generated content\n",
    "    \"\"\"\n",
    "    n_expected = count_masks(texts) # Count number of masks per text\n",
    "    stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0] # Define stopping condition\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    tokens = t5_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "    # Generate replacements for the masks using T5\n",
    "    outputs = t5_model.generate(\n",
    "        **tokens, \n",
    "        max_length=150, \n",
    "        do_sample=True, \n",
    "        top_p=0.96, \n",
    "        num_return_sequences=1, \n",
    "        eos_token_id=stop_id\n",
    "    )\n",
    "\n",
    "    # Decode the generated output\n",
    "    texts_replaced = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "    return texts_replaced\n",
    "\n",
    "# Define a regex pattern to match all placeholder tokens in the format <extra_id_N> (where N is any int)\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "def extract_fills(texts: list[str]) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts the generated text fills from masked texts\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of texts where masked spans have been replaced with generated text\n",
    "\n",
    "    Returns:\n",
    "        extracted_fills (list): a list of lists, where each inner list contains the extracted fills for the corresponding input text\n",
    "    \"\"\"\n",
    "    # Remove \"<pad>\" and \"</s>\" tokens from the beginning/end of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # Extract the text between mask tokens (pattern should be defined elsewhere)\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # Trim whitespace from each extracted fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "def apply_extracted_fills(masked_texts: list[str], extracted_fills: list[list[str]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Replaces mask tokens in masked texts with corresponding extracted fills\n",
    "\n",
    "    Args:\n",
    "        masked_texts (list): list of texts containing mask tokens\n",
    "        extracted_fills (list): list of lists, where each inner list contains the extracted fill text for the corresponding masked text\n",
    "\n",
    "    Returns:\n",
    "        texts (list): list of texts with all masks replaced by their corresponding extracted fills\n",
    "    \"\"\"\n",
    "    # Split masked text into tokens, keeping spaces intact\n",
    "    tokens = [x.split(' ') for x in masked_texts]\n",
    "\n",
    "    # Count expected number of masks per text\n",
    "    n_expected = count_masks(masked_texts)\n",
    "\n",
    "    # Replace each mask token with the corresponding extracted fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = []  # Empty text if not enough fills are available\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                if f\"<extra_id_{fill_idx}>\" in text:\n",
    "                    text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # Join tokens back into text format\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts\n",
    "\n",
    "def perturb_texts_(texts: list[str], span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies T5-based perturbation to a list of texts by masking spans, generating replacements,and applying the generated fills\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of input texts to be perturbed\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool, optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        perturbed_texts (list): list of perturbed texts\n",
    "    \"\"\"\n",
    "    # Step 1: Mask spans in the input texts\n",
    "    masked_texts = [tokenize_and_mask(x, span_length, pct, buffer_size, ceil_pct) for x in texts]\n",
    "    print(f\"Masked texts: {masked_texts}\")\n",
    "\n",
    "    # Step 2: Generate replacement texts\n",
    "    raw_fills = replace_masks(masked_texts)\n",
    "    print(f\"Raw fills: {raw_fills}\")\n",
    "\n",
    "    # Step 3: Extract only the generated fills\n",
    "    extracted_fills = extract_fills(raw_fills)\n",
    "    print(f\"Extracted fills: {extracted_fills}\")\n",
    "\n",
    "    # Step 4: Apply the extracted fills to reconstruct the perturbed texts\n",
    "    perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "    print(f\"Perturbed texts: {perturbed_texts}\")\n",
    "    print(f\"Original texts: {texts}\")\n",
    "\n",
    "    # Handle cases where the model doesn't generate the correct number of fills\n",
    "    attempts = 1\n",
    "    while '' in perturbed_texts:\n",
    "        idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
    "        print(f'WARNING: {len(idxs)} texts have no fills. Retrying [attempt {attempts}].')\n",
    "\n",
    "        # Retry perturbation for failed cases\n",
    "        masked_texts = [tokenize_and_mask(texts[idx], span_length, pct, buffer_size, ceil_pct) for idx in idxs]\n",
    "        raw_fills = replace_masks(masked_texts)\n",
    "        extracted_fills = extract_fills(raw_fills)\n",
    "        new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "\n",
    "        # Update perturbed texts\n",
    "        for idx, new_text in zip(idxs, new_perturbed_texts):\n",
    "            perturbed_texts[idx] = new_text\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    return perturbed_texts\n",
    "\n",
    "def perturb_texts(texts: list[str], span_length: int, pct: float, buffer_size: int, ceil_pct: bool = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Applies T5-based perturbation to a list of texts (in chunks for efficiency)\n",
    "\n",
    "    Note: wrapper function around `perturb_texts_`\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of input texts to be perturbed\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "        ceil_pct (bool, optional): whether to round up the number of spans (Default: False)\n",
    "\n",
    "    Returns:\n",
    "        outputs (list): list of perturbed texts\n",
    "    \"\"\"\n",
    "    chunk_size = 20  # Process texts in batches of 20 for efficiency\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        batch = texts[i:i + chunk_size]\n",
    "        perturbed_batch = perturb_texts_(batch, span_length, pct, buffer_size, ceil_pct=ceil_pct)\n",
    "        outputs.extend(perturbed_batch)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **🔍 Main functions: *DetectGPT* Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the **DetectGPT method**.\n",
    "\n",
    "- **Key idea:** once again, AI-generated texts often **reside in low-curvature probability regions**.\n",
    "- **How does it work?**\n",
    "  - We perturb the text multiple times (``num_perturbation``). We will use ``n_samples`` texts with ``max_length`` words.\n",
    "  - Compute log probabilities for both **original** and **perturbed** texts\n",
    "  - Measure the **discrepancy score** (a higher score suggests AI-generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll(text: list) ->list:\n",
    "    \"\"\"\n",
    "    Compute log prob for a single text\n",
    "    \n",
    "    Args:\n",
    "        text (str): input text\n",
    "\n",
    "    Returns:\n",
    "        log_prob (float): log prob of the text\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the text\n",
    "        tokenized = computation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        labels = tokenized.input_ids\n",
    "\n",
    "        # Compute loss (NLL per token)\n",
    "        loss = computation_model(**tokenized, labels=labels).loss.item()\n",
    "\n",
    "        # Compute log-prob \n",
    "        log_prob = -loss\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "def get_lls(texts: list) -> list:\n",
    "    \"\"\"\n",
    "    Compute log prob for multiple texts\n",
    "    \n",
    "    Args:\n",
    "        texts (list): list of texts\n",
    "\n",
    "    Returns:\n",
    "        log_probs (list): log prob of each text\n",
    "    \"\"\"\n",
    "    log_probs = [get_ll(text) for text in texts]\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(text: list) ->list:\n",
    "    \"\"\"\n",
    "    Compute perplexity for a single text\n",
    "    \n",
    "    Args:\n",
    "        text (str): input text\n",
    "\n",
    "    Returns:\n",
    "        perplexity (float): perplexity of the text\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the text\n",
    "        tokenized = computation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        labels = tokenized.input_ids\n",
    "\n",
    "        # Compute loss (NLL per token)\n",
    "        loss = computation_model(**tokenized, labels=labels).loss.item()\n",
    "\n",
    "        # Compute perplexity\n",
    "        perplexity_score = torch.exp(torch.tensor(loss)).item()\n",
    "    \n",
    "    return perplexity_score\n",
    "\n",
    "\n",
    "def get_ppls(texts: list) -> list:\n",
    "    \"\"\"\n",
    "    Compute perplexity for multiple texts\n",
    "    \n",
    "    Args:\n",
    "        texts (list): list of texts\n",
    "\n",
    "    Returns:\n",
    "        perplexity_scores (list): perplexity score of each text\n",
    "    \"\"\"\n",
    "    perplexity_scores = [get_ppl(text) for text in texts]\n",
    "    return perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list, log_probs_per_text_transformed: list, normalization: bool=False) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts. Computed for n_perturbations perturbations.\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the n_perturbations perturbed log probs\n",
    "        normalization (bool): True if you want to normalize the discrepancy scores, False otherwise\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texts\n",
    "    \"\"\"\n",
    "    n_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "        n_perturbations = len(perturbed_log_probs) # Number of perturbations\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = sum(perturbed_log_probs) / n_perturbations  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (n_perturbations - 1)\n",
    "            sigma = variance ** 0.5\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else 0\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_processing(data: list, \n",
    "                         n_samples: int, \n",
    "                         max_length: int, \n",
    "                         n_perturbations: int, \n",
    "                         span_length: int, \n",
    "                         pct: float, \n",
    "                         buffer_size: int)-> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Compute log probabilities for original and perturbed texts.\n",
    "    \n",
    "    This function processes multiple text samples, computes their log probabilities, \n",
    "    applies perturbations to the texts, and then computes the log probabilities \n",
    "    of the perturbed versions.\n",
    "\n",
    "    Args:\n",
    "        data (list): list of dictionaries containing text (e.g. [{\"text\": \"sample text\"}, ...])\n",
    "        n_samples (int): number of texts to process\n",
    "        max_length (int): maximum number of words to consider in each text\n",
    "        n_perturbations (int): number of perturbations applied to each text\n",
    "        span_length (int): length of each masked span\n",
    "        pct (float): percentage of the text to be masked (as in DetectGPT codebase, so not exactly pure percentage of the text)\n",
    "        buffer_size (int): buffer size around masked spans (to prevent overlap)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: \n",
    "            - log_probs_per_text_base (list): log probs of the original texts\n",
    "            - log_probs_per_text_transformed (list of lists): log probs of the perturbed texts,\n",
    "              structured as a list where each element corresponds to a text and contains \n",
    "              a list of its perturbed log probs\n",
    "    \"\"\"\n",
    "    log_probs_per_text_transformed = []\n",
    "\n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "    # Calculate log probabilities of the original texts\n",
    "    log_probs_per_text_base = get_lls(original_texts)\n",
    "\n",
    "    for perturbation_idx in tqdm(range(n_perturbations), desc=f\"For all the {n_samples} texts, processing perturbation\"):\n",
    "        # Apply perturbation\n",
    "        all_perturbed_texts = perturb_texts(original_texts, span_length=span_length, pct=pct, buffer_size=buffer_size, ceil_pct=False)\n",
    "        \n",
    "        # Calculate log probabilities of the perturbed texts\n",
    "        all_log_probs = get_lls(all_perturbed_texts)\n",
    "\n",
    "        # Organize results\n",
    "        for j in range(n_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "\n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- **Data loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📌 Dataset format guidelines**\n",
    "\n",
    "All datasets (human-written and AI-generated) must follow this format:\n",
    "\n",
    "- Stored as a **`.jsonl`** where each line is a dictionary.\n",
    "- Each entry contains (minimum requirement):\n",
    "  - `\"text\"`: the text content\n",
    "  - `\"model\"`: for human text please label it as `\"human\"` and for AI-generated texts, please specify the model used (e.g. ``\"gpt2-large\"``)\n",
    "  - `\"source\"`: the origin of the text (e.g., `\"wikihow\"`, `\"reddit\"`, `\"news articles\"`)\n",
    "\n",
    "#### Exemple (as in ``subtaskB_train.jsonl`` located in `Datasets\\SemEval2024-Task8`):\n",
    "```json\n",
    "{\"text\": \"A groundbreaking discovery in physics was made today.\", \"model\": \"human\", \"source\": \"news articles\"}\n",
    "{\"text\": \"The AI revolution is shaping the future of work.\", \"model\": \"chatGPT\", \"source\": \"AI Generated\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Human texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_human = []\n",
    "\n",
    "# Efficiently process the file line by line\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") == \"human\":\n",
    "            data_human.append(record)\n",
    "\n",
    "# Print first human record\n",
    "print(\"First human text record:\", data_human[0] if data_human else \"No human data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **AI-generated texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. *Option 1: produce own AI-generated texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate AI text from a given prompt\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): max length of generated text\n",
    "\n",
    "    Returns:\n",
    "        generated_text (str): generated text\n",
    "    \"\"\"\n",
    "    inputs = generation_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = generation_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    generated_text = generation_tokenizer.decode(output[0].cpu(), skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def generate_dataset(n_samples: int, max_length: int, generation_model_name: str, prompt: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a dataset of N AI-generated texts in the required dictionary format\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): number of AI-generated texts\n",
    "        max_length (int): max length of each generated text\n",
    "        generation_model_name (str): name of the AI generation model\n",
    "        prompt (str): prompt to start text generation\n",
    "    \n",
    "    Returns:\n",
    "        data_ai (list): dataset of AI-generated texts (list of dictionaries)\n",
    "    \"\"\"\n",
    "    data_ai = []\n",
    "\n",
    "    for _ in tqdm(range(n_samples), desc=\"Generating dataset\", unit=\"sample\"):\n",
    "        generated_text = generate_text(prompt, max_length)\n",
    "        data_ai.append({\n",
    "            \"text\": generated_text,\n",
    "            \"model\": generation_model_name,\n",
    "            \"source\": \"FleLLM\"\n",
    "        })\n",
    "\n",
    "    return data_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model.to(DEVICE)\n",
    "print(DEVICE)\n",
    "\n",
    "n_samples = 1\n",
    "max_length = 8\n",
    "PROMPT = \"In a faraway galaxy,\"\n",
    "\n",
    "clear_cuda_cache()\n",
    "data_ai_generated = generate_dataset(n_samples=n_samples, max_length=max_length, generation_model_name=GENERATION_MODEL_NAME, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset in the correct .jsonl format\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH)\n",
    "\n",
    "with open(FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data_ai_generated:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_generated[0] if data_ai_generated else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. *Option 2: load AI-generated texts from a dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "# FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_ai_dataset = []\n",
    "\n",
    "# Read entire file and parse as JSON list\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") != \"human\":\n",
    "            data_ai_dataset.append(record)\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_dataset[0] if data_ai_dataset else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Texts perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human\n",
    "n_samples = 2\n",
    "max_length = 10\n",
    "\n",
    "original_texts = [\" \".join(DATA[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "\n",
    "clear_cuda_cache()\n",
    "perplexity_scores = get_ppls(texts=original_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- **Exemple usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATA = data_human\n",
    "\n",
    "# Experiment setup\n",
    "n_samples = 2\n",
    "max_length = 20\n",
    "\n",
    "# Perturbation setup\n",
    "n_perturbations = 2\n",
    "span_length = 2\n",
    "pct = 0.3\n",
    "buffer_size = 1\n",
    "\n",
    "clear_cuda_cache()\n",
    "\n",
    "# Compute log probs before and after perturbation\n",
    "log_probs_base_human, log_probs_transformed_human = optimized_processing(\n",
    "    data=DATA,\n",
    "    n_samples=n_samples,\n",
    "    max_length=max_length,\n",
    "    n_perturbations=n_perturbations,\n",
    "    span_length=span_length,\n",
    "    pct=pct,\n",
    "    buffer_size=buffer_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = False # True if you want to normalize the discrepancy scores\n",
    "\n",
    "# Compute discrepancy scores\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=normalization)\n",
    "\n",
    "# Store all results\n",
    "results_human = {}\n",
    "results_human[\"log_probs_base\"] = log_probs_base_human\n",
    "results_human[\"log_probs_transformed\"] = log_probs_transformed_human\n",
    "results_human[\"discrepancy_scores\"] = discrepancy_scores_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_human, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATA = data_ai_dataset # DATA = data_ai_generated\n",
    "\n",
    "# Experiment setup\n",
    "n_samples = 2\n",
    "max_length = 20\n",
    "n_perturbations = 2\n",
    "\n",
    "# Perturbation setup\n",
    "span_length = 2\n",
    "pct = 0.3\n",
    "buffer_size = 1\n",
    "\n",
    "# Compute log probs before and after perturbation\n",
    "clear_cuda_cache()\n",
    "log_probs_base_ai, log_probs_transformed_ai = optimized_processing(\n",
    "    data=DATA,\n",
    "    n_samples=n_samples,\n",
    "    max_length=max_length,\n",
    "    n_perturbations=n_perturbations,\n",
    "    span_length=span_length,\n",
    "    pct=pct,\n",
    "    buffer_size=buffer_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = False # True if you want to normalize the discrepancy scores\n",
    "\n",
    "# Compute discrepancy scores\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=normalization)\n",
    "\n",
    "# Store all results\n",
    "results_ai = {}\n",
    "results_ai[\"log_probs_base\"] = log_probs_base_ai\n",
    "results_ai[\"log_probs_transformed\"] = log_probs_transformed_ai\n",
    "results_ai[\"discrepancy_scores\"] = discrepancy_scores_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_ai, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Histograms of the discrepancy scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION = False # True if you want to normalize the discrepancy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI texts results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\\\results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    data_ai = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base_ai = data_ai[\"log_probs_base\"]\n",
    "log_probs_transformed_ai = data_ai[\"log_probs_transformed\"]\n",
    "\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human texts results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\\\results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    data_human = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base_human = data_human[\"log_probs_base\"]\n",
    "log_probs_transformed_human = data_human[\"log_probs_transformed\"]\n",
    "\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def optimal_threshold(list1, list2):\n",
    "    X = np.concatenate([list1, list2]).reshape(-1, 1)\n",
    "    y = np.concatenate([np.zeros(len(list1)),np.ones(len(list2))])\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    best_threshold = -clf.intercept_[0] / clf.coef_[0][0]\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = (clf.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    return best_threshold, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold,auroc = optimal_threshold(discrepancy_scores_human, discrepancy_scores_ai)\n",
    "print(f\"Optimal threshold: {threshold:.2f}\")\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "\n",
    "plt.hist(discrepancy_scores_human, bins=15, alpha=0.5, label='Human', edgecolor='black', density=True)\n",
    "plt.hist(discrepancy_scores_ai, bins=15, alpha=0.5, label='AI', edgecolor='black', density=True)\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "plt.xlabel(f'Discrepancy scores (normalization={normalization})')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Computation model: {COMPUTATION_MODEL_NAME}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
