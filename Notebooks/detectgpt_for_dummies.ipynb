{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT for dummies**: Identifying AI-generated text\n",
    "This notebook implements the **DetectGPT** method from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "from datatime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- **Model setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a simple setup of different transformer based models that will be needed to:\n",
    "1. produce the AI-generated text - ``generation_model``\n",
    "2. compute the log-probablities - ``computation_model``\n",
    "3. perturb the text with the T5 perturbation - ``t5_model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulh\\miniconda3\\envs\\detectgpt_env\\lib\\site-packages\\huggingface_hub-0.28.1-py3.8.egg\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "\n",
    "GENERATION_MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # CHANGE GENERATION MODEL\n",
    "# Model list (all tested)\n",
    "# openai-community/gpt2\n",
    "# openai-community/gpt2-large\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(GENERATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "generation_model.eval()\n",
    "\n",
    "generation_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # CHANGE COMPUTATION MODEL\n",
    "# openai-community/gpt2-large\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "\n",
    "# Set model to evaluation mode (ensures stable log prob estimation + disables dropout)\n",
    "computation_model.eval()\n",
    "\n",
    "computation_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulh\\miniconda3\\envs\\detectgpt_env\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:220: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "PERTURBATION_MODEL_NAME = \"t5-large\"    # CHANGE PERTURBATION MODEL\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(PERTURBATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(PERTURBATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Set to evaluation mode\n",
    "t5_model.eval()\n",
    "\n",
    "t5_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- **Code setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **ðŸ”€ Text perturbation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the **T5-based perturbation function**, which modifies the input text slightly while preserving its meaning. \n",
    "\n",
    "- **Why is perturbation needed?** AI-generated text often sits in **low-curvature** probability regions, meaning slight perturbations can significantly change their log probabilities\n",
    "- **How does it work?** The **T5 model** introduces variations to the text and helps in detecting AI-generated content\n",
    "\n",
    "These perturbed texts will later be compared to their original versions to compute the discrepancy scores d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_text(texts, mask_ratio=0.15, max_words=512): # CHANGE THIS TO CHANGE THE MAXIMUM NUMBER OF WORDS TO MASK (DETERMINED BY max_words * mask_ratio) (originally 370)\n",
    "    \"\"\"Mask multiple texts at once.\"\"\"\n",
    "    masked_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()    # Splits the text at whitespaces\n",
    "        \n",
    "        # Truncate text\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        \n",
    "        num_masks = int(len(words) * mask_ratio)\n",
    "        \n",
    "        # Randomly select spans to mask (sorted in reverse to avoid index shifts)\n",
    "        mask_indices = sorted(random.sample(range(len(words) - 1), num_masks), reverse=True)\n",
    "        \n",
    "        for i, idx in enumerate(mask_indices):\n",
    "            words[idx] = f\"<extra_id_{i}>\"\n",
    "            if idx + 1 < len(words):  # Ensure a 2-word span - CHANGE THIS TO CHANGE THE WORD SPAN\n",
    "                del words[idx + 1]  # Remove instead of replacing with \"\"\n",
    "        \n",
    "        masked_texts.append(\" \".join(words))\n",
    "    \n",
    "    return masked_texts\n",
    "\n",
    "def batch_replace_masks(texts, batch_size=128):\n",
    "    \"\"\"Generate T5 model outputs for masked texts in batches.\"\"\"\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        n_expected = [text.count(\"<extra_id_\") for text in batch_texts]\n",
    "        stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]   # Stop token is the largest <extra_id_X> token in the batch of texts\n",
    "        \n",
    "        tokens = t5_tokenizer(batch_texts, return_tensors=\"pt\", padding=True)   # Tokenize batch of texts\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Move input tensors to model's device\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=tokens[\"input_ids\"].to(t5_model.device),\n",
    "                attention_mask=tokens[\"attention_mask\"].to(t5_model.device),    # Ensure padded tokens are ignored\n",
    "                max_length=150, # CHANGE THIS TO CHANGE THE MAXIMUM LENGTH OF THE OUTPUT OF EACH INSTANCE\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=stop_id\n",
    "            )\n",
    "            \n",
    "        # Move outputs back to CPU to save GPU memory\n",
    "        outputs = outputs.detach().cpu()\n",
    "        batch_decoded = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)   # Decode model outputs\n",
    "        all_outputs.extend(batch_decoded)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def batch_extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output for multiple texts.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()    # Clean up text\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "        \n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "    \n",
    "    return extracted_fills\n",
    "\n",
    "def batch_apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "        \n",
    "        filled_text = masked_text\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            filled_text = filled_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "        \n",
    "        filled_texts.append(filled_text)\n",
    "    \n",
    "    return filled_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_perturbation(text: str, batch_size: int) -> str:\n",
    "    \"\"\"\n",
    "    T5 perturbation - batch version\n",
    "\n",
    "    Args:\n",
    "        text (str): the input texts to be perturbed\n",
    "        batch_size (int): batch_size for compute\n",
    "\n",
    "    Returns:\n",
    "        all_perturbed_texts (str): the perturbed texts\n",
    "    \"\"\"\n",
    "    # Step 1: mask all texts at once\n",
    "    all_masked_texts = batch_mask_text(text)\n",
    "\n",
    "    # Step 2: generate replacements in batches\n",
    "    all_raw_fills = batch_replace_masks(all_masked_texts, batch_size)\n",
    "\n",
    "    # Step 3: extract fills\n",
    "    all_extracted_fills = batch_extract_fills(all_raw_fills)\n",
    "\n",
    "    # Step 4: apply fills\n",
    "    all_perturbed_texts = batch_apply_extracted_fills(all_masked_texts, all_extracted_fills)\n",
    "    \n",
    "    return all_perturbed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **ðŸ” Main functions: *DetectGPT* Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the **DetectGPT method**.\n",
    "\n",
    "- **Key idea:** once again, AI-generated texts often **reside in low-curvature probability regions**.\n",
    "- **How does it work?**\n",
    "  - We perturb the text multiple times (``num_perturbation``). We will use ``n_samples`` texts with ``max_length`` words.\n",
    "  - Compute log probabilities for both **original** and **perturbed** texts\n",
    "  - Measure the **discrepancy score** (a higher score suggests AI-generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_average_log_prob(texts, batch_size=128):\n",
    "    \"\"\"Calculate average log probability for multiple texts in batches.\"\"\"\n",
    "    \n",
    "    all_log_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = computation_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = computation_model(input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract logits\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Shift logits and labels to align\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()  # Ensure mask aligns\n",
    "\n",
    "        # Compute per-token loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=computation_tokenizer.pad_token_id)\n",
    "        loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Reshape to [batch_size, seq_length - 1]\n",
    "        loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "        # Compute per-sample log prob\n",
    "        sample_losses = []\n",
    "        for j in range(loss_per_token.size(0)):\n",
    "            mask = shift_mask[j].bool()  # Use shift_mask for actual tokens\n",
    "            if mask.sum() > 0:\n",
    "                sample_loss = loss_per_token[j][mask].mean().item()\n",
    "                sample_losses.append(-sample_loss)  # Negative loss as log prob\n",
    "            else:\n",
    "                sample_losses.append(float('-inf'))  # Avoid zero prob bias\n",
    "\n",
    "        all_log_probs.extend(sample_losses)\n",
    "\n",
    "    return all_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimized processing loop\n",
    "def optimized_processing(data, num_samples=200, max_length=512, num_perturbation=100, batch_size=128):\n",
    "    log_probs_per_text_transformed = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(num_samples)]\n",
    "    log_probs_per_text_base = batch_average_log_prob(original_texts, batch_size)\n",
    "    \n",
    "    # Inside the loop in optimized_processing()\n",
    "    for perturbation_idx in tqdm(range(num_perturbation), desc=f\"Processing {num_perturbation} perturbations for {num_samples} texts. Perturbation number:\"):\n",
    "        all_perturbed_texts = t5_perturbation(original_texts,batch_size)\n",
    "        all_log_probs = batch_average_log_prob(all_perturbed_texts, batch_size)\n",
    "        \n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "    \n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_perturbed(data, num_samples=200, max_length=512, num_perturbation=100, batch_size=128):\n",
    "    '''Generates pertubations for text. Returns list of length num_pertubations, each entry being a JSON object with perturbed text'''\n",
    "    # Initialise list to store all perturbed JSON\n",
    "    all_perturbed_texts = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(num_samples)]   # Truncate text to max_length, returns list of strings as before\n",
    "    \n",
    "    # Iterate for length num_pertubation\n",
    "    for perturbation_idx in tqdm(range(num_perturbation), desc=f\"Processing {num_perturbation} perturbations for {num_samples} texts. Perturbation number:\"):\n",
    "\n",
    "        # Randomly select 15% of text to mask, creates pertubations\n",
    "        perturbed_texts = t5_perturbation(original_texts,batch_size)\n",
    "\n",
    "        # Store in list\n",
    "        all_perturbed_texts.append(perturbed_texts)\n",
    "    \n",
    "    return all_perturbed_texts\n",
    "    # The output is a list of length num_pertubations, each entry being a list of length num_samples, each entry being a string\n",
    "    # (so the outer list is over the perturbations, the inner list is over the samples, and the string is the perturbed text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_log_prob(original_texts, all_perturbed_texts, num_perturbation=100, batch_size=128):\n",
    "    '''Compares log probs of original text vs list of JSONs perturbed texts'''\n",
    "    # Initialise list to store log prob of each perturbed JSON\n",
    "    \n",
    "    log_probs_per_text_pert = []\n",
    "\n",
    "    # Get num_samples\n",
    "    num_samples = len(original_texts)\n",
    "\n",
    "    # Calculate log prob for original text in batch\n",
    "    log_probs_per_text_base = batch_average_log_prob(original_texts, batch_size)\n",
    "\n",
    "    # Iterate \n",
    "    for perturbation in tqdm(range(num_perturbation)):\n",
    "\n",
    "        # Get the JSON file\n",
    "        perturbed_texts = all_perturbed_texts[perturbation]\n",
    "\n",
    "        # Calculate log prob\n",
    "        log_probs_per_text_tran = batch_average_log_prob(perturbed_texts, batch_size)\n",
    "\n",
    "        # Organize results by original text\n",
    "        for j in range(num_samples):\n",
    "            if perturbation == 0:\n",
    "                log_probs_per_text_pert.append([])\n",
    "            log_probs_per_text_pert[j].append(log_probs_per_text_tran[j])\n",
    "        \n",
    "    return log_probs_per_text_base, log_probs_per_text_pert\n",
    "    # log_probs_per_text_base is a list of length num_samples, each entry being the log prob of the corresponding original text\n",
    "    # log_probs_per_text_pert is a list of length num_samples, each entry being a list of length num_pertubations, each entry being the log prob of the corresponding perturbed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(data: list, n_samples: int, max_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Compute perplexity of each individual text in data\n",
    "\n",
    "    Args:\n",
    "        data (list): list of dictionaries containing text\n",
    "        n_samples (int): number of texts to process\n",
    "        max_length (int): max length for tokenization\n",
    "\n",
    "    Returns:\n",
    "        perplexities (list): perplexity scores of each text\n",
    "    \"\"\"\n",
    "    \n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "    perplexities = []\n",
    "\n",
    "    # Tokenize all inputs at once (better efficiency)\n",
    "    inputs = computation_tokenizer(original_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move tensors to GPU\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = computation_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Shift logits and labels to align\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "    shift_mask = attention_mask[:, 1:].contiguous() # Ensure mask aligns\n",
    "\n",
    "    # per-token loss for each sequence\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=computation_tokenizer.pad_token_id)\n",
    "    loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    # Reshape\n",
    "    loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "    # Compute per-sentence loss by averaging over valid tokens\n",
    "    valid_token_counts = shift_mask.sum(dim=1) # Number of valid tokens per sample\n",
    "    sentence_losses = (loss_per_token * shift_mask).sum(dim=1) / valid_token_counts.clamp(min=1) # Avoid division by zero\n",
    "\n",
    "    # Compute perplexity per sample\n",
    "    perplexities = torch.exp(sentence_losses).cpu().tolist()\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list, log_probs_per_text_transformed: list, normalization: bool=True) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts\n",
    "    Calculated for num_perturbations perturbations\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the num_perturbations perturbed log probabilities\n",
    "        normalization (bool)\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texs\n",
    "    \"\"\"\n",
    "    num_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "        num_perturbations = len(perturbed_log_probs) # Number of perturbations\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = sum(perturbed_log_probs) / num_perturbations  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (num_perturbations - 1)\n",
    "            sigma = variance ** 0.5\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else 0\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Add caching for tokenization\n",
    "@lru_cache(maxsize=1024)\n",
    "def cached_tokenize(text, is_t5=False):\n",
    "    \"\"\"Cache tokenization results to avoid repeated work.\"\"\"\n",
    "    if is_t5:\n",
    "        return t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        return computation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- **Data loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ“Œ Dataset format guidelines**\n",
    "\n",
    "All datasets (human-written and AI-generated) must follow this format:\n",
    "\n",
    "- Stored as a **`.jsonl`** where each line is a dictionary.\n",
    "- Each entry contains (minimum requirement):\n",
    "  - `\"text\"`: the text content\n",
    "  - `\"model\"`: for human text please label it as `\"human\"` and for AI-generated texts, please specify the model used (e.g. ``\"gpt2-large\"``)\n",
    "  - `\"source\"`: the origin of the text (e.g., `\"wikihow\"`, `\"reddit\"`, `\"news articles\"`)\n",
    "\n",
    "#### Exemple (as in ``subtaskB_train.jsonl`` located in `Datasets\\SemEval2024-Task8`):\n",
    "```json\n",
    "{\"text\": \"A groundbreaking discovery in physics was made today.\", \"model\": \"human\", \"source\": \"news articles\"}\n",
    "{\"text\": \"The AI revolution is shaping the future of work.\", \"model\": \"chatGPT\", \"source\": \"AI Generated\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Human texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\\nWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\\nThe Welsh Government said more people than ever were getting help to address housing problems.\\nChanges to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.\\nPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.\\nHowever, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.\\nAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".\\n\"There\\'s a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.\\n\"It could take six months to a year, without a lot of help they could be on the streets for six months.\\n\"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\"\\nMr Stevens believes building more one-bedroom flats could help ease the problem.\\n\"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said.\\nOfficial figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales.\\nMarc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released.\\nHe said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\"\\n\"You\\'re put out among the same sort of people doing the same sort of thing, and it\\'s difficult, it\\'s difficult to get away from it. It\\'s like every man for himself, there\\'s nothing.\"\\nMarc has now found stable accommodation with homeless charity Emmaus and said it had been life changing.\\n\"You feel safe, you got hot food, you\\'ve got company of people in similar situations to yourself but all dealing with different issues. It\\'s a constructive, helpful atmosphere,\" he said.\\nTom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available.\\n\"We do still see [people] homeless on the streets, so clearly they haven\\'t got accommodation and haven\\'t got provision,\" he said.\\n\"I think the key is connecting people with the services they need. I don\\'t delude myself that Emmaus can offer a one size fits all for everyone, we can\\'t.\\n\"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\"\\nA Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence.\\nIt added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.',\n",
       "  'id': '38264402',\n",
       "  'model': 'human'},\n",
       " {'text': 'Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\\nDetectives said three firearms, ammunition and a five-figure sum of money were recovered.\\nA 26-year-old man who was arrested and charged appeared at Edinburgh Sheriff Court on Thursday.',\n",
       "  'id': '34227252',\n",
       "  'model': 'human'},\n",
       " {'text': 'Jordan Hill, Brittany Covington and Tesfaye Cooper, all 18, and Tanishia Covington, 24, appeared in a Chicago court on Friday.\\nThe four have been charged with hate crimes and aggravated kidnapping and battery, among other things.\\nAn online fundraiser for their victim has collected $51,000 (Ã‚Â£42,500) so far.\\nDenying the four suspects bail, Judge Maria Kuriakos Ciesil asked: \"Where was your sense of decency?\"\\nProsecutors told the court the beating started in a van and continued at a house, where the suspects allegedly forced the 18-year-old white victim, who suffers from schizophrenia and attention deficit disorder, to drink toilet water and kiss the floor.\\nPolice allege the van was earlier stolen by Mr Hill, who is also accused of demanding $300 from the victim\\'s mother while they held him captive, according to the Chicago Tribune.\\nThe court was also told the suspects stuffed a sock into his mouth, taped his mouth shut and bound his hands with a belt.\\nIn a video made for Facebook Live which was watched millions of times, the assailants can be heard making derogatory statements against white people and Donald Trump.\\nThe victim had been dropped off at a McDonalds to meet Mr Hill - who was one of his friends - on 31 December.\\nHe was found by a police officer on Tuesday, 3 January, a day after he was reported missing by his parents.\\nProsecutors say the suspects each face two hate crimes counts, one because of the victim\\'s race and the other because of his disabilities.',\n",
       "  'id': '38537698',\n",
       "  'model': 'human'},\n",
       " {'text': 'The 48-year-old former Arsenal goalkeeper played for the Royals for four years.\\nHe was appointed youth academy director in 2000 and has been director of football since 2003.\\nA West Brom statement said: \"He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012.\"',\n",
       "  'id': '36175342',\n",
       "  'model': 'human'},\n",
       " {'text': 'Restoring the function of the organ - which helps control blood sugar levels - reversed symptoms of diabetes in animal experiments.\\nThe study, published in the journal Cell, says the diet reboots the body.\\nExperts said the findings were \"potentially very exciting\" as they could become a new treatment for the disease.\\nThe experiments were on mice put on a modified form of the \"fasting-mimicking diet\".\\nWhen people go on it they spend five days on a low calorie, low protein, low carbohydrate but high unsaturated-fat diet.\\nIt resembles a vegan diet with nuts and soups, but with around 800 to 1,100 calories a day.\\nThen they have 25 days eating what they want - so overall it mimics periods of feast and famine.\\nPrevious research has suggested it can slow the pace of ageing.\\nBut animal experiments showed the diet regenerated a special type of cell in the pancreas called a beta cell.\\nThese are the cells that detect sugar in the blood and release the hormone insulin if it gets too high.\\nDr Valter Longo, from the University of Southern California, said: \"Our conclusion is that by pushing the mice into an extreme state and then bringing them back - by starving them and then feeding them again - the cells in the pancreas are triggered to use some kind of developmental reprogramming that rebuilds the part of the organ that\\'s no longer functioning.\"\\nThere were benefits in both type 1 and type 2 diabetes in the mouse experiments.\\nType 1 is caused by the immune system destroying beta cells and type 2 is largely caused by lifestyle and the body no longer responding to insulin.\\nFurther tests on tissue samples from people with type 1 diabetes produced similar effects.\\nDr Longo said: \"Medically, these findings have the potential to be very important because we\\'ve shown - at least in mouse models - that you can use diet to reverse the symptoms of diabetes.\\n\"Scientifically, the findings are perhaps even more important because we\\'ve shown that you can use diet to reprogram cells without having to make any genetic alterations.\"\\nBBC reporter Peter Bowes took part in a separate trial with Dr Valter Longo.\\nHe said: \"During each five-day fasting cycle, when I ate about a quarter of the average person\\'s diet, I lost between 2kg and 4kg (4.4-8.8lbs).\\n\"But before the next cycle came round, 25 days of eating normally had returned me almost to my original weight.\\n\"But not all consequences of the diet faded so quickly.\"\\nHis blood pressure was lower as was a hormone called IGF-1, which is linked to some cancers.\\nHe said: \"The very small meals I was given during the five-day fast were far from gourmet cooking, but I was glad to have something to eat\"\\nPeter Bowes: Fasting for science\\nPeter Bowes: Intermittent fasting and the good things it did to my body\\nSeparate trials of the diet in people have been shown to improve blood sugar levels. The latest findings help to explain why.\\nHowever, Dr Longo said people should not rush off and crash diet.\\nHe told the BBC: \"It boils down to do not try this at home, this is so much more sophisticated than people realise.\"\\nHe said people could \"get into trouble\" with their health if it was done without medical guidance.\\nDr Emily Burns, research communications manager at Diabetes UK, said: \"This is potentially very exciting news, but we need to see if the results hold true in humans before we\\'ll know more about what it means for people with diabetes.\\n\"People with type-1 and type-2 diabetes would benefit immensely from treatments that can repair or regenerate insulin-producing cells in the pancreas.\"\\nFollow James on Twitter.',\n",
       "  'id': '39070183',\n",
       "  'model': 'human'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Data/EdinburghNLP___xsum/default/1.2.0/40db7604fedb616a9d2b0673d11838fa5be8451c/xsum-test.arrow\"  # CHANGE THIS TO THE PATH OF THE HUMAN-GENERATED TEXT FILE\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "\n",
    "# For XSUM data - convert into a list of dictionaries\n",
    "# Load dataset using Hugging Face's datasets library\n",
    "dataset = Dataset.from_file(FILE_PATH)\n",
    "# Keep only the first 200 rows\n",
    "dataset = dataset.select(range(200))    # CHANGE THIS TO CONTROL THE NUMBER OF DATAPOINTS TO USE\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = dataset.to_pandas()\n",
    "# Adapt the dataframe to the desired style\n",
    "df = df.rename(columns={\"document\": \"text\"}).drop(columns=[\"summary\"])  # Rename and drop columns\n",
    "df[\"model\"] = \"human\"   # Add a new column to track that all these documents are human-generated\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "data_human = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Display first few rows of the list\n",
    "display(data_human[:5])  # Display first 5 rows as sample output\n",
    "\n",
    "\n",
    "## For JSON format\n",
    "# data_human = []\n",
    "## Efficiently process the file line by line\n",
    "#with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#    for line in file:\n",
    "#        record = json.loads(line)  # Parse JSON once\n",
    "#        if record.get(\"model\") == \"human\":\n",
    "#            data_human.append(record)\n",
    "#\n",
    "## Print first human record\n",
    "#print(\"First human text record:\", data_human[0] if data_human else \"No human data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **AI-generated texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. *Option 1: produce own AI-generated texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate AI text from a given prompt\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): max length of generated text\n",
    "\n",
    "    Returns:\n",
    "        cleaned_text (str): cleaned generated text\n",
    "    \"\"\"\n",
    "    inputs = generation_tokenizer(prompt, return_tensors=\"pt\").to(generation_model.device)\n",
    "    with torch.no_grad():\n",
    "        output = generation_model.generate(**inputs, max_length=max_length, do_sample=True, temperature=1.0)    # CHANGE THE GENERATION MODEL TEMPERATURE HERE (originally 0.7)\n",
    "    \n",
    "    generated_text = generation_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "    # The output is a string of the generated text\n",
    "\n",
    "\n",
    "def generate_prompt(prompt_list: list) -> list:\n",
    "    \"\"\"\n",
    "    Truncate the 'text' field in each dictionary to the first 30 tokens.\n",
    "\n",
    "    Args:\n",
    "        prompt_list (list): List of dictionaries with keys 'text', 'id', and 'model'.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated list with truncated 'text' values.\n",
    "    \"\"\"\n",
    "    truncated_prompts = []\n",
    "    for entry in prompt_list:\n",
    "        tokens = generation_tokenizer.tokenize(entry['text'])\n",
    "        truncated_text = generation_tokenizer.convert_tokens_to_string(tokens[:30]) # CHANGE THIS TO CONTROL THE NUMBER OF TOKENS THE GENERATION MODEL IS GIVEN AS A PROMPT\n",
    "        truncated_prompts.append({\n",
    "            \"text\": truncated_text,\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"model\": entry[\"model\"]\n",
    "        })\n",
    "    return truncated_prompts\n",
    "    # The output is a list of dictionaries with keys 'text', 'id', and 'model'\n",
    "\n",
    "\n",
    "def generate_dataset(prompt_list: list, max_length: int, generation_model_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Generates a dataset of AI-generated texts based on the given original prompt list.\n",
    "\n",
    "    Args:\n",
    "        prompt_list (list): Original list of dictionaries with full 'text'.\n",
    "        max_length (int): Maximum length of each generated text.\n",
    "\n",
    "    Returns:\n",
    "        list: Dataset of AI-generated texts (list of dictionaries).\n",
    "    \"\"\"\n",
    "    truncated_prompts = generate_prompt(prompt_list)  # Apply generate_prompt within generate_dataset\n",
    "\n",
    "    data_ai = [\n",
    "        {\n",
    "            \"text\": generate_text(entry[\"text\"], max_length),\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"model\": generation_model_name\n",
    "        }\n",
    "        for entry in truncated_prompts\n",
    "    ]\n",
    "    return data_ai\n",
    "    # The output is a list of dictionaries, each dictionary has keys 'text', 'id', and 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Create the AI-generated dataset\n",
    "\n",
    "max_length = 1000    # Maximum length of each generated text (originally 100)\n",
    "data_ai_generated = generate_dataset(data_human, max_length, GENERATION_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the AI-generated dataset in the correct .jsonl format\n",
    "\n",
    "def save_ai_dataset_jsonl(data, file_name_base, generation_model_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in JSONL format in a structured directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of dictionaries to be saved.\n",
    "        file_name_base (str): Base name for the file.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_generated_by_{generation_model_name}_{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Data\", \"AI-Generated\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First AI-generated text record: {'text': 'In a faraway galaxy, the alien galaxy is far away, and on the surface of that distant galaxy, we see the stars. The galaxies are very dense, and this is a big difference.\\n\\nBut there are many other galaxies, and they are very dense, and we can see some of them. There are three other galaxies, and they are very dense, and they are very dark. We can look at these stars as if they are the equivalent of a telescope. We can look', 'model': 'gpt2', 'source': 'FleLLM'}\n"
     ]
    }
   ],
   "source": [
    "# Save and check the AI-generated dataset\n",
    "\n",
    "# Save the AI-generated dataset\n",
    "save_ai_dataset_jsonl(data_ai_generated, \"XSUM_200_Samples\", GENERATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_generated[0] if data_ai_generated else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. *Option 2: load AI-generated texts from a dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First AI-generated text record: {'text': 'Forza Motorsport is a popular racing game that provides players with the ability to race on various tracks and in different vehicles. Whether you\\'re a seasoned racer or a newbie, playing Forza Motorsport can be a fun experience. In this article, we will take you through the different steps on how to play Forza Motorsport.\\n\\nStep 1. Insert The Game Disc\\n\\nThe first step is to insert the game disc into your console or computer. Follow the instructions to set up the game.\\n\\nStep 2. Choose Your Game\\n\\nOnce the game is set up, choose the game you\\'d like to play. Forza Motorsport has different modes: Career, Free Play, and Arcade. In this article, we will focus on the Arcade mode.\\n\\nStep 3. Just Make A Quick Race By The Arcade Mode\\n\\nOnce the Arcade mode is selected, choose \"Quick Race\" to get started quickly.\\n\\nStep 4. Pick A Racetrack\\n\\nPick a racetrack from the different ones available like Road Atlanta, New York, Rio de Janeiro, Maple Valley, or anything to choose from.\\n\\nStep 5. Pick A Class And A Car\\n\\nForza Motorsport has different car classes, from Classes A, B, C, D, S to R. Pick a class that suits your gaming style and choose a car of your liking.\\n\\nStep 6. The Options Panel\\n\\nThe Options Panel opens automatically after choosing a car. Press \"OK\", and be patient while the race is loading. It will show what time, wind direction, wind heading, and miles.\\n\\nStep 7. Accelerate\\n\\nTo accelerate, press the Right trigger to make the car engine running.\\n\\nStep 8. Go!\\n\\nOnce acceleration is complete, let go of the brakes and hit the gas to go!\\n\\nStep 9. The Arrows\\n\\nTo steer, use the left thumbstick and to turn to the left, move the thumbstick left, and to turn right, move the thumbstick right.\\n\\nStep 10. Pause The Game\\n\\nTo pause the game, just press the \"start\" button. If you feel lonely and wanted an operant, choose the \"Ghost car\" setting and turn it on.\\n\\nStep 11. Replay the Race\\n\\nIf you want to see how well you did, use this feature to see a video of the race, and it will show you turns for some reason.\\n\\nStep 12. Have Fun And Enjoy The Game!\\n\\nIn conclusion, Forza Motorsport can be a fun game with its impressive graphics, user-friendly interface, and thrilling gameplay. Follow these steps to improve your gameplay and have fun.', 'model': 'chatGPT', 'source': 'wikihow', 'label': 1, 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"  # CHANGE THIS TO THE PATH OF THE AI-GENERATED TEXT FILE\n",
    "# # FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "# if not os.path.exists(FILE_PATH):\n",
    "#     raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "# data_ai_dataset = []\n",
    "\n",
    "# # Read entire file and parse as JSON list\n",
    "# with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "#     for line in file:\n",
    "#         record = json.loads(line)  # Parse JSON once\n",
    "#         if record.get(\"model\") != \"human\":\n",
    "#             data_ai_dataset.append(record)\n",
    "\n",
    "# # Print first AI-generated text record\n",
    "# print(\"First AI-generated text record:\", data_ai_dataset[0] if data_ai_dataset else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- **Exemple usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the perturbations in the correct .jsonl format\n",
    "\n",
    "def save_perturbations_jsonl(all_perturbed_texts, file_name_base, generation_model_name, perturbation_model_name):\n",
    "    \"\"\"\n",
    "    Saves perturbed texts in JSONL format, where each line is a list of perturbed texts (one perturbation batch).\n",
    "\n",
    "    Parameters:\n",
    "        all_perturbed_texts (list of lists): List of lists containing perturbed texts.\n",
    "        file_name_base (str): Base name for the file.\n",
    "        generation_model_name (str): Name of the model that generated the original texts.\n",
    "        perturbation_model_name (str): Name of the model that generated the perturbations.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_generated_by_{generation_model_name}_perturbed_by_{perturbation_model_name}_{timestamp}.jsonl\"\n",
    "    file_relative_path = os.path.join(\"Data\", \"Perturbations\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for perturbed_texts in all_perturbed_texts:\n",
    "            f.write(json.dumps(perturbed_texts) + \"\\n\")\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the results in the correct .jsonl format\n",
    "\n",
    "def save_results_jsonl(data, file_name_base, generation_model_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in JSONL format in a structured directory.\n",
    "    \n",
    "    Parameters:\n",
    "        data (list): List of results to be saved.\n",
    "        file_name_base (str): Base name for the file.\n",
    "    \"\"\"\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{file_name_base}_{generation_model_name}_{timestamp}.jsonl\"   # CHANGE THIS TO CONTROL THE FORMAT OF THE FILE NAME\n",
    "    file_relative_path = os.path.join(\"Results\", file_name)\n",
    "    file_path = os.path.join(BASE_DIR, file_relative_path)\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save the dataset in JSONL format\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human\n",
    "NUM_SAMPLES = 200\n",
    "MAX_LENGTH = 512\n",
    "NUM_PERTURBATIONS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# log_probs_base_human, log_probs_transformed_human = optimized_processing(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "perturbed_texts_human = gen_perturbed(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "save_perturbations_jsonl(perturbed_texts_human, \"XSUM__Human_100_Perturbations_200_Samples\", \"Human\", PERTURBATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "log_probs_base_human, log_probs_transformed_human = compare_log_prob(DATA, perturbed_texts_human, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human, log_probs_transformed_human)\n",
    "\n",
    "results_human = {}\n",
    "results_human[\"log_probs_base\"] = log_probs_base_human\n",
    "results_human[\"log_probs_transformed\"] = log_probs_transformed_human\n",
    "results_human[\"discrepancy_scores\"] = discrepancy_scores_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "save_results_jsonl(results_human, \"XSUM__Human_100_Perturbations_200_Samples\", \"Human\") # CHANGE THIS TO CONTROL THE FILE NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_ai_generated\n",
    "NUM_SAMPLES = 200\n",
    "MAX_LENGTH = 512\n",
    "NUM_PERTURBATIONS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# log_probs_base_ai, log_probs_transformed_ai = optimized_processing(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "perturbed_texts_ai = gen_perturbed(DATA, NUM_SAMPLES, MAX_LENGTH, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "save_perturbations_jsonl(perturbed_texts_ai, \"XSUM_AI_100_Perturbations_200_Samples\", GENERATION_MODEL_NAME, PERTURBATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME\n",
    "\n",
    "log_probs_base_ai, log_probs_transformed_ai = compare_log_prob(DATA, perturbed_texts_ai, NUM_PERTURBATIONS, BATCH_SIZE)\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai, log_probs_transformed_ai)\n",
    "\n",
    "results_ai = {}\n",
    "results_ai[\"log_probs_base\"] = log_probs_base_ai\n",
    "results_ai[\"log_probs_transformed\"] = log_probs_transformed_ai\n",
    "results_ai[\"discrepancy_scores\"] = discrepancy_scores_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "save_results_jsonl(results_ai, \"XSUM_AI_100_Perturbations_200_Samples\", GENERATION_MODEL_NAME) # CHANGE THIS TO CONTROL THE FILE NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AI texts results\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Results\\\\\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(FILE_PATH, 'r') as file:\n",
    "#     data_ai = json.load(file)\n",
    "# log_probs_base_ai = data_ai[\"log_probs_base\"]\n",
    "# log_probs_transformed_ai = data_ai[\"log_probs_transformed\"]\n",
    "\n",
    "# discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Human texts results\n",
    "# BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# FILE_RELATIVE_PATH = \"Results\\\\\"\n",
    "# FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(FILE_PATH, 'r') as file:\n",
    "#     data_human = json.load(file)\n",
    "# log_probs_base_human = data_human[\"log_probs_base\"]\n",
    "# log_probs_transformed_human = data_human[\"log_probs_transformed\"]\n",
    "\n",
    "# discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def optimal_threshold(list1, list2):\n",
    "    X = np.concatenate([list1, list2]).reshape(-1, 1)\n",
    "    y = np.concatenate([np.zeros(len(list1)),np.ones(len(list2))])\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    best_threshold = -clf.intercept_[0] / clf.coef_[0][0]\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = (clf.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    return best_threshold, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold,auroc = optimal_threshold(discrepancy_scores_human, discrepancy_scores_ai)\n",
    "print(f\"Optimal threshold: {threshold:.2f}\")\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "\n",
    "plt.hist(discrepancy_scores_human, bins=15, alpha=0.5, label='Human', edgecolor='black', density=True)\n",
    "plt.hist(discrepancy_scores_ai, bins=15, alpha=0.5, label='AI', edgecolor='black', density=True)\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "plt.xlabel(f'Discrepancy scores (normalization={normalization})')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Computation model: {COMPUTATION_MODEL_NAME}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
