{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DetectGPT for dummies**: Identifying AI-generated text\n",
    "This notebook implements the **DetectGPT** method from Mitchell et al. (2023) [1], which helps determine whether a given text is AI-generated. The approach involves perturbing the text and analyzing its log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- **Model setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a simple setup of different transformer based models that will be needed to:\n",
    "1. produce the AI-generated text - ``generation_model``\n",
    "2. compute the log-probablities - ``computation_model``\n",
    "3. perturb the text with the T5 perturbation - ``t5_model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "\n",
    "GENERATION_MODEL_NAME = \"openai-community/gpt2-large\"\n",
    "# Model list (all tested)\n",
    "# gpt2\n",
    "# gpt2-large\n",
    "# EleutherAI/gpt-j-6B\n",
    "# EleutherAI/gpt-neox-20b\n",
    "\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(GENERATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "generation_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "COMPUTATION_MODEL_NAME = \"openai-community/gpt2-large\"\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "computation_model = AutoModelForCausalLM.from_pretrained(COMPUTATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "computation_tokenizer = AutoTokenizer.from_pretrained(COMPUTATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "computation_tokenizer.pad_token = computation_tokenizer.eos_token\n",
    "\n",
    "# Set model to evaluation mode (ensures stable log prob estimation + disables dropout)\n",
    "computation_model.eval()\n",
    "\n",
    "computation_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Perturbation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/tmp/huggingface\"\n",
    "PERTURBATION_MODEL_NAME = \"t5-large\"\n",
    "TORCH_DTYPE = torch.bfloat16 # use bfloat16 for all models\n",
    "\n",
    "# Load model\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(PERTURBATION_MODEL_NAME, torch_dtype=TORCH_DTYPE, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load tokenizer \n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(PERTURBATION_MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Set to evaluation mode\n",
    "t5_model.eval()\n",
    "\n",
    "t5_model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- **Code setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **🔀 Text perturbation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the **T5-based perturbation function**, which modifies the input text slightly while preserving its meaning. \n",
    "\n",
    "- **Why is perturbation needed?** AI-generated text often sits in **low-curvature** probability regions, meaning slight perturbations can significantly change their log probabilities\n",
    "- **How does it work?** The **T5 model** introduces variations to the text and helps in detecting AI-generated content\n",
    "\n",
    "These perturbed texts will later be compared to their original versions to compute the discrepancy scores d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: max_words=370 default but T5 max_length=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mask_text(texts, mask_ratio=0.15, max_words=370):\n",
    "    \"\"\"Mask multiple texts at once.\"\"\"\n",
    "    masked_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        \n",
    "        # Truncate text\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        \n",
    "        num_masks = int(len(words) * mask_ratio)\n",
    "        \n",
    "        # Randomly select spans to mask (sorted in reverse to avoid index shifts)\n",
    "        mask_indices = sorted(random.sample(range(len(words) - 1), num_masks), reverse=True)\n",
    "        \n",
    "        for i, idx in enumerate(mask_indices):\n",
    "            words[idx] = f\"<extra_id_{i}>\"\n",
    "            if idx + 1 < len(words):  # Ensure a 2-word span\n",
    "                del words[idx + 1]  # Remove instead of replacing with \"\"\n",
    "        \n",
    "        masked_texts.append(\" \".join(words))\n",
    "    \n",
    "    return masked_texts\n",
    "\n",
    "def batch_replace_masks(texts, batch_size=128):\n",
    "    \"\"\"Generate T5 model outputs for masked texts in batches.\"\"\"\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        n_expected = [text.count(\"<extra_id_\") for text in batch_texts]\n",
    "        stop_id = t5_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "        \n",
    "        tokens = t5_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Move input tensors to model's device\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=150,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=stop_id\n",
    "            )\n",
    "            \n",
    "        # Move outputs back to CPU to save GPU memory\n",
    "        outputs = outputs.detach().cpu()\n",
    "        batch_decoded = t5_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "        all_outputs.extend(batch_decoded)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def batch_extract_fills(texts):\n",
    "    \"\"\"Extract the generated fills from T5's output for multiple texts.\"\"\"\n",
    "    extracted_fills = []\n",
    "    for text in texts:\n",
    "        text = text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n",
    "        \n",
    "        # Use regex to extract text inside <extra_id_X> tokens\n",
    "        fills = re.findall(r\"<extra_id_\\d+>\\s*(.*?)\\s*(?=<extra_id_\\d+>|$)\", text)\n",
    "        \n",
    "        # Clean extracted tokens\n",
    "        extracted_fills.append([fill.strip() for fill in fills])\n",
    "    \n",
    "    return extracted_fills\n",
    "\n",
    "def batch_apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    \"\"\"Replace mask tokens in the masked texts with generated fills.\"\"\"\n",
    "    filled_texts = []\n",
    "    \n",
    "    for masked_text, fills in zip(masked_texts, extracted_fills):\n",
    "        if not fills:\n",
    "            filled_texts.append(masked_text)\n",
    "            continue\n",
    "        \n",
    "        filled_text = masked_text\n",
    "        # Iterate through expected mask positions and replace them\n",
    "        for i, fill in enumerate(fills):\n",
    "            filled_text = filled_text.replace(f\"<extra_id_{i}>\", fill, 1)\n",
    "        \n",
    "        filled_texts.append(filled_text)\n",
    "    \n",
    "    return filled_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_perturbation(text: str, batch_size: int) -> str:\n",
    "    \"\"\"\n",
    "    T5 perturbation - batch version\n",
    "\n",
    "    Args:\n",
    "        text (str): the input texts to be perturbed\n",
    "        batch_size (int): batch_size for compute\n",
    "\n",
    "    Returns:\n",
    "        all_perturbed_texts (str): the perturbed texts\n",
    "    \"\"\"\n",
    "    # Step 1: mask all texts at once\n",
    "    all_masked_texts = batch_mask_text(text)\n",
    "\n",
    "    # Step 2: generate replacements in batches\n",
    "    all_raw_fills = batch_replace_masks(all_masked_texts, batch_size)\n",
    "\n",
    "    # Step 3: extract fills\n",
    "    all_extracted_fills = batch_extract_fills(all_raw_fills)\n",
    "\n",
    "    # Step 4: apply fills\n",
    "    all_perturbed_texts = batch_apply_extracted_fills(all_masked_texts, all_extracted_fills)\n",
    "    \n",
    "    return all_perturbed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **🔍 Main functions: *DetectGPT* Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section implements the **DetectGPT method**.\n",
    "\n",
    "- **Key idea:** once again, AI-generated texts often **reside in low-curvature probability regions**.\n",
    "- **How does it work?**\n",
    "  - We perturb the text multiple times (``num_perturbation``). We will use ``n_samples`` texts with ``max_length`` words.\n",
    "  - Compute log probabilities for both **original** and **perturbed** texts\n",
    "  - Measure the **discrepancy score** (a higher score suggests AI-generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_average_log_prob(texts, batch_size=128):\n",
    "    \"\"\"Calculate average log probability for multiple texts in batches.\"\"\"\n",
    "    \n",
    "    all_log_probs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = computation_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = computation_model(input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract logits\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Shift logits and labels to align\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "        shift_mask = attention_mask[..., 1:].contiguous()  # Ensure mask aligns\n",
    "\n",
    "        # Compute per-token loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=computation_tokenizer.pad_token_id)\n",
    "        loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        # Reshape to [batch_size, seq_length - 1]\n",
    "        loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "        # Compute per-sample log prob\n",
    "        sample_losses = []\n",
    "        for j in range(loss_per_token.size(0)):\n",
    "            mask = shift_mask[j].bool()  # Use shift_mask for actual tokens\n",
    "            if mask.sum() > 0:\n",
    "                sample_loss = loss_per_token[j][mask].mean().item()\n",
    "                sample_losses.append(-sample_loss)  # Negative loss as log prob\n",
    "            else:\n",
    "                sample_losses.append(float('-inf'))  # Avoid zero prob bias\n",
    "\n",
    "        all_log_probs.extend(sample_losses)\n",
    "\n",
    "    return all_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_processing(data, n_samples=300, max_length=50, n_perturbations=100, batch_size=128):\n",
    "    log_probs_per_text_transformed = []\n",
    "    \n",
    "    # Process original texts in batches\n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "    log_probs_per_text_base = batch_average_log_prob(original_texts, batch_size)\n",
    "    \n",
    "    # Inside the loop in optimized_processing()\n",
    "    for perturbation_idx in tqdm(range(n_perturbations), desc=f\"Processing {n_perturbations} perturbations for {n_samples} texts. Perturbation number:\"):\n",
    "        all_perturbed_texts = t5_perturbation(original_texts,batch_size)\n",
    "        all_log_probs = batch_average_log_prob(all_perturbed_texts, batch_size)\n",
    "        \n",
    "        # Organize results by original text\n",
    "        for j in range(n_samples):\n",
    "            if perturbation_idx == 0:\n",
    "                log_probs_per_text_transformed.append([])\n",
    "            log_probs_per_text_transformed[j].append(all_log_probs[j])\n",
    "    \n",
    "    return log_probs_per_text_base, log_probs_per_text_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(data: list, n_samples: int, max_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Compute perplexity of each individual text in data\n",
    "\n",
    "    Args:\n",
    "        data (list): list of dictionaries containing text\n",
    "        n_samples (int): number of texts to process\n",
    "        max_length (int): max length for tokenization\n",
    "\n",
    "    Returns:\n",
    "        perplexities (list): perplexity scores of each text\n",
    "    \"\"\"\n",
    "    \n",
    "    original_texts = [\" \".join(data[j][\"text\"].split()[:max_length]) for j in range(n_samples)]\n",
    "    perplexities = []\n",
    "\n",
    "    # Tokenize all inputs at once (better efficiency)\n",
    "    inputs = computation_tokenizer(original_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move tensors to GPU\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = computation_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Shift logits and labels to align\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "    shift_mask = attention_mask[:, 1:].contiguous() # Ensure mask aligns\n",
    "\n",
    "    # per-token loss for each sequence\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none', ignore_index=computation_tokenizer.pad_token_id)\n",
    "    loss_per_token = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    # Reshape\n",
    "    loss_per_token = loss_per_token.view(shift_labels.size())\n",
    "\n",
    "    # Compute per-sentence loss by averaging over valid tokens\n",
    "    valid_token_counts = shift_mask.sum(dim=1) # Number of valid tokens per sample\n",
    "    sentence_losses = (loss_per_token * shift_mask).sum(dim=1) / valid_token_counts.clamp(min=1) # Avoid division by zero\n",
    "\n",
    "    # Compute perplexity per sample\n",
    "    perplexities = torch.exp(sentence_losses).cpu().tolist()\n",
    "    return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detectgpt_discrepancy(log_probs_per_text_base: list, log_probs_per_text_transformed: list, normalization: bool=False) -> list:\n",
    "    \"\"\"\n",
    "    Compute the DetectGPT discrepancy metric for each of the n_samples texts\n",
    "    Calculated for num_perturbations perturbations\n",
    "\n",
    "    Args:\n",
    "        log_probs_per_text_base (list): original log probability of each text\n",
    "        log_probs_per_text_transformed (list): list of size n_samples where each element is a list of the num_perturbations perturbed log probabilities\n",
    "        normalization (bool)\n",
    "\n",
    "    Returns:\n",
    "        discrepancy_scores (list): list of discrepancy values (d) for the n_samples texs\n",
    "    \"\"\"\n",
    "    num_samples = len(log_probs_per_text_base) \n",
    "    discrepancy_scores = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        original_log_prob = log_probs_per_text_base[i]\n",
    "        perturbed_log_probs = log_probs_per_text_transformed[i] # List of perturbed log probs\n",
    "        num_perturbations = len(perturbed_log_probs) # Number of perturbations\n",
    "\n",
    "        # Compute mean log probability of the perturbed texts\n",
    "        mu = sum(perturbed_log_probs) / num_perturbations  \n",
    "\n",
    "        # Compute discrepancy\n",
    "        discrepancy_score_unormalized = original_log_prob - mu\n",
    "        if normalization:\n",
    "            # Normalize\n",
    "            variance = sum((log_prob - mu) ** 2 for log_prob in perturbed_log_probs) / (num_perturbations - 1)\n",
    "            sigma = variance ** 0.5\n",
    "            discrepancy_score_normalized = discrepancy_score_unormalized / sigma if sigma > 0 else 0\n",
    "            discrepancy_scores.append(discrepancy_score_normalized)\n",
    "        else:\n",
    "            discrepancy_scores.append(discrepancy_score_unormalized)\n",
    "    \n",
    "    return discrepancy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "def clear_cuda_cache():\n",
    "    \"\"\"Clear CUDA cache to free up memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Add caching for tokenization\n",
    "@lru_cache(maxsize=1024)\n",
    "def cached_tokenize(text, is_t5=False):\n",
    "    \"\"\"Cache tokenization results to avoid repeated work.\"\"\"\n",
    "    if is_t5:\n",
    "        return t5_tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        return computation_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- **Data loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**📌 Dataset format guidelines**\n",
    "\n",
    "All datasets (human-written and AI-generated) must follow this format:\n",
    "\n",
    "- Stored as a **`.jsonl`** where each line is a dictionary.\n",
    "- Each entry contains (minimum requirement):\n",
    "  - `\"text\"`: the text content\n",
    "  - `\"model\"`: for human text please label it as `\"human\"` and for AI-generated texts, please specify the model used (e.g. ``\"gpt2-large\"``)\n",
    "  - `\"source\"`: the origin of the text (e.g., `\"wikihow\"`, `\"reddit\"`, `\"news articles\"`)\n",
    "\n",
    "#### Exemple (as in ``subtaskB_train.jsonl`` located in `Datasets\\SemEval2024-Task8`):\n",
    "```json\n",
    "{\"text\": \"A groundbreaking discovery in physics was made today.\", \"model\": \"human\", \"source\": \"news articles\"}\n",
    "{\"text\": \"The AI revolution is shaping the future of work.\", \"model\": \"chatGPT\", \"source\": \"AI Generated\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Human texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_human = []\n",
    "\n",
    "# Efficiently process the file line by line\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") == \"human\":\n",
    "            data_human.append(record)\n",
    "\n",
    "# Print first human record\n",
    "print(\"First human text record:\", data_human[0] if data_human else \"No human data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **AI-generated texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. *Option 1: produce own AI-generated texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate AI text from a given prompt\n",
    "\n",
    "    Args:\n",
    "        prompt (str): prompt to generate text\n",
    "        max_length (int): max length of generated text\n",
    "\n",
    "    Returns:\n",
    "        generated_text (str): generated text\n",
    "    \"\"\"\n",
    "    inputs = generation_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = generation_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    generated_text = generation_tokenizer.decode(output[0].cpu(), skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def generate_dataset(n_samples: int, max_length: int, generation_model_name: str, prompt: str = \"In a faraway galaxy,\") -> list:\n",
    "    \"\"\"\n",
    "    Generates a dataset of N AI-generated texts in the required dictionary format\n",
    "\n",
    "    Args:\n",
    "        n_samples (int): number of AI-generated texts\n",
    "        max_length (int): max length of each generated text\n",
    "        generation_model_name (str): name of the AI generation model\n",
    "        prompt (str, optional): prompt to start text generation\n",
    "    \n",
    "    Returns:\n",
    "        data_ai (list): dataset of AI-generated texts (list of dictionaries)\n",
    "    \"\"\"\n",
    "    data_ai = []\n",
    "\n",
    "    for _ in tqdm(range(n_samples), desc=\"Generating dataset\", unit=\"sample\"):\n",
    "        generated_text = generate_text(prompt, max_length)\n",
    "        data_ai.append({\n",
    "            \"text\": generated_text,\n",
    "            \"model\": generation_model_name,\n",
    "            \"source\": \"FleLLM\"\n",
    "        })\n",
    "\n",
    "    return data_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model.to(device)\n",
    "print(device)\n",
    "\n",
    "n_samples = 1000\n",
    "max_length = 100\n",
    "PROMPT = \"In a faraway galaxy,\"\n",
    "\n",
    "# clear_cuda_cache()\n",
    "data_ai_generated = generate_dataset(n_samples=n_samples, max_length=max_length, generation_model_name=GENERATION_MODEL_NAME, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataset in the correct .jsonl format\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH)\n",
    "\n",
    "with open(FILE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data_ai_generated:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_generated[0] if data_ai_generated else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. *Option 2: load AI-generated texts from a dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Datasets\\SemEval2024-Task8\\subtaskB_train.jsonl\"\n",
    "# FILE_RELATIVE_PATH = \"Datasets\\AI-generated\\dataset_ai.jsonl\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "data_ai_dataset = []\n",
    "\n",
    "# Read entire file and parse as JSON list\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)  # Parse JSON once\n",
    "        if record.get(\"model\") != \"human\":\n",
    "            data_ai_dataset.append(record)\n",
    "\n",
    "# Print first AI-generated text record\n",
    "print(\"First AI-generated text record:\", data_ai_dataset[0] if data_ai_dataset else \"No AI-generated data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Texts perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human\n",
    "n_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "# clear_cuda_cache()\n",
    "perplexity_scores = compute_perplexity(data=DATA,n_samples=n_samples,max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- **Exemple usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_human\n",
    "n_samples = 1000\n",
    "max_length = 50\n",
    "n_perturbations = 100\n",
    "batch_size = 128\n",
    "\n",
    "# clear_cuda_cache()\n",
    "log_probs_base_human, log_probs_transformed_human = optimized_processing(data=DATA,n_samples=n_samples,max_length=max_length,n_perturbations=n_perturbations,batch_size=batch_size)\n",
    "\n",
    "normalization = False\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=normalization)\n",
    "\n",
    "results_human = {}\n",
    "results_human[\"log_probs_base\"] = log_probs_base_human\n",
    "results_human[\"log_probs_transformed\"] = log_probs_transformed_human\n",
    "results_human[\"discrepancy_scores\"] = discrepancy_scores_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_human, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI-generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data_ai_dataset # or DATA = data_ai_generated\n",
    "n_samples = 1000\n",
    "max_length = 50\n",
    "n_perturbations = 100\n",
    "batch_size = 128\n",
    "\n",
    "# clear_cuda_cache()\n",
    "log_probs_base_ai, log_probs_transformed_ai = optimized_processing(data=DATA,n_samples=n_samples,max_length=max_length,n_perturbations=n_perturbations,batch_size=batch_size)\n",
    "\n",
    "normalization = False\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=normalization)\n",
    "\n",
    "results_ai = {}\n",
    "results_ai[\"log_probs_base\"] = log_probs_base_ai\n",
    "results_ai[\"log_probs_transformed\"] = log_probs_transformed_ai\n",
    "results_ai[\"discrepancy_scores\"] = discrepancy_scores_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\experiment_0_results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, \"w\") as f:\n",
    "    json.dump(results_ai, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Histograms of the discrepancy scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI texts results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\\\results_ai.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    data_ai = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base_ai = data_ai[\"log_probs_base\"]\n",
    "log_probs_transformed_ai = data_ai[\"log_probs_transformed\"]\n",
    "\n",
    "discrepancy_scores_ai = compute_detectgpt_discrepancy(log_probs_base_ai,log_probs_transformed_ai,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human texts results\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "FILE_RELATIVE_PATH = \"Results\\\\results_human.json\"\n",
    "FILE_PATH = os.path.join(BASE_DIR,FILE_RELATIVE_PATH) \n",
    "\n",
    "with open(FILE_PATH, 'r') as file:\n",
    "    data_human = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs_base_human = data_human[\"log_probs_base\"]\n",
    "log_probs_transformed_human = data_human[\"log_probs_transformed\"]\n",
    "\n",
    "discrepancy_scores_human = compute_detectgpt_discrepancy(log_probs_base_human,log_probs_transformed_human,normalization=NORMALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def optimal_threshold(list1, list2):\n",
    "    X = np.concatenate([list1, list2]).reshape(-1, 1)\n",
    "    y = np.concatenate([np.zeros(len(list1)),np.ones(len(list2))])\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    best_threshold = -clf.intercept_[0] / clf.coef_[0][0]\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = (clf.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    return best_threshold, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold,auroc = optimal_threshold(discrepancy_scores_human, discrepancy_scores_ai)\n",
    "print(f\"Optimal threshold: {threshold:.2f}\")\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "\n",
    "plt.hist(discrepancy_scores_human, bins=15, alpha=0.5, label='Human', edgecolor='black', density=True)\n",
    "plt.hist(discrepancy_scores_ai, bins=15, alpha=0.5, label='AI', edgecolor='black', density=True)\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "plt.xlabel(f'Discrepancy scores (normalization={normalization})')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Computation model: {COMPUTATION_MODEL_NAME}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] E. Mitchell, C. Lin, A. Bosselut, and C. D. Manning, \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\" *arXiv preprint*, 2023. Available at: [arXiv:2301.11305](https://arxiv.org/abs/2301.11305)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectgpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
